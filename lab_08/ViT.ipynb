{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vision Transformer (ViT)\n",
    "번 실습에서는 Vision Transformer (ViT) 아키텍처를 직접 구현해보겠습니다. \n",
    "\n",
    "ViT는 원래 자연어 처리에서 주로 사용되던 Transformer 아키텍처를 이미지에 적용하여 이미지 분류 문제에서 뛰어난 성능을 발휘하도록 한 모델입니다. \n",
    "\n",
    "실습에서는 ViT를 구성하는 핵심 모듈들을 단계별로 구현해봄으로써, ViT 아키텍처의 구조와 원리를 이해하게 됩니다. 각 모듈이 어떤 역할을 하고, 최종적으로 어떻게 조합되어 이미지 분류를 수행하는지 알아보도록 합시다.\n",
    "\n",
    "이번 실습은 다음과 같은 주요 모듈들을 순차적으로 구현하는 것으로 이루어져 있습니다.\n",
    "- MultiHeadSelfAttention : 이미지의 패치(patch)들 간의 상호작용을 학습하며, 각 패치의 정보가 다른 패치들의 정보와 자연스럽게 통합(integrate) 됩니다. 이를 통해 각 패치는 이미지 전체 맥락을 반영한 더욱 풍부하고 정교한 표현(representation)을 학습할 수 있습니다.\n",
    "- FeedForwardNetwork : Self-Attention에서 나온 정보를 비선형 변환하여 더욱 복잡한 표현을 학습하게 합니다.\n",
    "- TransformerEncoder : Self-Attention과 FeedForwardNetwork를 결합하여 하나의 Transformer 인코더 블록을 구성합니다. 여러 인코더 블록을 쌓아 ViT의 깊이를 조절할 수 있습니다.\n",
    "- ImagePatchifier : 이미지를 여러 작은 패치(patch)로 분할하여 Transformer의 입력으로 사용할 수 있는 형태로 변환합니다. \n",
    "- PatchEmbedding : 분할된 패치를 벡터로 변환하여 Transformer가 처리할 수 있는 임베딩(embedding) 형태로 만듭니다. ViT에서는 각 패치가 단어처럼 취급되어 입력됩니다.\n",
    "- ViT : 위의 모듈들을 종합하여 최종 Vision Transformer 모델을 구현합니다. 완성된 모델은 입력된 이미지 패치들 간의 관계를 학습하고, 최종적으로 이미지를 분류하는 결과를 제공합니다.\n",
    "\n",
    "Original paper : An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale ([link](https://arxiv.org/pdf/2010.11929))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, models\n",
    "import torchvision.transforms.v2 as transforms\n",
    "\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from training_utilities import train_loop, evaluation_loop, save_checkpoint, load_checkpoint, load_cifar10_dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Head Self-Attention\n",
    "\n",
    "Transformer의 핵심 모듈로, 이미지의 패치(patch)들 간의 상호작용을 학습하며, 각 패치의 정보가 다른 패치들의 정보와 자연스럽게 통합(integrate) 됩니다. 이를 통해 각 패치는 이미지 전체 맥락을 반영한 더욱 풍부하고 정교한 표현(representation)을 학습할 수 있습니다.\n",
    "\n",
    "### 구성요소\n",
    "\n",
    "- Linear Projections: 입력 임베딩(embedding)을 queries, keys, values로 변환하는 학습가능한 선형 레이어\n",
    "- Attention Mechanism: queries와 keys를 이용하여 Attention score를 계산하고, 이를 values값에 적용합니다. 이 과정에서 패치 간 중요한 정보가 통합됩니다.\n",
    "- Output Projection: 모든 attention head로부터 나온 출력값을 합쳐 하나의 최종 결과를 생성하는 선형 레이어입니다.\n",
    "\n",
    "### 계산과정\n",
    "\n",
    "Attention Score는 아래의 수식을 통해 계산됩니다:\n",
    "$$ Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V$$\n",
    "여기서 $d_k$는 queries, keys, values의 차원의 크기 (hidden dimension size)입니다 \n",
    "\n",
    "실제 계산에서는 $d_{model}$크기의 hidden vector에 한번의 attention을 적용하는 것 보다 (single head) 서로 다른 query, key, value 선형변환을 $h$번 적용하는것이 더 성능이 뛰어납니다. (Multi head attention이라고 불림, $h$는 head의 수(number of heads))\n",
    "\n",
    "$$ MultiHead(Q, K, V) = Concat(head_1, ..., head_h)W^O$$\n",
    "where $ head_i = Attention(QW_i^Q,KW_i^K, VW_i^V)$\n",
    " - $d_{model} = h \\times d_k$\n",
    " - $W_i^Q,W_i^K, W_i^V$ are linear projection matrices of shape $(d_{model}, d_k)$\n",
    " - $W^O$ are linear projection matrix of shape $(hd_k, d_{model}) = (d_{model}, d_{model})$\n",
    "\n",
    "### <mark>실습 </mark> 아래 구현 과정에 따라 `MultiHeadSelfAttention`을 완성하세요.\n",
    "\n",
    "1. Queries, Keys, Values 계산 : 입력 텐서 `x`에 linear layer를 적용하여 queries `Q`, keys `K`, values `V` 들을 계산합니다.\n",
    "\n",
    "2. multiple heads로 쪼개기: `Q`, `K`, `V`를 변환하여 `(batch_size, num_heads, seq_length, head_dim)`의 shape을 가지도록 변환합니다.\n",
    "   - [torch.Tensor.view](https://pytorch.org/docs/stable/generated/torch.Tensor.view.html), [torch.reshape](https://pytorch.org/docs/stable/generated/torch.reshape.html): 텐서의 shape 변경\n",
    "   - [torch.permute](https://pytorch.org/docs/stable/generated/torch.permute.html): 텐서의 차원 순서를 재배열\n",
    "\n",
    "3. Scaled Dot-Product Attention 계산:\n",
    "   - $Q$와 $K^T$의 dot product를 계산하여 attention score를 계산한다.\n",
    "     - [torch.transpose](https://pytorch.org/docs/stable/generated/torch.transpose.html)\n",
    "     - [torch.matmul](https://pytorch.org/docs/stable/generated/torch.matmul.html)\n",
    "   - attention scores를 `self.head_dim`의 제곱근으로 나누어 스케일링해주어 안정적인 학습을 돕습니다.\n",
    "   - `torch.softmax` 함수를 이용하여 attention weight를 계산합니다.\n",
    "   - attention weight에 dropout layer를 적용한다.\n",
    "\n",
    "4. Attention Output:\n",
    "   - attention weights와 values `V`를 곱하여 각 head에 대한 attention 출력값을 계산합니다.\n",
    "   - 모든 head의 attention출력값을 concat합니다 (`permute`와 `view`함수 이용).\n",
    "   - concat된 output에 $W^O$에 대응하는 선형 변환(`self.fc`)을 수행합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Self Attention Module.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim, num_head, dropout_prob = 0.1):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim            # d_model, total hidden dimension\n",
    "        self.num_head = num_head                # h\n",
    "        self.head_dim = hidden_dim // num_head  # d_k\n",
    "        assert self.head_dim * num_head == hidden_dim, \"hidden_dim must be divisible by num_head.\"\n",
    "\n",
    "        self.query_projection = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.key_projection = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.value_projection = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_length, hidden_dim = x.size() \n",
    "\n",
    "        # (batch_size, seq_len, num_head, head_dim)\n",
    "        Q = self.query_projection(x).view(batch_size, seq_length, self.num_head, self.head_dim)\n",
    "        K = self.key_projection(x).view(batch_size, seq_length, self.num_head, self.head_dim)\n",
    "        V = self.value_projection(x).view(batch_size, seq_length, self.num_head, self.head_dim)\n",
    "\n",
    "        # (batch_size, num_head, seq_len, head_dim)\n",
    "        Q = Q.permute(0, 2, 1, 3)\n",
    "        K = K.permute(0, 2, 1, 3)\n",
    "        V = V.permute(0, 2, 1, 3)\n",
    "\n",
    "        ##### YOUR CODE START #####\n",
    "\n",
    "        ##### YOUR CODE END #####\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "hidden_dim = 64       # d_model\n",
    "num_head = 8          # Number of heads\n",
    "seq_length = 10       # Sequence length\n",
    "batch_size = 32       # Batch size\n",
    "\n",
    "attention_module = MultiHeadSelfAttention(hidden_dim=hidden_dim, num_head=num_head)\n",
    "x = torch.randn(batch_size, seq_length, hidden_dim)\n",
    "output = attention_module(x)\n",
    "print(\"Output shape:\", output.shape)\n",
    "\n",
    "assert output.shape == (batch_size, seq_length, hidden_dim), \"Output shape is incorrect!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Position-wise Feed-Forward Networks\n",
    "`MultiHeadSelfAttention` 모듈의 출력값에 fully connected feed-forward network(FFN)를 적용합니다. 이 네트워크는 각 위치(position)별로 독립적으로 작동하며, 동일한 변환을 각 위치에 개별적으로 수행합니다. FFN은 모델이 더 복잡한 패턴을 학습할 수 있도록 돕습니다.\n",
    "\n",
    "$$ FFN(x) = GELU(xW_1+b_1)W_2+b_2$$\n",
    "\n",
    "구성요소:\n",
    " - 두개의 Linear Layers: 임베딩 차원(embedding diemmension)을 확장하였다가 다시 원래 차원으로 축소해 줍니다.\n",
    " - Activation Function: GELU non-linear activation 함수를 적용하여 모델이 복잡한 패턴을 학습할 수 있도록 합니다.\n",
    " - Dropout Layer for regularization\n",
    "\n",
    "### <mark>실습 </mark> 아래 구현 과정에 따라 `FeedForwardNetwork`을 구현하세요.\n",
    "\n",
    "- linear layer that maps from `hidden_dim` to `feedforward_dim`.\n",
    "- GELU activation ([troch.nn.GELU](https://pytorch.org/docs/stable/generated/torch.nn.GELU.html)).\n",
    "- dropout layer\n",
    "- A linear layer that maps back from `feedforward_dim` to `hidden_dim`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Position-wise Feed-Forward Networks\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim, feedforward_dim, dropout_prob = 0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, feedforward_dim),\n",
    "            ##### YOUR CODE START #####\n",
    "\n",
    "            ##### YOUR CODE END #####\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "hidden_dim = 64\n",
    "feedforward_dim = 1024\n",
    "seq_length = 10       # Sequence length\n",
    "batch_size = 32       # Batch size\n",
    "\n",
    "ffn = FeedForwardNetwork(hidden_dim=hidden_dim, feedforward_dim=feedforward_dim, dropout_prob = 0.2)\n",
    "x = torch.randn(batch_size, seq_length, hidden_dim)\n",
    "output = ffn(x)\n",
    "print(\"Output shape:\", output.shape)\n",
    "\n",
    "assert output.shape == (batch_size, seq_length, hidden_dim), \"Output shape is incorrect!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TransformerEncoder\n",
    "\n",
    "<img src=\"resources/vit_encoder.png\" style=\"width:200px;\">\n",
    "\n",
    "`MultiHeadSelfAttention`, `FeedForwardNetwork`, layer normalization을 모두 조합하여 Transformer encoder를 구현합니다.\n",
    "\n",
    "구성 요소:\n",
    "- Layer Normalization (LN): attention과 feed-forward 레이어 <u>전에</u> 적용해줍니다. 이를 Pre-LN방식이라 부르며, 특히 깊은 모델에서 학습 안정성을 높이는 데 효과적입니다.\n",
    "- Dropout Layers for regularization : 각 레이어 출력값에 dropout을 적용하여 과적합을 방지합니다.\n",
    "- Residual Connections: 각 레이어의 출력값에 입력값을 더해준다. 이를 통해 학습 안정성과 정보 흐름을 개선할 수 있습니다.\n",
    "\n",
    "### <mark>실습 </mark> 아래 구현 과정에 따라 `TransformerEncoder`을 완성하세요.\n",
    "\n",
    "1. First Sub-layer (Attention):\n",
    "    - 입력값에 layer normalization을 적용한다 ([nn.LayerNorm](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html) 이용).\n",
    "    - `MultiHeadSelfAttention` 모듈에 통과시킨다.\n",
    "    - 출력값에 dropout을 적용합니다.\n",
    "    - dropout이 적용된 출력값에 이 sub-layer의 입력값을 더해줍니다 (residual connection).\n",
    "\n",
    "2. Second Sub-layer (Feed-Forward Network):\n",
    "\n",
    "    - 첫번째 sub-layer출력값에 layer normalization을 적용한다.\n",
    "    - `FeedForwardNetwork` module에 통과시킨다\n",
    "    - feed-forward 출력값에 dropout을 적용합니다.\n",
    "    - 출력값에 이 sub-layer의 입력값을 더해준다\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Single Transformer Encoder Layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim, num_head, feedforward_dim, dropout_prob=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(hidden_dim)\n",
    "        self.mha = MultiHeadSelfAttention(hidden_dim, num_head, dropout_prob = dropout_prob)\n",
    "        self.dropout1 = nn.Dropout(dropout_prob)\n",
    "        self.norm2 = nn.LayerNorm(hidden_dim)\n",
    "        self.ffn = FeedForwardNetwork(hidden_dim, feedforward_dim, dropout_prob = dropout_prob)\n",
    "        self.dropout2 = nn.Dropout(dropout_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        ##### YOUR CODE START #####\n",
    "\n",
    "        ##### YOUR CODE END #####\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "hidden_dim = 64\n",
    "num_head = 8\n",
    "feedforward_dim = 1024\n",
    "seq_length = 10       # Sequence length\n",
    "batch_size = 32       # Batch size\n",
    "\n",
    "encoder = TransformerEncoder(hidden_dim=hidden_dim, num_head = num_head, feedforward_dim=feedforward_dim, dropout_prob = 0.2)\n",
    "x = torch.randn(batch_size, seq_length, hidden_dim)\n",
    "output = encoder(x)\n",
    "print(\"Output shape:\", output.shape)\n",
    "\n",
    "assert output.shape == (batch_size, seq_length, hidden_dim), \"Output shape is incorrect!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ImagePatchifier\n",
    "\n",
    "입력 이미지를 서로 겹치지 않는 패치(patch)들로 분할하고, 이를 Transformer에서 처리할 수 있는 시퀀스(sequence)형태의 입력을 변환하는 역할을 합니다. \n",
    "\n",
    "$(C, H, W)$ 차원의 이미지는 $(N, C \\times P^2)$ 형태로 변환됩니다.\n",
    "- $(H, W)$는 원본 이미지의 해상도(높이, 너비)이다.\n",
    "- $C$는 채널 수, $(P, P)$는 각 패치의 해상도이다.\n",
    "- $N = HW/P^2$는 생성되는 패치의 수 입니다.\n",
    "\n",
    "<img src=\"resources/vit_model.png\" style=\"width:600px;\">\n",
    "\n",
    "### <mark>실습 </mark> 아래 구현 과정에 따라 `ImagePatchifier`을 완성하세요.\n",
    "1. Patch Extraction: 이미지를 (patch_size, patch_size) 크기의 패치들로 나눕니다.\n",
    "    - [torch.nn.Unfold](https://pytorch.org/docs/stable/generated/torch.nn.Unfold.html)함수 이용\n",
    "    - 입력 shape : (batch_size, num_channels, height, width)\n",
    "    - 출력 shape : (batch_size, num_channels, num_patches_y, num_patches_x, patch_size, patch_size)\n",
    "2. Flattening: 각 patch들을 Flatten하여 시퀀스 형태로 변환합니다.\n",
    "    - `view`, `reshape`, `permute`함수를 이용하여 텐서의 형태를 변경합니다.\n",
    "    - 입력 shape : (batch_size, num_channels, num_patches_y, num_patches_x, patch_size, patch_size)\n",
    "    - 출력 shape : (batch_size, num_patches, num_channels * patch_size * patch_size)\n",
    "    - 주의: heigth-to-width 순서를 유지하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImagePatchifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Divides an image into patches and flatten\n",
    "    \"\"\"\n",
    "    def __init__(self, image_size, patch_size):\n",
    "        super().__init__()\n",
    "\n",
    "        assert image_size % patch_size == 0, \"Image dimensions must be divisible by the patch size.\"\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = (image_size // patch_size) ** 2\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            Tensor of shape (batch_size, num_channels, height, width)\n",
    "        Returns:\n",
    "            Tensor of shape (batch_size, num_patches, num_channels * patch_size * patch_size)\n",
    "        \"\"\"\n",
    "        B, C, H, W = x.shape\n",
    "        x = x.unfold(2, self.patch_size, self.patch_size).unfold(3, self.patch_size, self.patch_size) #(batch_size, num_channels, num_patches_y, num_patches_x, patch_size, patch_size)\n",
    "        \n",
    "        ##### YOUR CODE START #####\n",
    "\n",
    "        ##### YOUR CODE END #####\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "image_size = 224\n",
    "patch_size = 16\n",
    "batch_size = 64\n",
    "\n",
    "patchfier = ImagePatchifier(image_size, patch_size)\n",
    "x = torch.randn(batch_size, 3, image_size, image_size)\n",
    "output = patchfier(x)\n",
    "print(\"Output shape:\", output.shape)\n",
    "\n",
    "assert output.shape == (batch_size, 14 * 14, 3 * 16 * 16), \"Output shape is incorrect!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def visualize_patches(image, patch_size):\n",
    "    \"\"\"\n",
    "    Visualizes the patches of an image.\n",
    "    Args:\n",
    "        image: Tensor of shape (C, H, W)\n",
    "        patch_size: int\n",
    "    \"\"\"\n",
    "    patches = ImagePatchifier(224, patch_size)(image)[0]\n",
    "    num_patches = patches.size(0)\n",
    "    grid_size = int(np.sqrt(num_patches))\n",
    "    fig, axes = plt.subplots(grid_size, grid_size, figsize=(grid_size*2, grid_size*2))\n",
    "    for idx, patch in enumerate(patches):\n",
    "        row = idx // grid_size\n",
    "        col = idx % grid_size\n",
    "        ax = axes[row, col]\n",
    "        \n",
    "        patch = patch.view(-1, patch_size, patch_size).permute(1, 2, 0).numpy()\n",
    "        ax.imshow(patch)\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "img = Image.open('resources/n01580077_1031.JPEG')\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "img_tensor = transform(img).unsqueeze(0)  # Shape: (B, C, H, W)\n",
    "\n",
    "visualize_patches(image = img_tensor, patch_size = 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PatchEmbedding\n",
    "이미지 패치들을 임베딩 벡터(embedding vector)로 변환한 후, 위치 정보(positional embedding)와 [class] 토큰을 추가합니다.\n",
    "\n",
    "`ImagePatchifier`에서 시퀀스 형태로 변환된 이미지 패치들을 $[x_p^1; x_p^2; ... ; x_p^N]$이라 하겠습니다. ($N$은 패치 수)\n",
    "\n",
    "1. Patch embedding\n",
    "\n",
    "각 이미지 패치에 학습 가능한 선형 변환 행렬 $E$를 곱하여 tranformer에서 사용할 `embedding_dim` $D$으로 변환합니다. 이 변환은 각 패치를 고정된 크기의 벡터로 바꾸어, Transformer가 이미지 패치를 토큰처럼 다룰 수 있도록 합니다.\n",
    "\n",
    "$$[x_p^1E; x_p^2E; ... ; x_p^NE]$$\n",
    "where $E$ is embedding projection matrix of shape $(P^2\\times C, D)$, $P$는 patch_size, $C$는 채널 수(num_channels)\n",
    "\n",
    "\n",
    "2. `[class]` token\n",
    "\n",
    "BERT의 `[class]` 토큰(token)과 유사하게 새로운 학습 가능한 벡터 $x_{class}$를 패치 임베딩의 맨 앞에 추가해줍니다.\n",
    "$$[x_{class}; x_p^1E; x_p^2E; ... ; x_p^NE]$$\n",
    "\n",
    "이 `[class]` 토큰은 Transformer encoder를 거치며 이미지 전체의 정보를 요약하는 임베딩으로 학습됩니다. 최종적으로, Transformer Encoder의 출력에서 `[class]` 토큰 해당하는 벡터는 이미지 전체를 대표하는 임베딩 값(image representation)으로 사용되며, 여기에 MLP 헤드를 연결하여 이미지 분류 작업을 수행합니다. \n",
    "\n",
    "<img src=\"resources/vit_model.png\" style=\"width:400px;\">\n",
    "\n",
    "3. Position embeddings\n",
    "\n",
    "이미지의 위치 정보를 보존하기 위해 학습가능한(learnable) Positional Embedding을 더해줍니다. 이는 모델이 각 패치의 위치를 인식할 수 있도록 하여, 이미지의 공간적 구조를 유지하는 데 도움을 줍니다.\n",
    "$$[x_{class}; x_p^1E; x_p^2E; ... ; x_p^NE] + E_{pos}$$\n",
    "\n",
    "여기서 $E_{pos}$는 $(N+1, D)$의 shape을 가집니다.\n",
    "\n",
    "### <mark>실습 </mark> 아래 구현 과정에 따라 `PatchEmbedding`을 완성하세요.\n",
    "- Linear Projection: 각 패치에 linear 레이어를 적용하여 `embedding_dim`차원을 가지는 embedding vector를 얻습니다.\n",
    "- [Class] Token: 이미지 전체를 대표하는 learnable [class] 토큰을 앞선 embedding vector의 앞에 concat합니다.\n",
    "- Positional Embeddings: 패치들의 위치 정보를 보존하기 위한 Learnable embedding을 더해줍니다.\n",
    "\n",
    "`nn.Parameter`를 이용해 learnable parameter를 만들어 줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Embeds patches, adds classification token and positional embeddings.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_patches, patch_size, embedding_dim, num_channels=3):\n",
    "        super().__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.patch_size = patch_size\n",
    "        self.num_channels = num_channels\n",
    "\n",
    "        # Linear projection of flattened features for each patch\n",
    "        self.linear_projection = nn.Linear(num_channels * patch_size * patch_size, embedding_dim)\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, embedding_dim))\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, embedding_dim))\n",
    "\n",
    "    def forward(self, patches):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patches: Tensor of shape (batch_size, num_patches, num_channels * patch_size * patch_size)\n",
    "        Returns:\n",
    "            embeddings: Tensor of shape (batch_size, num_patches + 1, embedding_dim)\n",
    "        \"\"\"\n",
    "        batch_size = patches.size(0)\n",
    "\n",
    "        # Project patches to embedding dimension\n",
    "        patch_embeddings = self.linear_projection(patches)  # (batch_size, num_patches, embedding_dim)\n",
    "\n",
    "        # Expand class token to batch size\n",
    "        class_token = self.cls_token.expand(batch_size, -1, -1)  # (batch_size, 1, embedding_dim)\n",
    "\n",
    "        ##### YOUR CODE START #####\n",
    "        # Concatenate class token with patch embeddings\n",
    "\n",
    "        # Add positional embeddings\n",
    "        \n",
    "        ##### YOUR CODE END #####\n",
    "\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ViT (Vision Transformer)\n",
    "지금까지 구현한 모듈들을 모두 조합하여 ViT 아키텍쳐를 완성합니다.\n",
    "\n",
    "### <mark>실습 </mark> 아래 과정에 따라 `ViT`를 완성하세요.\n",
    "1. Patchification: `ImagePatchifier`를 사용해 이미지를 시퀀스 형태의 패치들로 변홥합니다.\n",
    "2. Embedding: `PatchEmbedding`모듈을 사용하여 patch embedding을 얻습니다,\n",
    "3. Transformer Encoders: `TransformerEncoder` 레이어를 `num_transformer_layers`만큼 통과시킵니다.\n",
    "4. Classification Head\n",
    "    - [class] token에 대응되는 Transformer Encoder의 출력값을 추출하여, 이미지 전체를 대표하는 임베딩으로 사용합니다.\n",
    "    - 해당 임베딩에 linear layer를 적용하여 이미지 분류를 위한 logit 값을 얻습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    \"\"\"\n",
    "    Vision Transformer (ViT) model.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_size=224,\n",
    "        num_channels=3, \n",
    "        patch_size=16,\n",
    "        num_classes=1000,\n",
    "        hidden_dim=768,\n",
    "        num_transformer_layers=12,\n",
    "        num_head=12,\n",
    "        feedforward_dim=3072,\n",
    "        dropout_prob=0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.patchifier = ImagePatchifier(image_size, patch_size)\n",
    "        self.patch_embedding = PatchEmbedding(self.patchifier.num_patches, patch_size, hidden_dim, num_channels)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "        self.transformer_encoder = nn.Sequential(\n",
    "            *[\n",
    "                TransformerEncoder(hidden_dim, num_head, feedforward_dim, dropout_prob)\n",
    "                for _ in range(num_transformer_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(hidden_dim)\n",
    "        self.classifier = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, images):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            images: Tensor of shape (batch_size, num_channels, image_height, image_width)\n",
    "        Returns:\n",
    "            logits: Tensor of shape (batch_size, num_classes)\n",
    "        \"\"\"\n",
    "        patches = self.patchifier(images) # (batch_size, num_patches, num_channels * patch_size * patch_size)\n",
    "        embeddings = self.patch_embedding(patches)  # (batch_size, 1 + num_patches, hidden_dim)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        \n",
    "        x = self.transformer_encoder(embeddings) # (batch_size, num_patches + 1, embedding_dim)\n",
    "        x = self.layer_norm(x)\n",
    "\n",
    "        ##### YOUR CODE START #####\n",
    "\n",
    "        ##### YOUR CODE END #####\n",
    "  \n",
    "        return logits "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "model = ViT()\n",
    "assert model(torch.randn(4, 3, 224, 224)).shape == torch.Size((4, 1000)), \"output shape does not match\"\n",
    "assert sum(p.numel() for p in model.parameters()) == 86567656, \"Number of model parameter does not match\"\n",
    "\n",
    "print(\"\\033[92m All test passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model_name, num_classes, config):\n",
    "    if model_name == \"ViT\":\n",
    "        model = ViT(image_size=32,\n",
    "                    num_channels=3, \n",
    "                    patch_size=4,\n",
    "                    num_classes=num_classes,\n",
    "                    hidden_dim=256,\n",
    "                    num_transformer_layers=6,\n",
    "                    num_head=4,\n",
    "                    feedforward_dim=1024,\n",
    "                    dropout_prob=0.1,)\n",
    "    else:\n",
    "        raise Exception(\"Model not supported: {}\".format(model_name))\n",
    "    \n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "    print(f\"Using model {model_name} with {total_params} parameters ({trainable_params} trainable)\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_main(config):\n",
    "    ## data and preprocessing settings\n",
    "    data_root_dir = config['data_root_dir']\n",
    "    num_worker = config.get('num_worker', 4)\n",
    "\n",
    "    ## Hyper parameters\n",
    "    batch_size = config['batch_size']\n",
    "    learning_rate = config['learning_rate']\n",
    "    start_epoch = config.get('start_epoch', 0)\n",
    "    num_epochs = config['num_epochs']\n",
    "\n",
    "    ## checkpoint setting\n",
    "    checkpoint_save_interval = config.get('checkpoint_save_interval', 10)\n",
    "    checkpoint_path = config.get('checkpoint_path', \"checkpoints/checkpoint.pth\")\n",
    "    best_model_path = config.get('best_model_path', \"checkpoints/best_model.pth\")\n",
    "    load_from_checkpoint = config.get('load_from_checkpoint', None)\n",
    "\n",
    "    ## variables\n",
    "    best_metric = 0\n",
    "\n",
    "    wandb.init(\n",
    "        project=config[\"wandb_project_name\"],\n",
    "        config=config\n",
    "    )\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "    print(f\"Using {device} device\")\n",
    "\n",
    "    train_dataloader, val_dataloader, test_dataloader, num_classes = load_cifar10_dataloaders(\n",
    "        data_root_dir, device, batch_size = batch_size, num_worker = num_worker)\n",
    "    \n",
    "    model = get_model(model_name = config[\"model_name\"], num_classes= num_classes, config = config).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.1) \n",
    "\n",
    "    if load_from_checkpoint:\n",
    "        load_checkpoint_path = (best_model_path if load_from_checkpoint == \"best\" else checkpoint_path)\n",
    "        start_epoch, best_metric = load_checkpoint(load_checkpoint_path, model, optimizer, scheduler, device)\n",
    "\n",
    "    if config.get('test_mode', False):\n",
    "        # Only evaluate on the test dataset\n",
    "        print(\"Running test evaluation...\")\n",
    "        test_metric = evaluation_loop(model, device, test_dataloader, criterion, phase = \"test\")\n",
    "        print(f\"Test Accuracy: {test_metric}\")\n",
    "        \n",
    "    else:\n",
    "        # Train and validate using train/val datasets\n",
    "        for epoch in range(start_epoch, num_epochs):\n",
    "            train_loop(model, device, train_dataloader, criterion, optimizer, epoch)\n",
    "            val_metric = evaluation_loop(model, device, val_dataloader, criterion, epoch = epoch, phase = \"validation\")\n",
    "            scheduler.step()\n",
    "\n",
    "            if (epoch + 1) % checkpoint_save_interval == 0 or (epoch + 1) == num_epochs:\n",
    "                is_best = val_metric > best_metric\n",
    "                best_metric = max(val_metric, best_metric)\n",
    "                save_checkpoint(checkpoint_path, model, optimizer, scheduler, epoch, best_metric, is_best, best_model_path)\n",
    "\n",
    "\n",
    "\n",
    "    wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    'data_root_dir': '/datasets',\n",
    "    'batch_size': 16,\n",
    "    'learning_rate': 1e-3,\n",
    "    'model_name': 'ViT',\n",
    "    'num_epochs': 2,\n",
    "\n",
    "    \"dataset\": \"CIFAR-10\",\n",
    "    'wandb_project_name': 'ViT-CIFAR',\n",
    "\n",
    "    \"checkpoint_save_interval\" : 10,\n",
    "    \"checkpoint_path\" : \"checkpoints/checkpoint.pth\",\n",
    "    \"best_model_path\" : \"checkpoints/best_model.pth\",\n",
    "    \"load_from_checkpoint\" : None,    # Options: \"latest\", \"best\", or None\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "train_main(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 저장된 checkpoint를 모두 지워 저장공간을 확보한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "import shutil, os\n",
    "if os.path.exists('checkpoints/'):\n",
    "    shutil.rmtree('checkpoints/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2.3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
