{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLO (You Only Look at Once)\n",
    "\"You Only Look Once\" (YOLO) 모델은 object detection을 위한 성능이 좋으면서도 예측이 실시간으로 가능할정도로 빠른 매우 유명한 모델이다. 이 알고리즘은 이미지를 \"단 한번만 보며 (only looks once)\", 즉 단 한번의 forward propagation만으로 예측을 수행한다.\n",
    "\n",
    "이번 실습에서는 YOLO v1 모델을 직접 구현하여 object detection을 수행해본다\n",
    "\n",
    "(YOLOv1에서 target을 수정한 변형된 버전임)\n",
    "\n",
    "<img src=\"resources/yolo_fig1.png\" style=\"width:600px;height:200;\">\n",
    "\n",
    "\n",
    "### inputs and outputs(target)\n",
    "Object detection이란 이미지를 입력으로 받아 물체의 존재유무를 검출하고 검출된 object에 대응하는 bounding box를 찾는 문제이다.\n",
    "아래 이미지는 YOLO의 target을 도식화한 것으로 box의 중심 $b_x$, $b_y$와 너비와 높이 $b_w$와 $b_h$를 찾는것이 목표이다.\n",
    "\n",
    "<img src=\"resources/yolo_box_label.png\" style=\"width:500px;height:250;\">\n",
    "\n",
    "이와 더불어 YOLO 모델에서는 object 존재 유무에 대한 confidence값 $p_c$와 검출된 물체가 어떤 class에 속하는지에 대한 $c$값도 함께 학습한다.\n",
    "\n",
    "만약 검출하고자 하는 물체의 종류가 20가지라면 class label $c$는 0부터 19사이의 정수로 표현될 수도 있고, class에 대응하는 길이 20의 one-hot encoding vector로 표현될 수도 있다.\n",
    "\n",
    "정리하자면 하나의 bounding box에 대한 target $y$는 25개의 숫자 $(x, y, h, w, p_c, c_1, c_2, ..., c_{20})$ 로 표현된다\n",
    "\n",
    "### S x S grid cells\n",
    "\n",
    "YOLO에서는 이미지가 $S \\times S$ 개의 grid cell로 분할되며, 각 grid cell은 box의 개수 $B$ 개만큼의 물체를 검출하는 역할이 주어진다. 즉 YOLO는 최대 $S \\times S \\times B$ 개의 물체를 검출할 수 있다.\n",
    "\n",
    "각 grid cell이 담당하는 물체는 어떻게 할당 될까? 어떤 grid cell이 물체를 둘러싸는 bounding box의 중심을 포함하고 있으면 해당 물체는 그 grid cell에 할당되게 된다. \n",
    "\n",
    "예를 들어 아래 그림에서 개의 중심은 빨간색으로 표기된 grid cell에 포함되므로 해당 grid cell이 개의 검출을 담당하게 된다.\n",
    "\n",
    "<img src=\"resources/yolo_grid_cell.png\" style=\"width:400px;height:200;\">\n",
    "\n",
    "따라서 한개 이지미에 대한 target tensor는 $(S, S, B, 25)$ 의 shape을 같게 되며 $S = 7, B = 2$일때는 아래 그림과 같은 모양을 가진다.\n",
    "\n",
    "<img src=\"resources/yolo_target.jpg\" style=\"width:600px;height:200;\">\n",
    "\n",
    "\n",
    "### Bounding Box representation\n",
    "\n",
    "물체를 둘러싸는 bounding box는 (center x, center y, width, height)의 4개 값으로 표현된다\n",
    "\n",
    "* $x$와 $y$값은 grid cell 내에서의 상대적인 위치 값으로 항상 $[0,1]$사이의 값을 가진다\n",
    "* $w$와 $h$는 grid cell 크기에 대한 상대적인 값으로 1보다 큰 값을 가질 수 있다.\n",
    "\n",
    "아래 이미지에 이에 대한 설명이 잘 표현되어 있다\n",
    "\n",
    "<img src=\"resources/yolo_bb.png\" style=\"width:500px;height:200;\">\n",
    "\n",
    "### Object confidence and class score\n",
    "Object condifence score $p_c$는 물체가 존재할 확률 $P(Object)$에 대한 예측값이다.\n",
    "또한 class score $c_i$는 물체가 존재할 경우 그 물체가 $class_i$일 확률 $P(Class_i|Object)$에 대한 예측 값이다.\n",
    "\n",
    "이 두 값의 곱을 통해서 class-specific confidence score를 계산할 수 있다\n",
    "$$P(Class_i) = P(Class_i|Object) \\times P(Object) = p_c \\times c_i$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets, models\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import wandb\n",
    "import pprint\n",
    "from tqdm import tqdm\n",
    "\n",
    "from training_utilities import create_dataloaders, train_loop, AverageMeter, save_checkpoint, load_checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch에서 제공하는 Pascal VOC 2012 데이터셋을 이용해 이미지와 타겟을 읽어보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "data_root_dir = '/datasets'\n",
    "\n",
    "voc_train_dataset = datasets.VOCDetection(root=data_root_dir, year='2012', image_set='train', download=False,\n",
    "                                          transform = transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래와 같이 이미지와 dictionary로 된 label값이 리턴되는것을 확인할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "print(f\"{i+1}-th example\")\n",
    "print(f\" - image shape: {voc_train_dataset[i][0].shape}. \")\n",
    "print(f\" - label: \")\n",
    "pprint.PrettyPrinter().pprint(voc_train_dataset[i][1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <mark>실습</mark> YOLODataset\n",
    "`VOCDetection` 데이터셋의 이미지와 label을 입력으로 받아 YOLO를 위한 이미지와 YOLO target 텐서를 리턴하는 데이터셋 `YOLODataset`을 완성하라. \n",
    "\n",
    "- 하나의 grid cell에 여러개의 object가 할당되는 경우 처음 B개까지의 물체만 target 텐서에 할당 할 것.\n",
    "- image shape이 (# channel, width, height)가 아니라 (# channel, height(y), width(x))임에 주의하고, target 텐서도 이와 같은 순서로 구성할 것.\n",
    "- transform은 추후에 사용할 것이므로 현재 구현에서는 이미지나 타겟에 적용하지 말것."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YOLODataset(Dataset):\n",
    "    def __init__(self, voc_dataset, S=7, B=2, transform=None):\n",
    "        self.voc_dataset = voc_dataset\n",
    "        self.S = S   # Number of grids (e.g., 7 for YOLOv1)\n",
    "        self.B = B   # Number of bounding boxes per grid cell\n",
    "        self.transform = transform\n",
    "\n",
    "        self.classes = [\"aeroplane\", \"bicycle\", \"bird\", \"boat\", \"bottle\",\n",
    "                        \"bus\", \"car\", \"cat\", \"chair\", \"cow\", \"diningtable\",\n",
    "                        \"dog\", \"horse\", \"motorbike\", \"person\", \"pottedplant\",\n",
    "                        \"sheep\", \"sofa\", \"train\", \"tvmonitor\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.voc_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, label = self.voc_dataset[idx]\n",
    "\n",
    "        objects = label['annotation']['object']\n",
    "        image_width = float(label['annotation']['size']['width'])\n",
    "        image_height = float(label['annotation']['size']['height'])\n",
    "\n",
    "        ##### YOUR CODE START #####\n",
    "        \n",
    "        ##### YOUR CODE END #####\n",
    "\n",
    "        return image, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_VOC_YOLO_datasets(data_root_dir, S, B):\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "    \n",
    "    train_transforms = transforms.Compose([\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ])\n",
    "\n",
    "    test_transforms = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ])\n",
    "    \n",
    "    voc_train_dataset = datasets.VOCDetection(root=data_root_dir, year='2012', image_set='train', download=False,\n",
    "                                              transform = train_transforms)\n",
    "    voc_test_dataset = datasets.VOCDetection(root=data_root_dir, year='2012', image_set='val', download=False,\n",
    "                                             transform = test_transforms)\n",
    "    \n",
    "    train_dataset = YOLODataset(voc_train_dataset, S= S, B = B)\n",
    "    test_dataset = YOLODataset(voc_test_dataset, S= S, B = B)\n",
    "\n",
    "\n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = load_VOC_YOLO_datasets(data_root_dir, S = 7, B = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "i = 3\n",
    "X, y = train_dataset[i]\n",
    "print(f\"{i+1}-th example X.shape : {X.shape}, y.shape: {y.shape}\")\n",
    "print(f\"target value at grid (1, 3) : {y[1][3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output:**\n",
    "```\n",
    "4-th example X.shape : torch.Size([3, 224, 224]), y.shape: torch.Size([7, 7, 2, 25])\n",
    "target value at grid (1, 3) : tensor([[0.3634, 0.8410, 6.4745, 3.6540, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000],\n",
    "        [0.2162, 0.0150, 0.6727, 1.0220, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "         0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]])\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "몇개 이미지와 bounding box를 같이 visualize해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cv2  # You can use PIL too if you prefer\n",
    "\n",
    "def visualize_image_with_bboxes(image, target, S=7, B=2, class_names=None, confidence_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Visualizes an image with bounding boxes drawn over it.\n",
    "    \n",
    "    Args:\n",
    "    - image (Tensor): Image tensor (C, H, W).\n",
    "    - target (Tensor): The corresponding YOLOv1 target tensor (S, S, B * (5 + num_classes)).\n",
    "    - S (int): Grid size (default 7).\n",
    "    - B (int): Number of bounding boxes per grid cell (default 2).\n",
    "    - class_names (list): List of class names.\n",
    "    - confidence_threshold (float): Minimum confidence score to display bounding boxes.\n",
    "    \"\"\"\n",
    "    if isinstance(image, torch.Tensor):\n",
    "        # Undo normalization\n",
    "        mean = torch.tensor([0.485, 0.456, 0.406]).to(image.device).view(-1, 1, 1)\n",
    "        std = torch.tensor([0.229, 0.224, 0.225]).to(image.device).view(-1, 1, 1)\n",
    "        image = image * std + mean\n",
    "\n",
    "        image = image.permute(1, 2, 0).cpu().numpy() # Convert tensor to numpy (C, H, W) to (H, W, C)#\n",
    "\n",
    "    img_copy = (image * 255).astype(np.uint8).copy()\n",
    "\n",
    "    height, width = image.shape[:2]\n",
    "\n",
    "    # Iterate over grid cells\n",
    "    for grid_row in range(S):\n",
    "        for grid_col in range(S):\n",
    "            for box_index in range(B):\n",
    "                # Extract confidence for this box\n",
    "                confidence = target[grid_row, grid_col, box_index, 4]\n",
    "                \n",
    "                if confidence > confidence_threshold:\n",
    "                    # Extract bounding box details\n",
    "                    x_center_cell, y_center_cell, box_width_cell, box_height_cell = target[grid_row, grid_col, box_index, :4]\n",
    "                    \n",
    "                    # Convert the grid-relative coordinates to image coordinates\n",
    "                    x_center = (x_center_cell + grid_col) / S * width\n",
    "                    y_center = (y_center_cell + grid_row) / S * height\n",
    "                    box_width = box_width_cell / S * width\n",
    "                    box_height = box_height_cell / S * height\n",
    "\n",
    "                    # Convert center coordinates to top-left corner (x_min, y_min)\n",
    "                    x_min = int(x_center - box_width / 2)\n",
    "                    y_min = int(y_center - box_height / 2)\n",
    "                    x_max = int(x_center + box_width / 2)\n",
    "                    y_max = int(y_center + box_height / 2)\n",
    "\n",
    "                    # Find the predicted class (one-hot encoded class probabilities)\n",
    "                    class_probs = target[grid_row, grid_col, box_index, 5:]\n",
    "                    class_idx = torch.argmax(class_probs).item()\n",
    "                    class_name = class_names[class_idx]\n",
    "\n",
    "                    # Draw bounding box and label on the image using OpenCV\n",
    "                    cv2.rectangle(img_copy, (x_min, y_min), (x_max, y_max), (255, 0, 0), 2)\n",
    "                    cv2.putText(img_copy, f'{class_name} ({confidence:.2f})', (x_min, y_min - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 1)\n",
    "\n",
    "    # Display the image with bounding boxes\n",
    "    plt.figure() #figsize=(8, 8)\n",
    "    plt.imshow(img_copy)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "for i in range(len(train_dataset)):\n",
    "    image, target = train_dataset[i]\n",
    "    visualize_image_with_bboxes(image, target, S=7, B=2, class_names=train_dataset.classes)\n",
    "    if i == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOLO Loss\n",
    "YOLO loss $L$은 아래와 같이 4개의 요소로 구성되어 있다\n",
    "\n",
    "1. **Localization Loss** (for box coordinates $x$, $y$, $w$, and $h$):\n",
    "   $$\n",
    "   L_{\\text{coord}} = \\lambda_{\\text{coord}} \\sum_{i=0}^{S^2} \\sum_{j=0}^{B} 1_{\\text{obj}}^{ij} \\left[ (x_i - \\hat{x}_i)^2 + (y_i - \\hat{y}_i)^2 \\right] + \\lambda_{\\text{coord}} \\sum_{i=0}^{S^2} \\sum_{j=0}^{B} 1_{\\text{obj}}^{ij} \\left[ \\left(\\sqrt{w_i} - \\sqrt{\\hat{w}_i}\\right)^2 + \\left(\\sqrt{h_i} - \\sqrt{\\hat{h}_i}\\right)^2 \\right]\n",
    "   $$\n",
    "\n",
    "   where:\n",
    "   - $1_{\\text{obj}}^{ij}$ is an indicator function that is 1 if object is present in bounding box $j$ of grid cell $i$.\n",
    "   - $(x_i, y_i, w_i, h_i)$ are the predicted box parameters for box $j$ in grid cell $i$.\n",
    "\n",
    "2. **Object Confidence Loss** (for boxes containing objects):\n",
    "   $$\n",
    "   L_{\\text{obj}} = \\sum_{i=0}^{S^2} \\sum_{j=0}^{B} 1_{\\text{obj}}^{ij} \\left( C_{ij} - \\hat{C}_{ij} \\right)^2\n",
    "   $$\n",
    "\n",
    "   where:\n",
    "   - $C_{ij}$ is the predicted confidence score for bounding box $j$ in grid cell $i$.\n",
    "   - $\\hat{C}_{ij}$ is the ground truth confidence for box $j$.\n",
    "\n",
    "3. **No Object Confidence Loss** (for boxes without objects):\n",
    "   $$\n",
    "   L_{\\text{noobj}} = \\lambda_{\\text{noobj}} \\sum_{i=0}^{S^2} \\sum_{j=0}^{B} 1_{\\text{noobj}}^{ij} \\left( C_{ij} - \\hat{C}_{ij} \\right)^2\n",
    "   $$\n",
    "\n",
    "   where:\n",
    "   - $1_{\\text{noobj}}^{ij}$ is 1 if there is no object in grid cell $i$ and box $j$.\n",
    "\n",
    "4. **Class Probability Loss** (for predicting the correct class probabilities for boxes that contain objects):\n",
    "   $$\n",
    "   L_{\\text{class}} = \\sum_{i=0}^{S^2} \\sum_{j=0}^{B} 1_{\\text{obj}}^{ij} \\sum_{c=0}^{C} \\left( p_{ij}(c) - \\hat{p}_{ij}(c) \\right)^2\n",
    "   $$\n",
    "\n",
    "   where:\n",
    "   - $p_{ij}(c)$ is the predicted probability of class $c$ for bounding box $j$ in grid cell $i$.\n",
    "   - $\\hat{p}_{ij}(c)$ is the one-hot encoded ground truth class probability for box $j$ in grid cell $i$.\n",
    "\n",
    "#### Total Loss:\n",
    "The total loss $L$ is the sum of all the components:\n",
    "$$\n",
    "L = L_{\\text{coord}} + L_{\\text{obj}} + L_{\\text{noobj}} + L_{\\text{class}}\n",
    "$$\n",
    "\n",
    "### <mark>실습</mark> YOLOLoss\n",
    "YOLO loss를 계산하는 `YOLOLoss` 모듈을 완성하라\n",
    "- For loop를 사용하면 계산이 매우 느려지므로 tensor 연산만을 이용하여 구현할것."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YOLOLoss(nn.Module):\n",
    "    def __init__(self, lambda_coord=5, lambda_noobj=0.5):\n",
    "        super().__init__()\n",
    "        self.lambda_coord = lambda_coord  # Weight for localization loss\n",
    "        self.lambda_noobj = lambda_noobj  # Weight for no-object confidence loss\n",
    "        \n",
    "        self.mse = nn.MSELoss(reduction=\"sum\")  # MSE loss for coordinates and confidence\n",
    "\n",
    "    def forward(self, predictions, target):\n",
    "        \"\"\"\n",
    "        predictions and target have shape (batch_size, S, S, B, 5 + C)\n",
    "        elements for each grid cell and box = [x, y, w, h, confidence, class1, class2, ..., classC]\n",
    "        \"\"\"\n",
    "        \n",
    "        # Extract (x, y, w, h, confidence)\n",
    "        pred_xy = predictions[..., :2]  # (x_center, y_center)\n",
    "        target_xy = target[..., :2]  # (x_center, y_center)\n",
    "\n",
    "        pred_wh = torch.sign(predictions[..., 2:4]) * torch.sqrt(torch.abs(predictions[..., 2:4] + 1e-6)) # Retain sign while applying sqrt on width and height\n",
    "        target_wh = target[..., 2:4].sqrt() # sqrt(w), sqrt(h) to stabilize large boxes\n",
    "\n",
    "        ##### YOUR CODE START #####      \n",
    "        \n",
    "        ##### YOUR CODE END #####\n",
    "\n",
    "        return total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "X, target = train_dataset[3]\n",
    "torch.manual_seed(1004) \n",
    "y_pred = torch.randn_like(target)\n",
    "\n",
    "print(f\"Your loss : \", YOLOLoss()(y_pred.unsqueeze(0), target.unsqueeze(0)).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output**:\n",
    "\n",
    "Your loss :  476.0494079589844"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing bounding boxes of YOLO prediction\n",
    "YOLO 예측 결과를 모두 bounding box로 그려보면 아래와 같은 이미지를 얻을 수 있다.\n",
    "\n",
    "<img src=\"resources/yolo_bb_wo_nms.png\" style=\"width:300px;\">\n",
    "\n",
    "만약 모든 grid cell에서 bounding box를 예측한다면 $7\\times7\\times2 = 98$개의 box를 단 한번의 forward pass로 얻을 수 있다.\n",
    "\n",
    "위 이미지를 살펴보면 YOLO예측으로 과하게 많은 bounding box 출력됨을 확인할 수 있다. 이를 줄이기 위해서는 서로 겹치는 bbox를 제거하는 **non-max suppression**이라는 알고리즘이 사용된다.\n",
    "\n",
    "이를 위해서 먼저 IoU (Intersection over Union)값을 계산하는 함수를 구현하자.\n",
    "\n",
    "<img src=\"resources/yolo_iou.png\" style=\"width:600px;\">\n",
    "\n",
    "\n",
    "### <mark>실습</mark> convert_midpoint_to_corner_bboxes\n",
    "[center x, center y, box width, box height]로 bounding box를 표현하는 midpoint coordinate에서 [x_min, y_min, x_max, y_max]로 bounding box를 표현하는 corner coordinate로 변환하는 함수 `convert_midpoint_to_corner_bboxes`를 완성하라\n",
    "\n",
    "- `bboxes_midpoint_coords`의 각 값은 YOLODataset의 target tensor와 달리 **전체 이미지 크기**를 기준으로 normalize되어 모두 [0, 1] 사이의 값을 가진다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_midpoint_to_corner_bboxes(bboxes_midpoint_coords: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Convert bounding boxes from midpoint format [x_center, y_center, width, height]\n",
    "    to corner format [xmin, ymin, xmax, ymax].\n",
    "\n",
    "    Parameters:\n",
    "        bboxes_midpoint_coords: \n",
    "            A tensor of shape (batch_size, 4) containing bounding boxes in midpoint format.\n",
    "            Each bounding box is represented by four values [x_center, y_center, width, height] and all the four values are already normalized relative to the entire image having values between [0, 1].\n",
    "    Returns:\n",
    "        torch.Tensor:\n",
    "            A tensor of shape (batch_size, 4) containing bounding boxes in corner format.\n",
    "            Each bounding box is represented by four values:\n",
    "                - xmin: x-coordinate of the top-left corner (normalized between 0 and 1).\n",
    "                - ymin: y-coordinate of the top-left corner (normalized between 0 and 1).\n",
    "                - xmax: x-coordinate of the bottom-right corner (normalized between 0 and 1).\n",
    "                - ymax: y-coordinate of the bottom-right corner (normalized between 0 and 1).\n",
    "    \n",
    "\n",
    "    Example:\n",
    "        >>> bboxes_midpoint_coords = torch.tensor([[0.5, 0.5, 0.4, 0.4]])\n",
    "        >>> convert_midpoint_to_corner_bboxes(bboxes_midpoint_coords)\n",
    "        tensor([[0.3000, 0.3000, 0.7000, 0.7000]])\n",
    "    \"\"\"\n",
    "\n",
    "    ##### YOUR CODE START #####\n",
    "    \n",
    "    ##### YOUR CODE END #####\n",
    "\n",
    "    return bb_corners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "convert_midpoint_to_corner_bboxes(torch.tensor([[0.5, 0.5, 0.4, 0.4]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output:**\n",
    "```\n",
    "tensor([[0.3000, 0.3000, 0.7000, 0.7000]])\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <mark>실습</mark> Intersection over Union\n",
    "\n",
    "IoU값을 계산하는 함수 `intersection_over_union`를 완성하라\n",
    "- corner coordinate에서는 (0,0) 이 이미지의 왼쪽 위를, (1,0)이 오른쪽 위를, (1,1)이 오른쪽 아래를 지칭한다.\n",
    "- 두 box의 **intersection** $(xi_{1}, yi_{1}, xi_{2}, yi_{2})$를 구하기 위해서는:\n",
    "    - `xi1` = **max**imum of the x1 coordinates of the two boxes\n",
    "    - `yi1` = **max**imum of the y1 coordinates of the two boxes\n",
    "    - `xi2` = **min**imum of the x2 coordinates of the two boxes\n",
    "    - `yi2` = **min**imum of the y2 coordinates of the two boxes\n",
    "- 두 박스가 **intersection이 없을** 수도 있다. 이는 높이 $(yi_2 - yi_1)$ 혹은 너비 $(xi_2 - xi_1)$ 중 하나 이상이 음수인지 여부로 확인할 수 있으며, 이 경우 intersection area값이 0이다.\n",
    "- 두 박스가 **edges 혹은 vertices**에서 만날수도 있다. 이는 높이 $(yi_2 - yi_1)$ 혹은 너비 $(xi_2 - xi_1)$ 값이 0인지 여부로 확인할 수 있으며 이 경우에도 마찬가지로 intersection area값이 0이다.\n",
    "\n",
    "<details>\n",
    "    <summary>Additional Hints</summary>\n",
    "  \n",
    "  intersection area값 계산에 `max(height, 0)` 와 `max(width, 0)` 값을 사용하면 계산이 편리하다\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def intersection_over_union(boxes_preds, boxes_targets):\n",
    "    \"\"\"\n",
    "    Calculate the Intersection over Union (IoU) between predicted and target bounding boxes.\n",
    "    \n",
    "    Parameters:\n",
    "        boxes_preds (torch.Tensor):\n",
    "            Predicted bounding boxes with shape (batch_size, 4) in midpoint format\n",
    "            [x_center, y_center, width, height]. Values are normalized between 0 and 1.\n",
    "        boxes_targets (torch.Tensor):\n",
    "            Ground truth bounding boxes with shape (batch_size, 4) in midpoint format\n",
    "            [x_center, y_center, width, height]. Values are normalized between 0 and 1.\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: Intersection over union for all examples with shape (batch_size, 1)\n",
    "\n",
    "    Note:\n",
    "        - If the boxes do not overlap, the intersection area will be zero, resulting in an IoU of zero.\n",
    "        - A small epsilon (1e-6) is added to the denominator to prevent division by zero.\n",
    "    \"\"\"\n",
    "    \n",
    "    box1 = convert_midpoint_to_corner_bboxes(boxes_preds)\n",
    "    box2 = convert_midpoint_to_corner_bboxes(boxes_targets)\n",
    "\n",
    "    # Obtain x1, y1, x2, y2 tensor with shape (batch_size, 1).\n",
    "    box1_x1 = box1[..., 0:1]\n",
    "    box1_y1 = box1[..., 1:2]\n",
    "    box1_x2 = box1[..., 2:3]\n",
    "    box1_y2 = box1[..., 3:4] \n",
    "    box2_x1 = box2[..., 0:1]\n",
    "    box2_y1 = box2[..., 1:2]\n",
    "    box2_x2 = box2[..., 2:3]\n",
    "    box2_y2 = box2[..., 3:4]\n",
    "\n",
    "    ##### YOUR CODE START #####      \n",
    "    \n",
    "    ##### YOUR CODE END #####\n",
    "\n",
    "    return intersection / (box1_area + box2_area - intersection + 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# BEGIN UNIT TEST\n",
    "## Test case 1: boxes intersect\n",
    "box1 = torch.tensor([[3, 2, 2, 2]]) # torch.tensor([[2, 1, 4, 3]])\n",
    "box2 = torch.tensor([[2, 3, 2, 2]]) #torch.tensor([[1, 2, 3, 4]])\n",
    "\n",
    "print(\"iou for intersecting boxes = \" + str(intersection_over_union(box1, box2)))\n",
    "assert intersection_over_union(box1, box2) < 1, \"The intersection area must be always smaller or equal than the union area.\"\n",
    "assert np.isclose(intersection_over_union(box1, box2), 0.14285714), \"Wrong value. Check your implementation. Problem with intersecting boxes\"\n",
    "\n",
    "## Test case 2: boxes do not intersect\n",
    "box1 = torch.tensor([[2, 3, 2, 2]])# torch.tensor([[1,2,3,4]])\n",
    "box2 = torch.tensor([[6, 7, 2, 2]])# torch.tensor([[5,6,7,8]])\n",
    "print(\"iou for non-intersecting boxes = \" + str(intersection_over_union(box1,box2)))\n",
    "assert intersection_over_union(box1, box2) == 0, \"Intersection must be 0\"\n",
    "\n",
    "## Test case 3: boxes intersect at vertices only\n",
    "box1 = torch.tensor([[1.5, 1.5, 1, 1]]) # torch.tensor([[1,1,2,2]])\n",
    "box2 = torch.tensor([[2.5, 2.5, 1, 1]]) # torch.tensor([[2,2,3,3]])\n",
    "print(\"iou for boxes that only touch at vertices = \" + str(intersection_over_union(box1,box2)))\n",
    "assert intersection_over_union(box1, box2) == 0, \"Intersection at vertices must be 0\"\n",
    "\n",
    "## Test case 4: boxes intersect at edge only\n",
    "box1 = torch.tensor([[2, 2, 2, 2]]) # torch.tensor([[1,1,3,3]])\n",
    "box2 = torch.tensor([[2.5, 3.5, 1, 1]]) # torch.tensor([[2,3,3,4]])\n",
    "print(\"iou for boxes that only touch at edges = \" + str(intersection_over_union(box1,box2)))\n",
    "assert intersection_over_union(box1, box2) == 0, \"Intersection at edges must be 0\"\n",
    "\n",
    "print(\"\\033[92m All tests passed!\")\n",
    "# END UNIT TEST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nom-max suppression (NMS)\n",
    "Nom-max suppression은 크게 두 과정으로 이루어져 있다.\n",
    "\n",
    "1. Object condifence score $p_c$ 가 특정 기준값(confidence_threshold) 보다 낮은 bounding box를 모두 제거한다.\n",
    "2. IoU값이 특정 기준값 (iou_threshold) 이상으로 서로 겹치는 bounding box중 하나만 남기고 모두 제거한다.\n",
    "\n",
    "<img src=\"resources/yolo_non-max-suppression.png\" style=\"width:600px;height:400;\">\n",
    "\n",
    "위 그림을 살펴보면 YOLO model에 3개의 자동차를 검출하였지만 사실 모두 같은 자동차이다. Running non-max suppression (NMS) 알고리즘을 사용하면 이중에 가장 정확한 (highest probability) bounding box를 하나 고르게 된다.\n",
    "\n",
    "좀더 구체적으로는 아래와 같은 과정으로 이루어진다.\n",
    "1. $p_c$ <= `confidence_threshold`인 모든 bbox를 제거한다.\n",
    "2. object confidence score $p_c$에 따라 bbox들을 sorting한다\n",
    "3. 최종 선택된 bounding box를 저장하기 위한 빈 list `bboxes_after_nms`를 초기화한다\n",
    "4. **While** there are still bounding boxes in the sorted bboxes:\n",
    "    - 가장 confidence score가 높은 bounding box를 하나 꺼낸뒤 (pop) `bboxes_after_nms`에 append한다.\n",
    "    - 이 bounding box와 나머지 bbox를 순차적으로 비교한다\n",
    "        - 같은 class를 예측하였으면서 iou >= `iou_threshold` 인 box들을 모두 제거한다.\n",
    "\n",
    "이렇게 함으로써 서로 겹치는 bbox들 중 가장 정확한 bbox만 남길 수 있다.\n",
    "\n",
    "### <mark>실습</mark> Nom-max suppression \n",
    "`non_max_suppression` 함수를 완성하라\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_max_suppression(bboxes, iou_threshold, confidence_threshold):\n",
    "    \"\"\"\n",
    "    Performs Non-Maximum Suppression (NMS) on the predicted bounding boxes to remove redundant overlapping boxes.\n",
    "\n",
    "    The goal of NMS is to retain the most confident bounding box for each detected object and suppress \n",
    "    all other boxes that predict the same object with high overlap (IoU).\n",
    "\n",
    "    Parameters:\n",
    "        bboxes (list): A list of bounding boxes. Each bounding box is represented as:\n",
    "                       [class_pred, prob_score, x, y, w, h]\n",
    "            - class_pred (int): The predicted class index.\n",
    "            - prob_score (float): The object confidence score for the bounding box (probability between 0 and 1).\n",
    "            - x (float): The x-coordinate of the bounding box center (normalized between 0 and 1).\n",
    "            - y (float): The y-coordinate of the bounding box center (normalized between 0 and 1).\n",
    "            - w (float): The width of the bounding box (normalized between 0 and 1).\n",
    "            - h (float): The height of the bounding box (normalized between 0 and 1).\n",
    "        iou_threshold (float): The IoU threshold for suppressing overlapping boxes.\n",
    "                               Bounding boxes with IoU >= this threshold and the same class will be suppressed.\n",
    "        confidence_threshold (float): The minimum confidence score to keep a bounding box.\n",
    "                                      Bounding boxes with confidence scores below this threshold will be discarded.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of bounding boxes after NMS, in the same format as the input, containing only the most confident,\n",
    "              non-overlapping bounding boxes for each object.\n",
    "    \"\"\"\n",
    "\n",
    "    assert type(bboxes) == list\n",
    "\n",
    "    ##### YOUR CODE START #####      \n",
    "    \n",
    "    ##### YOUR CODE END #####\n",
    "\n",
    "    return bboxes_after_nms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# unit test \n",
    "torch.manual_seed(0)\n",
    "class_preds = torch.randint(20, (64,1))\n",
    "scores = torch.rand((64,1))\n",
    "boxes = torch.rand(64, 4)\n",
    "nms_input = torch.cat((class_preds, scores, boxes), dim = 1).tolist()\n",
    "\n",
    "nms_result = non_max_suppression(nms_input, 0.5, 0.5)\n",
    "assert len(nms_result) == 33\n",
    "assert torch.isclose(torch.tensor(nms_result[10]), torch.tensor([19.0, 0.8369089365, 0.9384382367, 0.1752943992, 0.44311922788, 0.6432467699])).all()\n",
    "\n",
    "nms_result = non_max_suppression(nms_input, 0.5, 0.6)\n",
    "assert len(nms_result) == 25\n",
    "\n",
    "nms_result = non_max_suppression(nms_input, 0.2, 0.6)\n",
    "assert len(nms_result) == 22\n",
    "\n",
    "print(\"\\033[92m All tests passed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOLOv1 논문에서 제안한 Network 구조는 아래 그림과 같다.\n",
    "\n",
    "<img src=\"resources/yolo_model.png\" style=\"width:800px\">\n",
    "\n",
    "하지만 우리는 CNN으로 pre-trained resnet18을 사용할 것이다.\n",
    "\n",
    "<mark>과제</mark> 아래 `YOLOv1Resnet18` 모델의 `fc_layers`구현을 완성하라.\n",
    "fc_layer는 아래와 같이 이루어져 있다.\n",
    "\n",
    "1. Linear layer with 4096 outputs\n",
    "2. [LeakyReLU](https://pytorch.org/docs/stable/generated/torch.nn.LeakyReLU.html) with negative slope 0.1\n",
    "3. Linear layer with output size fitted for YOLO target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YOLOv1Resnet18(nn.Module):\n",
    "    def __init__(self, S=7, B=2, C=20):\n",
    "        super().__init__()\n",
    "        self.S = S  # Grid size\n",
    "        self.B = B  # Number of bounding boxes per grid cell\n",
    "        self.C = C  # Number of classes\n",
    "\n",
    "        resnet = models.resnet18(weights = \"IMAGENET1K_V1\")\n",
    "        self.backbone = nn.Sequential(*list(resnet.children())[:-2]) # Remove the AdaptiveAvgPool2d and fc layers from ResNet\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((self.S, self.S))\n",
    "        \n",
    "        ##### YOUR CODE START #####\n",
    "        self.fc_layers = None\n",
    "        ##### YOUR CODE END #####\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.adaptive_pool(x) # Downsample to (S, S) = (7, 7)\n",
    "        x = self.fc_layers(x)\n",
    "        x = x.view(-1, self.S, self.S, self.B, 5 + self.C) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "model = YOLOv1Resnet18(S = 7, B = 2, C = 20)\n",
    "output = model(torch.randn(64, 3, 224, 224))\n",
    "assert output.shape == torch.Size((64, 7, 7, 2, 25))\n",
    "print(\"\\033[92m All tests passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model_name, num_classes, config):\n",
    "    if model_name == \"YOLOv1Resnet18\":\n",
    "        model = YOLOv1Resnet18(S = config[\"S\"], B = config[\"B\"], C = num_classes)\n",
    "    else:\n",
    "        raise Exception(\"Model not supported: {}\".format(model_name))\n",
    "    \n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "    print(f\"Using model {model_name} with {total_params} parameters ({trainable_params} trainable)\")\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델 평가를 위한 다른 함수들도 정의해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cellboxes_to_boxes(out, S=7, B=2):\n",
    "    \"\"\"\n",
    "    Convert YOLO output tensors into list of bounding boxes relative to the entire image.\n",
    "    This function is vectorized implementation of function cellboxes_to_boxes_naive. See cellboxes_to_boxes_naive function if you have trouble understanding this function\n",
    "\n",
    "    Parameters:\n",
    "    - out: Tensor of shape (batch_size, S, S, B, 25), where each bounding box has 25 attributes.\n",
    "    - S: The number of grid cells (e.g., 7 for a 7x7 grid).\n",
    "    - B: The number of bounding boxes per grid cell.\n",
    "\n",
    "    Returns:\n",
    "    - all_bboxes: List of bounding boxes for each example in the batch. Each bounding box is represented\n",
    "                  as [class_index, confidence_score, x_center, y_center, width, height], all normalized\n",
    "                  between 0 and 1 relative to the entire image.\n",
    "    \"\"\"\n",
    "    batch_size = out.shape[0]\n",
    "\n",
    "    # Extract the bounding box coordinates (x, y, w, h) from the output tensor\n",
    "    bboxes = out[..., :4]  # Shape: (batch_size, S, S, B, 4)\n",
    "\n",
    "    # Create a grid of cell indices to adjust x and y coordinates\n",
    "    # cell_x and cell_y help in adjusting the coordinates from cell-space to image-space\n",
    "    cell_indices = torch.arange(S).float()\n",
    "    cell_x = cell_indices.view(1, 1, S, 1, 1).repeat(batch_size, S, 1, B, 1)  # Shape: (batch_size, S, S, B, 1)\n",
    "    cell_y = cell_indices.view(1, S, 1, 1, 1).repeat(batch_size, 1, S, B, 1)  # Shape: (batch_size, S, S, B, 1)\n",
    "\n",
    "    # Adjust x and y coordinates relative to the entire image\n",
    "    x = (bboxes[..., 0:1] + cell_x) / S  # x_center adjusted to image scale\n",
    "    y = (bboxes[..., 1:2] + cell_y) / S  # y_center adjusted to image scale\n",
    "\n",
    "    # Adjust width and height relative to the entire image\n",
    "    w_h = bboxes[..., 2:4] / S  # width and height adjusted to image scale\n",
    "\n",
    "    # Combine adjusted x, y, w, h into one tensor\n",
    "    converted_bboxes = torch.cat((x, y, w_h), dim=-1)  # Shape: (batch_size, S, S, B, 4)\n",
    "\n",
    "    # Get the predicted class index and confidence score for each bounding box\n",
    "    predicted_class = out[..., 5:].argmax(-1).unsqueeze(-1).float()  # Shape: (batch_size, S, S, B, 1)\n",
    "    confidence = out[..., 4:5]  # Shape: (batch_size, S, S, B, 1)\n",
    "\n",
    "    # Concatenate class index, confidence score, and bounding box coordinates\n",
    "    converted_pred = torch.cat((predicted_class, confidence, converted_bboxes), dim=-1)  # Shape: (batch_size, S, S, B, 6)\n",
    "\n",
    "    # Reshape to (batch_size, S*S*B, 6) for easier processing\n",
    "    converted_pred = converted_pred.view(batch_size, S*S*B, 6)\n",
    "\n",
    "    # Convert the tensor to a list of bounding boxes for each example in the batch\n",
    "    all_bboxes = []\n",
    "    for ex_idx in range(batch_size):\n",
    "        bboxes = converted_pred[ex_idx].tolist()  # Convert to a list\n",
    "        all_bboxes.append(bboxes)\n",
    "\n",
    "    return all_bboxes\n",
    "\n",
    "def cellboxes_to_boxes_naive(output, S=7, B = 2):\n",
    "    all_boxes = []\n",
    "    for i in range(output.shape[0]):\n",
    "        boxes = []\n",
    "        for row in range(S):\n",
    "            for col in range(S):\n",
    "                for b in range(B):\n",
    "                    pred_box = output[i, row, col, b, :4].detach().cpu().numpy()  # [x, y, w, h]\n",
    "                    pred_conf = output[i, row, col, b, 4].detach().cpu().numpy().item()  # confidence\n",
    "                    pred_class = output[i, row, col, b, 5:].argmax().item()  # predicted class\n",
    "                    # Adjust x_center and y_center to be relative to the whole image\n",
    "                    x_center = (pred_box[0] + col) / S  # Scale x_center to image space\n",
    "                    y_center = (pred_box[1] + row) / S  # Scale y_center to image space\n",
    "                    box_width = pred_box[2] / S  # Scale width to image space\n",
    "                    box_height = pred_box[3] / S  # Scale height to image space\n",
    "\n",
    "                    # Append the adjusted bounding box and class\n",
    "                    boxes.append([pred_class, pred_conf, x_center, y_center, box_width, box_height])\n",
    "        all_boxes.append(boxes)\n",
    "    return all_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def mean_average_precision(\n",
    "    pred_boxes, true_boxes, iou_threshold=0.5, num_classes=20\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculates mean average precision \n",
    "    Parameters:\n",
    "        pred_boxes (list): list of lists containing all bboxes with each bboxes\n",
    "        specified as [train_idx, class_prediction, prob_score, x, y, w, h]\n",
    "        true_boxes (list): list of ground truth boudning boxes\n",
    "        iou_threshold (float): minimum iou required for bbox to be correct\n",
    "        num_classes (int): number of classes\n",
    "    Returns:\n",
    "        mAP (float): mAP across \"all\" classes\n",
    "    \"\"\"\n",
    "\n",
    "    # list storing all AP for respective classes\n",
    "    average_precisions = []\n",
    "\n",
    "    # used for numerical stability later on\n",
    "    epsilon = 1e-6\n",
    "\n",
    "    for c in range(num_classes):\n",
    "        detections = []\n",
    "        ground_truths = []\n",
    "\n",
    "        # Go through all predictions and targets,\n",
    "        # and only add the ones that belong to the\n",
    "        # current class c\n",
    "        for detection in pred_boxes:\n",
    "            if detection[1] == c:\n",
    "                detections.append(detection)\n",
    "\n",
    "        for true_box in true_boxes:\n",
    "            if true_box[1] == c:\n",
    "                ground_truths.append(true_box)\n",
    "\n",
    "        # find the amount of bboxes for each training example\n",
    "        # Counter here finds how many ground truth bboxes we get\n",
    "        # for each training example, so let's say img 0 has 3,\n",
    "        # img 1 has 5 then we will obtain a dictionary with:\n",
    "        # amount_bboxes = {0:3, 1:5}\n",
    "        amount_bboxes = Counter([gt[0] for gt in ground_truths])\n",
    "\n",
    "        # We then go through each key, val in this dictionary\n",
    "        # and convert to the following (w.r.t same example):\n",
    "        # ammount_bboxes = {0:torch.tensor[0,0,0], 1:torch.tensor[0,0,0,0,0]}\n",
    "        for key, val in amount_bboxes.items():\n",
    "            amount_bboxes[key] = torch.zeros(val)\n",
    "\n",
    "        # sort by box probabilities which is index 2\n",
    "        detections.sort(key=lambda x: x[2], reverse=True)\n",
    "        TP = torch.zeros((len(detections)))\n",
    "        FP = torch.zeros((len(detections)))\n",
    "        total_true_bboxes = len(ground_truths)\n",
    "        \n",
    "        # If none exists for this class then we can safely skip\n",
    "        if total_true_bboxes == 0:\n",
    "            continue\n",
    "\n",
    "        for detection_idx, detection in enumerate(detections):\n",
    "            # Only take out the ground_truths that have the same\n",
    "            # training idx as detection\n",
    "            ground_truth_img = [\n",
    "                bbox for bbox in ground_truths if bbox[0] == detection[0]\n",
    "            ]\n",
    "\n",
    "            num_gts = len(ground_truth_img)\n",
    "            best_iou = 0\n",
    "\n",
    "            for idx, gt in enumerate(ground_truth_img):\n",
    "                iou = intersection_over_union(\n",
    "                    torch.tensor(detection[3:]),\n",
    "                    torch.tensor(gt[3:]),\n",
    "                )\n",
    "\n",
    "                if iou > best_iou:\n",
    "                    best_iou = iou\n",
    "                    best_gt_idx = idx\n",
    "\n",
    "            if best_iou > iou_threshold:\n",
    "                # only detect ground truth detection once\n",
    "                if amount_bboxes[detection[0]][best_gt_idx] == 0:\n",
    "                    # true positive and add this bounding box to seen\n",
    "                    TP[detection_idx] = 1\n",
    "                    amount_bboxes[detection[0]][best_gt_idx] = 1\n",
    "                else:\n",
    "                    FP[detection_idx] = 1\n",
    "\n",
    "            # if IOU is lower then the detection is a false positive\n",
    "            else:\n",
    "                FP[detection_idx] = 1\n",
    "\n",
    "        TP_cumsum = torch.cumsum(TP, dim=0)\n",
    "        FP_cumsum = torch.cumsum(FP, dim=0)\n",
    "        recalls = TP_cumsum / (total_true_bboxes + epsilon)\n",
    "        precisions = torch.divide(TP_cumsum, (TP_cumsum + FP_cumsum + epsilon))\n",
    "        precisions = torch.cat((torch.tensor([1]), precisions))\n",
    "        recalls = torch.cat((torch.tensor([0]), recalls))\n",
    "        # torch.trapz for numerical integration\n",
    "        average_precisions.append(torch.trapz(precisions, recalls))\n",
    "\n",
    "    return sum(average_precisions) / len(average_precisions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`evaluation_loop`는 아래와 같이 구성되었다.\n",
    "1. Forward pass\n",
    "2. output과 target 텐서로 부터 predicted bbox들과 true bbox들을 얻는다.\n",
    "3. predicted bbox에 대해 NMS 알고리즘을 적용한다.\n",
    "4. mAP를 계산한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_loop(model, device, dataloader, criterion, epoch = 0, phase = \"validation\", iou_threshold=0.5, conf_threshold=0.4):\n",
    "\n",
    "    losses = AverageMeter('Loss', ':.4e')\n",
    "    metrics_list = [losses]\n",
    "\n",
    "    model.eval() # switch to evaluate mode\n",
    "\n",
    "    all_pred_boxes = []\n",
    "    all_true_boxes = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        progress_bar = tqdm(dataloader, desc=f'{phase.capitalize()} Epoch {epoch+1}', total=len(dataloader))\n",
    "\n",
    "        for images, target in progress_bar:\n",
    "            images = images.to(device, non_blocking=True)\n",
    "            target = target.to(device, non_blocking=True)\n",
    "\n",
    "            output = model(images)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            batch_size = output.shape[0]\n",
    "            pred_boxes = cellboxes_to_boxes(output.to(\"cpu\"), S = model.S, B = model.B)\n",
    "            true_boxes = cellboxes_to_boxes(target.to(\"cpu\"), S = model.S, B = model.B)\n",
    "            for i in range(batch_size): \n",
    "                pred_nms_boxes = non_max_suppression(pred_boxes[i], iou_threshold=iou_threshold, \n",
    "                                                     confidence_threshold =conf_threshold)\n",
    "                all_pred_boxes.extend([[losses.count + i] + box for box in pred_nms_boxes])  # [image_idx, class_pred, conf, x, y, w, h]\n",
    "                all_true_boxes.extend([[losses.count + i] + box for box in true_boxes[i] if box[1] > conf_threshold])  # [image_idx, class_true, x, y, w, h]\n",
    "\n",
    "            losses.update(loss.item(), images.size(0))\n",
    "\n",
    "\n",
    "            progress_bar.set_postfix(avg_metrics = \", \".join([str(x) for x in metrics_list]))\n",
    "\n",
    "        progress_bar.close()\n",
    "\n",
    "    num_classes = len(model.classes) if hasattr(model, 'classes') else 20  # Adjust as needed\n",
    "\n",
    "    mAP = mean_average_precision(all_pred_boxes, all_true_boxes, \n",
    "                                 iou_threshold=iou_threshold, num_classes=num_classes)\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1} {phase.capitalize()} Loss: {losses.avg:.4f}, mAP: {mAP:.4f}\")\n",
    "\n",
    "    wandb.log({\n",
    "        \"epoch\" : epoch,\n",
    "        f\"{phase.capitalize()} Loss\": losses.avg, \n",
    "        f\"{phase.capitalize()} mAP\": mAP\n",
    "    })\n",
    "\n",
    "    return mAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_main(config):\n",
    "    ## data and preprocessing settings\n",
    "    data_root_dir = config['data_root_dir']\n",
    "    num_worker = config.get('num_worker', 4)\n",
    "\n",
    "    ## Hyper parameters\n",
    "    batch_size = config['batch_size']\n",
    "    learning_rate = config['learning_rate']\n",
    "    start_epoch = config.get('start_epoch', 0)\n",
    "    num_epochs = config['num_epochs']\n",
    "    S = config['S']\n",
    "    B = config['B']\n",
    "    eval_interval = config.get('eval_interval', 10)\n",
    "\n",
    "\n",
    "    ## checkpoint setting\n",
    "    checkpoint_path = config.get('checkpoint_path', \"checkpoints/checkpoint.pth\")\n",
    "    best_model_path = config.get('best_model_path', \"checkpoints/best_model.pth\")\n",
    "    load_from_checkpoint = config.get('load_from_checkpoint', None)\n",
    "\n",
    "    ## variables\n",
    "    best_metric = 0\n",
    "\n",
    "    ## set learning deterministic\n",
    "    # torch.manual_seed(1)\n",
    "\n",
    "    wandb.init(\n",
    "        project=config[\"wandb_project_name\"],\n",
    "        config=config\n",
    "    )\n",
    "\n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "    print(f\"Using {device} device\")\n",
    "\n",
    "    train_dataset, test_dataset = load_VOC_YOLO_datasets(data_root_dir, S = S, B = B)\n",
    "    num_classes = len(train_dataset.classes)\n",
    "    \n",
    "    train_dataloader, test_dataloader = create_dataloaders(train_dataset, test_dataset, device, \n",
    "                                                           batch_size = batch_size, num_worker = num_worker)\n",
    "\n",
    "\n",
    "    \n",
    "    model = get_model(model_name = config[\"model_name\"], num_classes= num_classes, config = config).to(device)\n",
    "\n",
    "    criterion = YOLOLoss(lambda_coord=5, lambda_noobj=0.5)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1) \n",
    "\n",
    "    if load_from_checkpoint:\n",
    "        load_checkpoint_path = (best_model_path if load_from_checkpoint == \"best\" else checkpoint_path)\n",
    "        start_epoch, best_metric = load_checkpoint(load_checkpoint_path, model, optimizer, scheduler, device)\n",
    "\n",
    "    if config.get('test_mode', False):\n",
    "        # Only evaluate on the test dataset\n",
    "        print(\"Running test evaluation...\")\n",
    "        test_mAP = evaluation_loop(model, device, test_dataloader, criterion, phase = \"test\",\n",
    "                                   iou_threshold=0.5, conf_threshold=0.6)\n",
    "        print(f\"Test mAP: {test_mAP}\")\n",
    "        \n",
    "    else:\n",
    "        # Train and validate using train/val datasets\n",
    "        for epoch in range(start_epoch, num_epochs):\n",
    "            train_loop(model, device, train_dataloader, criterion, optimizer, epoch)\n",
    "\n",
    "\n",
    "            if (epoch + 1) % eval_interval == 0 or (epoch + 1) == num_epochs:\n",
    "                test_mAP = evaluation_loop(model, device, test_dataloader, criterion, epoch = epoch, phase = \"validation\", iou_threshold = 0.5, conf_threshold = 0.6)\n",
    "\n",
    "                is_best = test_mAP > best_metric\n",
    "                best_metric = max(test_mAP, best_metric)\n",
    "                save_checkpoint(checkpoint_path, model, optimizer, scheduler, epoch, best_metric, is_best, best_model_path)\n",
    "\n",
    "            scheduler.step()\n",
    "\n",
    "\n",
    "    wandb.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training (모델 학습)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    'data_root_dir': '/datasets',\n",
    "    'batch_size': 64,\n",
    "    'learning_rate': 1e-4,\n",
    "    'model_name': 'YOLOv1Resnet18',\n",
    "    'S' : 7,\n",
    "    'B' : 2,\n",
    "    'num_epochs': 10,\n",
    "    \"eval_interval\" : 10,\n",
    "\n",
    "    \"dataset\": \"VOC2012\",\n",
    "    'wandb_project_name': 'YOLOv1',\n",
    "\n",
    "    \"checkpoint_path\" : \"checkpoints/checkpoint.pth\",\n",
    "    \"best_model_path\" : \"checkpoints/best_model.pth\",\n",
    "    \"load_from_checkpoint\" : None,    # Options: \"latest\", \"best\", or None\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "train_main(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize your model's prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predicted_boxes(image, pred_boxes, class_names=None):\n",
    "    \"\"\"\n",
    "    Visualizes an image with predicted bounding boxes drawn over it.\n",
    "\n",
    "    Args:\n",
    "    - image (Tensor): Image tensor (C, H, W).\n",
    "    - pred_boxes (list): List of predicted bounding boxes for the image. Each box is [class_idx, confidence_score, x_center, y_center, width, height].\n",
    "    - class_names (list): List of class names.\n",
    "    \"\"\"\n",
    "    if isinstance(image, torch.Tensor):\n",
    "        # Undo normalization\n",
    "        mean = torch.tensor([0.485, 0.456, 0.406]).view(-1, 1, 1)\n",
    "        std = torch.tensor([0.229, 0.224, 0.225]).view(-1, 1, 1)\n",
    "        image = image * std + mean\n",
    "\n",
    "        # Clamp values to [0,1] after denormalization\n",
    "        image = torch.clamp(image, 0, 1)\n",
    "\n",
    "        # Convert tensor to numpy array and change from (C, H, W) to (H, W, C)\n",
    "        image = image.permute(1, 2, 0).numpy()\n",
    "\n",
    "    img_copy = (image * 255).astype(np.uint8).copy()\n",
    "\n",
    "    height, width = image.shape[:2]\n",
    "\n",
    "    # Draw bounding boxes on the image\n",
    "    for box in pred_boxes:\n",
    "        class_idx, confidence, x_center, y_center, box_width, box_height = box\n",
    "        class_idx = int(class_idx)\n",
    "        x_center *= width\n",
    "        y_center *= height\n",
    "        box_width *= width\n",
    "        box_height *= height\n",
    "\n",
    "        x_min = int(x_center - box_width / 2)\n",
    "        y_min = int(y_center - box_height / 2)\n",
    "        x_max = int(x_center + box_width / 2)\n",
    "        y_max = int(y_center + box_height / 2)\n",
    "\n",
    "        # Ensure coordinates are within image bounds\n",
    "        x_min = max(0, x_min)\n",
    "        y_min = max(0, y_min)\n",
    "        x_max = min(width, x_max)\n",
    "        y_max = min(height, y_max)\n",
    "\n",
    "        # Get class name\n",
    "        class_name = class_names[class_idx] if class_names else f'Class {class_idx}'\n",
    "        label = f'{class_name} ({confidence:.2f})'\n",
    "\n",
    "        (text_width, text_height), baseline = cv2.getTextSize(\n",
    "            label, cv2.FONT_HERSHEY_SIMPLEX, fontScale=0.4, thickness=1\n",
    "        )\n",
    "        text_x = x_min + 5\n",
    "        text_y = min(y_min + text_height + 5, y_max)\n",
    "\n",
    "        # Draw bounding box and label on the image using OpenCV\n",
    "        cv2.rectangle(img_copy, (x_min, y_min), (x_max, y_max), (255, 0, 0), 2)\n",
    "        cv2.putText(img_copy, label, (text_x, text_y),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 0, 0), 1)\n",
    "\n",
    "    # Display the image with bounding boxes\n",
    "    plt.figure()\n",
    "    plt.imshow(img_copy)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "data_root_dir = config['data_root_dir']\n",
    "train_dataset, test_dataset = load_VOC_YOLO_datasets(data_root_dir, S=config['S'], B=config['B'])\n",
    "\n",
    "_, test_dataloader = create_dataloaders(train_dataset, test_dataset,\n",
    "                                        device = device,\n",
    "                                        batch_size=4, \n",
    "                                        num_worker=config.get('num_worker', 4))\n",
    "\n",
    "num_classes = len(train_dataset.classes)\n",
    "class_names = train_dataset.classes\n",
    "\n",
    "model = get_model(model_name=config[\"model_name\"], num_classes=num_classes, config=config)\n",
    "model.to(device)\n",
    "\n",
    "model_checkpoint_path = config[\"best_model_path\"]\n",
    "checkpoint = torch.load(model_checkpoint_path, map_location=device)\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "print(f\"=> loaded checkpoint '{model_checkpoint_path}' with mAP {checkpoint['best_metric']} (epoch {checkpoint['epoch']})\")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "print(\"Model ready for inference\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test dataset에서 일부 이미지를 랜덤하게 불러와서 평가해보자.\n",
    "\n",
    "학습을 오래하지 않았지만 모델이 어느정도는 성능을 보이기 시작하는것을 확인할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "images, targets = next(iter(test_dataloader))\n",
    "images = images.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(images)\n",
    "\n",
    "print(outputs.shape, targets.shape)\n",
    "pred_boxes = cellboxes_to_boxes(outputs.cpu(), S=config['S'], B=config['B'])\n",
    "true_boxes = cellboxes_to_boxes(targets.cpu(), S=config['S'], B=config['B'])\n",
    "\n",
    "print(torch.tensor(pred_boxes).shape, torch.tensor(true_boxes).shape)\n",
    "for i in range(images.shape[0]):\n",
    "    image = images[i].cpu()\n",
    "    true_boxes_with_obj = [box for box in true_boxes[i] if box[1] > 0.5]\n",
    "    pred_nms_boxes = non_max_suppression(pred_boxes[i], iou_threshold=0.5, confidence_threshold=0.5)\n",
    "    visualize_predicted_boxes(image, pred_nms_boxes, class_names=class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 최신 YOLO model을 이용한 inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "! pip install ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from ultralytics import YOLO\n",
    "\n",
    "def process_image_with_yolo(model, image_path):\n",
    "    \"\"\"\n",
    "    Process a single image using the YOLO model and draw bounding boxes.\n",
    "\n",
    "    Args:\n",
    "        image (np.ndarray): The image to process (numpy array in BGR format).\n",
    "        model (YOLO): The YOLO model.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The image with bounding boxes drawn.\n",
    "    \"\"\"\n",
    "    image = cv2.imread(image_path)\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    results = model.predict(source=image_rgb, verbose=False)\n",
    "\n",
    "    result = results[0]\n",
    "\n",
    "    # Get boxes, confidences, and class IDs\n",
    "    boxes = result.boxes.xyxy.cpu().numpy()  # x1, y1, x2, y2\n",
    "    confidences = result.boxes.conf.cpu().numpy()\n",
    "    class_ids = result.boxes.cls.cpu().numpy().astype(int)\n",
    "\n",
    "    # Get class names\n",
    "    class_names = model.names  # List of class names\n",
    "\n",
    "    # Draw bounding boxes and labels on the image\n",
    "    for box, confidence, class_id in zip(boxes, confidences, class_ids):\n",
    "        x1, y1, x2, y2 = box.astype(int)\n",
    "        label = f\"{class_names[class_id]}: {confidence:.2f}\"\n",
    "        # Draw rectangle on the image\n",
    "        cv2.rectangle(image_rgb, (x1, y1), (x2, y2), color=(0, 255, 0), thickness=2)\n",
    "        # Put label near the bounding box\n",
    "        cv2.putText(image_rgb, label, (x1, max(y1 - 10, 0)), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    fontScale=0.5, color=(0, 255, 0), thickness=2)\n",
    "    return image_rgb  # Return RGB image for display with matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "model = YOLO('yolov8n.pt')  # You can choose other versions as needed\n",
    "image_path = 'resources/road.jpg'  # Replace with your video path or use 0 for webcam\n",
    "image_with_detections = process_image_with_yolo(model, image_path)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(image_with_detections)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 선택 사항 (optional)\n",
    "\n",
    "image와 target을 모두 transform하도록 dataloader를 수정하여 ([docs](https://pytorch.org/vision/main/auto_examples/transforms/plot_transforms_getting_started.html#getting-started-with-transforms-v2) 참조) 더 다양한 data augmentation을 수행해보라.\n",
    "Model 구조를 변경하거나 hyperparameter tuning을 수행하여 test set mAP 40% 이상을 달성해보라\n",
    "\n",
    "<mark>주의</mark> 추가 실습을 하기 전 완성한 과제를 git push하여, 추가실험으로 인한 변화가 제출되지 않도록 하자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lab을 마무리 짓기 전 저장된 checkpoint를 모두 지워 저장공간을 확보한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "import shutil, os\n",
    "if os.path.exists('checkpoints/'):\n",
    "    shutil.rmtree('checkpoints/')\n",
    "if os.path.exists('yolov8n.pt'):\n",
    "    os.remove('yolov8n.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2.3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
