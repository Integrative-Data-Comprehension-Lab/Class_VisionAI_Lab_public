{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# explanable AI (XAI)\n",
    "딥러닝 모델은 수 많은 매개변수(parameter)와 비선형 연산으로 이루어져 있으며, 이는 본질적으로 블랙박스(Black Box)입니다. 즉, 모델의 내부의 동작 원리를 알기 어려운 구조입니다.\n",
    "\n",
    "이 때문에 실제 현장에서 딥러닝 모델을 사용할 때, 그 결과를 신뢰할 수 있을지에 대한 의문이 생깁니다.\n",
    "\n",
    "설명 가능한 인공지능(XAI)은 이러한 문제를 해결하고자 모델의 내부 작동 방식과 결과를 인간이 이해하고 검증할 수 있도록 도와줍니다. 이를 통해 모델의 예측 결과에 대한 신뢰성을 높이고, 모델 개선 방향을 제시하는 데 유용합니다.\n",
    "\n",
    "그중에서도 시각화(visualization)를 통한 설명 방법은 인간이 모델의 내부 동작 원리를 보다 쉽게 이해하고 검증할 수 있도록 도와줍니다.\n",
    "\n",
    "# Class activation Map(CAM)\n",
    "클래스 활성화 맵(Class activation Map)은 [2016년 논문](http://cnnlocalization.csail.mit.edu/Zhou_Learning_Deep_Features_CVPR_2016_paper.pdf)에서 제안된 개념으로, CNN 모델이 특정 클래스(class)로 분류하는것에 기여한 입력 이미지의 영역을 시각화 하는 방법입니다.\n",
    "\n",
    "일반적으로 CNN 모델은 마지막 Convolution 레이어 뒤에 Global average pooling (GAP) 레이어를 배치하여 feature map의 공간적 평균을 계산합니다. \n",
    "\n",
    "<img src=\"resources/GAP.png\" style=\"width:400px\">\n",
    "\n",
    "\n",
    "| Layer | output shape |\n",
    "|------|----------|\n",
    "| input | 3x224x224  | \n",
    "|Conv|512x7x7 | \n",
    "|GAP|512x1x1|  \n",
    "| fc |1000 |   \n",
    "\n",
    "\n",
    "\n",
    "GAP 레이어의 주요 장점은 다음과 같습니다:\n",
    " - Translational Invariance: 물체가 이미지 내에서 어디에 위치하는지보다는 물체가 존재하는지 여부를 판단할 수 있도록 도와줍니다.\n",
    " - 차원 축소(Dimensionality Reduction): fc레이어 필요한 파라미터 수를 크게 줄여 오버피팅을 방지합니다. feature들의 공간적 평균을 사용해 정보 손실을 최소화하면서도 높은 분류 성능을 유지합니다\n",
    " - 해석 가능성 (Interpretable Features): GAP 레이어의 출력 벡터는 feature map의 공간적 평균을 나타내며, fc레이어의 가중치(weight) 값의 크기를 통해 각 feature들이 모델의 예측에 얼마나 기여하였는지(중요도)를 파악할 수 있습니다.\n",
    "\n",
    "<img src=\"resources/CAM.png\" style=\"width:800px\">\n",
    "\n",
    "Class activation Map (CAM) 아래 수식에 따라 계산됩니다.\n",
    "\n",
    "마지막 conv layer의 공간적 위치 $(x, y)$에서 $k$번째 채널(activation map)의 출력을 $f_k(x, y)$라 하겠습니다.\n",
    "\n",
    "이때, k번째 activation map의 global average pooling $F_k$은 아래와 같이 계산됩니다:\n",
    "$$F_k = \\sum_{x, y} f_k(x, y)$$\n",
    "\n",
    "class $c$와 $F_k$에 대응하는 fc레이어의 가중치를 $w_k^c$라 할때,\n",
    "class $c$의 fc레이어 출력값(softmax전 logit값) $S_c$는 다음과 같이 계산됩니다:\n",
    "$$S_c = \\sum_k w_k^c F_k$$\n",
    "\n",
    "이 수식에서 알 수 있듯이, $w_k^c$는 class $c$ 판단에 대한 $F_k$값들의 중요도(영향)를 표현합니다.\n",
    "\n",
    "$F_k = \\sum_{x, y} f_k(x, y)$를 위 수식에 대입하면 아래와 같은 결과를 얻습니다.\n",
    "\n",
    "$$S_c = \\sum_k w_k^c  \\sum_{x, y} f_k(x, y) = \\sum_{x, y} \\sum_k w_k^c  f_k(x, y)$$\n",
    "\n",
    "여기서 우리는 **class activation map** $M_c$를 다음과 같이 정의합니다:\n",
    "$$M_c(x, y) = \\sum_k w_k^c f_k(x, y)$$\n",
    "\n",
    "따라서 클래스 $c$에 대응하는 logit 값 $S_c$는 아래와 같이 표현되며, $M_c(x, y)$는 class $c$ 분류를 위한 각 공간 위치 $(x,y)$에서의 활성화 값들의 중요도를 나타냅니다.\n",
    "$$S_c = \\sum_{x, y} M_c(x, y)$$\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import models, transforms\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from utils import visualize_heatmap\n",
    "from ImageNet_utils import clsidx_to_labels as imagenet_idx_to_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "사전학습된 ResNet18을 이용하여 CAM을 실습해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrained_model():\n",
    "    model = models.resnet18(weights = \"IMAGENET1K_V1\")\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    \"\"\"\n",
    "    Preprocess the input image for ResNet-50.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): Path to the input image.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Preprocessed image tensor.\n",
    "    \"\"\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],   # ImageNet means\n",
    "            std=[0.229, 0.224, 0.225]     # ImageNet stds\n",
    "        )\n",
    "    ])\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image_tensor = transform(image).unsqueeze(0) #add batch dim\n",
    "    return image_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "model = load_pretrained_model()\n",
    "image = preprocess_image(\"resources/airplane.jpg\")\n",
    "output = model(image)\n",
    "class_idx = int(output.argmax())\n",
    "print(f\"image.shape: {image.shape}, class index : {class_idx}, class label : {imagenet_idx_to_labels[class_idx]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hook\n",
    "\n",
    "CAM 구현을 위해서는 모델의 중간 출력값을 가져와야 합니다.\n",
    "\n",
    "이를 위해 PyTorch hook 기능을 이용합니다. hook이란 `nn.Module` 중간에 원하는 코드를 삽입할 수 있게 해주는 기능입니다.\n",
    "\n",
    "모듈에 적용되는 hook에는 총 3가지 종류가 있으며, 호출 순서는 다음과 같습니다:\n",
    "forward_pre_hook → `forward()` → forward_hook → `backward()` → full_backward_hook\n",
    "\n",
    "아래 예시는 `forward_hook`을 사용해 각 레이어의 중간 출력값을 저장하는 방법을 보여줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "model = load_pretrained_model()\n",
    "\n",
    "intermediate_outputs = {}\n",
    "handles = []\n",
    "\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        intermediate_outputs[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "for name, module in model.named_modules():\n",
    "    handle = module.register_forward_hook(get_activation(name))\n",
    "    handles.append(handle)\n",
    "\n",
    "model(image) #Forward pass\n",
    "\n",
    "for layer_name, output in intermediate_outputs.items():\n",
    "    print(f\"{layer_name} : {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "또한, hook 함수에서 output을 수정하여 출력값을 변경할 수도 있습니다 (예: gradient clipping).\n",
    "\n",
    "hook은 아래와 같이 `hook_handle`을 사용하여 삭제할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "for handle in handles:\n",
    "    handle.remove()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet18\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "resnet18은 아래의 구조로 이루어져 있습니다:\n",
    "- stem(convl, bn1, relu, maxpool)\n",
    "- 네 개의 스테이지(layer1-4)\n",
    "- avgpool\n",
    "- fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <mark>실습</mark> Class Activation Map (CAM)\n",
    "Class Activation Map 을 계산하는 함수 `compute_cam`을 완성하세요.\n",
    "1. `forward_hook`을 이용하여 ResNet-18 모델의 `layer4`의 출력값(feature)을 가져옵니다.\n",
    "2. 모델에 입력을 전달하여 forward pass를 수행합니다.\n",
    "3. `hook_handle`을 통해 forward hook을 삭제합니다.\n",
    "4. 모델의 `fc`레이어에서 `weight`값을 가져와, `target_class`에 해당하는 가중치를 얻습니다.\n",
    "5. 위 수식을 참고하여 CAM 값을 계산합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cam(model, image_tensor, target_class):\n",
    "    \"\"\"\n",
    "    Compute the Class Activation Map (CAM) heatmap for the target class.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): Pretrained model.\n",
    "        image_tensor (torch.Tensor): Preprocessed image tensor.\n",
    "        target_class (int): Target class index.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: CAM heatmap.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Hook the feature extractor\n",
    "    features = []\n",
    "    def hook_feature(module, input, output):\n",
    "        features.append(output.detach())\n",
    "    hook_handle = model.layer4.register_forward_hook(hook_feature)\n",
    "\n",
    "    # Forward pass\n",
    "    output = model(image_tensor)\n",
    "\n",
    "    hook_handle.remove()\n",
    "\n",
    "    fc_weights = model.fc.weight  # (1000, 512)\n",
    "    feature_map = features[0]     # (1, 512, 7, 7)\n",
    "\n",
    "    ##### YOUR CODE START #####\n",
    "\n",
    "    ##### YOUR CODE END #####\n",
    "    \n",
    "    return cam # (7, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "model = load_pretrained_model()\n",
    "image = preprocess_image(\"resources/airplane.jpg\")\n",
    "output = model(image)\n",
    "class_idx = int(output.argmax())\n",
    "cam = compute_cam(model, image, class_idx)\n",
    "print(f\"Image shape : {image.shape}, CAM shape : {cam.shape}\")\n",
    "\n",
    "assert torch.isclose(torch.sum(cam, axis = 0), torch.tensor([122.27974700927734, 169.5097198486328, 156.0562744140625, 139.6700439453125, 121.7618408203125, 130.908447265625, 95.88688659667969]), rtol=1e-1).all(), \"cam activation map is different\"\n",
    "\n",
    "print(\"\\033[92m All tests passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "visualize_heatmap(\"resources/airplane.jpg\", cam, imagenet_idx_to_labels[class_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GradCAM\n",
    "CAM (Class Activation Map)은 Global Average Pooling 레이어와 fc 레이어가 있는 CNN 구조에서만 작동하며, 마지막 CNN 레이어의 활성화 맵만 시각화할 수 있다는 한계가이 존재합니다.\n",
    "\n",
    "GradCAM([논문 링크](https://arxiv.org/pdf/1610.02391))은 이러한 단점을 보완하기 위해 개발되었습니다.\n",
    "\n",
    "Grad-CAM은 임의의 CNN 레이어에 대한 클래스별 중요도를 계산하며, Transformer, RNN 등 다양한 네트워크 구조에도 확장될 수 있는 유연성을 제공합니다\n",
    "\n",
    "## Grad-CAM 계산과정\n",
    "\n",
    "모델의 class $c$에 대한 score값(softmax이전 logit값)을 $y^c$라 하고, 특정 conv layer의 k-번째 feature map을 $A^k$라 하겠습니다.\n",
    "\n",
    "미분 값 $\\frac{\\partial y^c}{\\partial A^k_{ij}}$는 $A^k$의 공간 위치 $(i, j)$가 클래스 $c$의 점수 $y^c$에 미치는 영향을 나타냅니다.\n",
    "\n",
    "이 미분 값에 에 대해 **global average pooling**을 수행하면 중요도 $\\alpha_k^c$를 계산할 수 있습니다.\n",
    "\n",
    "$$\\alpha_k^c = \\overbrace{\\frac{1}{Z} \\sum_{i} \\sum_{j}}^{\\text{global average pooling}} \\underbrace{\\frac{\\partial y^c}{\\partial A^k_{ij}}}_{\\text{gradients via backprop}}$$\n",
    "\n",
    "\n",
    "이 $\\alpha_k^c$값은 feature map $A^k$가 클래스 $c$ 예측에 미치는 중요도를 나타냅니다.\n",
    "\n",
    "Grad-CAM $L^c_{\\text{Grad-CAM}}$는 다음과 같이 얻을 수 있습니다 (class-discriminative localization map이라 불림).\n",
    "\n",
    "$$ L^c_{\\text{Grad-CAM}} = \\text{ReLU} \\underbrace{(\\sum_{k} \\alpha_k^c A^k )}_{\\text{linear combination}}$$\n",
    "\n",
    "여기서 ReLU를 사용하는 이유는 클래스 $c$에 대해 positive한 영향을 주는 부분만을 시각화하기 위해서입니다.\n",
    "\n",
    "<img src=\"resources/GradCAM.png\" style=\"width:1000px\">\n",
    "\n",
    "---\n",
    "\n",
    "**참고** Grad-CAM은 CAM의 일반화임을 수학적으로 증명할 수 있다.\n",
    "\n",
    "CAM 계산은 아래와 같이 주어집니다:\n",
    "$$M_c(x, y) = \\sum_k w_k^c A_k(x, y)$$\n",
    "\n",
    "여기서 fc레이어의 가중치 $w_k^c$는 사실상 Grad-CAM에서 사용되는 $\\alpha_k^c$와 동등합니다. \n",
    "$$ w_k^c = \\sum_{i} \\sum_{j} \\frac{\\partial y^c}{\\partial A^k_{ij}} $$\n",
    "\n",
    "자세한 유도는 Grad-CAM 논문에서 확인할 수 있습니다.\n",
    "\n",
    "## <mark>실습</mark> GradCAM\n",
    "`GradCAM`의 `__call__`을 완성하세요.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradCAM:\n",
    "    def __init__(self, model):\n",
    "        self.model = model.eval()\n",
    "        self.gradient = None\n",
    "        self.feature_map = None\n",
    "        self._register_hook()\n",
    "\n",
    "    def _register_hook(self):\n",
    "        \"\"\"\n",
    "        Register hooks to capture gradients and activations from the target layer.\n",
    "        \"\"\"\n",
    "        target_layer = self.model.layer4[-1].conv2\n",
    "        target_layer.register_forward_hook(self._forward_hook)\n",
    "        target_layer.register_full_backward_hook(self._backward_hook)\n",
    "\n",
    "    def _forward_hook(self, module, input, output):\n",
    "        \"\"\"\n",
    "        Forward hook to capture activations.\n",
    "\n",
    "        Args:\n",
    "            module (torch.nn.Module): The module being hooked.\n",
    "            input (torch.Tensor): Input to the module.\n",
    "            output (torch.Tensor): Output from the module.\n",
    "        \"\"\"\n",
    "        self.feature_map = output   #[1, 512, 7, 7]\n",
    "\n",
    "    def _backward_hook(self, module, grad_input, grad_output):\n",
    "        \"\"\"\n",
    "        Backward hook to capture gradients.\n",
    "\n",
    "        Args:\n",
    "            module (torch.nn.Module): The module being hooked.\n",
    "            grad_input (tuple): Gradients with respect to the module's inputs.\n",
    "            grad_output (tuple): Gradients with respect to the module's outputs.\n",
    "        \"\"\"\n",
    "        self.gradient = grad_output[0]  #[1, 512, 7, 7]\n",
    "        \n",
    "    def __call__(self, x, target_class):\n",
    "        \"\"\"\n",
    "        Compute the Grad-CAM heatmap.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input image tensor.\n",
    "            target_class (int): Target class index.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Grad-CAM heatmap.\n",
    "        \"\"\"\n",
    "\n",
    "        ##### YOUR CODE START #####\n",
    "        output = self.model(x) # save activation map to self.feature_map\n",
    "\n",
    "        self.model.zero_grad()\n",
    "        loss = None # TODO\n",
    "        loss.backward() # save gradient to self.gradient\n",
    "\n",
    "        a_k = None # TODO, Output shape: [1, C, 1, 1]\n",
    "        ##### YOUR CODE END #####\n",
    "\n",
    "        grad_cam = torch.sum(a_k * self.feature_map, dim=1) # Output shape: [1, H, W]\n",
    "        grad_cam = torch.relu(grad_cam)\n",
    "\n",
    "        return grad_cam.squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "image_files = [\n",
    "    \"resources/airplane.jpg\", \n",
    "    \"resources/bus.jpg\",\n",
    "    \"resources/dog.jpg\", \n",
    "    \"resources/african_hunting_dog.jpg\",\n",
    "    \"resources/dog_cat.jpg\"\n",
    "]\n",
    "\n",
    "model = load_pretrained_model()\n",
    "gradcam = GradCAM(model=model)\n",
    "\n",
    "gradcam_results = []\n",
    "for image_file in image_files:\n",
    "    image = preprocess_image(image_file)\n",
    "\n",
    "    output = model(image)\n",
    "    class_idx = int(output.argmax())\n",
    "\n",
    "    gradcam_heatmap = gradcam(image, class_idx)\n",
    "\n",
    "    # print(f\"Grad-CAM shape : {gradcam_heatmap.shape}\")\n",
    "    visualize_heatmap(image_file, gradcam_heatmap, imagenet_idx_to_labels[class_idx])\n",
    "    gradcam_results.append(gradcam_heatmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "gradcam_all = torch.stack(gradcam_results, dim = 0)\n",
    "\n",
    "assert torch.isclose(gradcam_all[:, 3, 4], torch.tensor([0.4690302014350891, 0.600059449672699, 0.35050174593925476, 0.6419365406036377, 0.15902754664421082]), atol = 1e-2).all()\n",
    "assert torch.isclose(gradcam_all[:, 1, 2], torch.tensor([0.0404171422123909, 0.41235652565956116, 0.5197665691375732, 0.07737737894058228, 0.6306908130645752]), atol = 1e-2).all()\n",
    "\n",
    "print(\"\\033[92m All tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "같은 이미지에서 서로 다른 클래스 $c$에 대한 Grad-CAM을 시각화해봅시다.\n",
    "\n",
    "`gradcam_single_image`함수는 주어진 이미지 파일에 대해 Grad-CAM heatmap을 계산하고 시각화합니다.\n",
    "\n",
    "이때, 이 함수에 `target_class` 인자를 전달하면 model의 예측값이 아니라 전달받은 `target_class`에 대한 Grad-CAM을 계산합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradcam_single_image(image_file, model, gradcam, target_class=None):\n",
    "    \"\"\"\n",
    "    Process a single image file to compute and display the Grad-CAM heatmap.\n",
    "\n",
    "    Args:\n",
    "        image_file (str): Path to the image file.\n",
    "        target_class (int, optional): Target class index. If None, use model's prediction.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Grad-CAM heatmap.\n",
    "    \"\"\"\n",
    "    image = preprocess_image(image_file)\n",
    "\n",
    "    if target_class is None:\n",
    "        output = model(image)\n",
    "        class_idx = int(output.argmax())\n",
    "    else:\n",
    "        class_idx = target_class\n",
    "\n",
    "    gradcam_heatmap = gradcam(image, class_idx)\n",
    "\n",
    "    visualize_heatmap(image_file, gradcam_heatmap, imagenet_idx_to_labels[class_idx])\n",
    "\n",
    "    return gradcam_heatmap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "image_file = \"resources/dog_cat.jpg\"\n",
    "\n",
    "target_classes = [None, 281]\n",
    "\n",
    "model = load_pretrained_model()\n",
    "gradcam = GradCAM(model=model)\n",
    "\n",
    "gradcam_results = []\n",
    "for target_class in target_classes:\n",
    "    gradcam_heatmap = gradcam_single_image(image_file, model, gradcam, target_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(선택 과제) 다른 마지막 CNN layer (layer4[-1].conv2) 가 아닌 다른 중간 CNN layer에서도 Grad-CAM을 시각화해보세요.\n",
    "\n",
    "이를 통해 네트워크가 점진적으로 추출하는 특성들이 어떻게 변화하는지 확인할 수 있습니다. 초기 레이어일수록 저수준의 특징(에지나 텍스처 등)을 강조하고, 마지막 레이어일수록 고수준의 의미론적 정보(객체 형태 등)에 집중하는 경향을 보입니다. 따라서 중간 레이어에서의 Grad-CAM 시각화를 통해, 네트워크가 특정 클래스와 관련된 특징을 점진적으로 학습해가는 과정을 시각적으로 분석할 수 있습니다.\n",
    "\n",
    "Tip: 중간 레이어의 Grad-CAM 결과를 마지막 레이어의 결과와 비교하여, 네트워크가 다양한 수준의 정보를 학습하고 사용하는 방식을 더욱 깊이 이해할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Attention Visualization\n",
    "\n",
    "이번에는 ViT (Vision Transformer)모델의 Attention Map을 시각화 해보겠습니다.\n",
    "\n",
    "사전 학습된 Vision Transformer (ViT) 모델을 불러와, 입력 이미지에 대한 각 레이어에서의 attention 패턴을 시각화해봅니다. ViT는 이미지를 패치(patch) 단위로 분할하여, 각 패치 간의 관계를 Transformer의 self-attention 메커니즘으로 학습합니다. 이 실습을 통해, ViT가 입력 이미지의 특정 영역에 어떻게 주목하는지 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import ViTImageProcessor , ViTForImageClassification\n",
    "from PIL import Image\n",
    "\n",
    "from utils import visualize_heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrained_vit():\n",
    "    \"\"\"\n",
    "    Load a pretrained Vision Transformer model and feature extractor.\n",
    "\n",
    "    Returns:\n",
    "        model (ViTForImageClassification): Pretrained ViT model.\n",
    "        feature_extractor (ViTFeatureExtractor): Corresponding feature extractor.\n",
    "    \"\"\"\n",
    "    model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224', attn_implementation=\"eager\")\n",
    "    image_processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\n",
    "    model.eval()\n",
    "    return model, image_processor\n",
    "\n",
    "def preprocess_image_vit(image_path, image_processor):\n",
    "    \"\"\"\n",
    "    Preprocess the input image for the Vision Transformer.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): Path to the input image.\n",
    "        feature_extractor (ViTFeatureExtractor): Feature extractor for preprocessing.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Preprocessed image tensor.\n",
    "    \"\"\"\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    inputs = image_processor(images=image, return_tensors=\"pt\")\n",
    "    return inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "image_path = 'resources/airplane.jpg'  # Replace with your image path\n",
    "model, image_processor = load_pretrained_vit()\n",
    "inputs = preprocess_image_vit(image_path, image_processor)\n",
    "\n",
    "outputs = model(**inputs, output_attentions=True)\n",
    "print(f\"Input shape : {inputs['pixel_values'].shape}, output shape : {outputs.logits.shape}\")\n",
    "print(f\"Num layer : {len(outputs.attentions)}, attention shape : {outputs.attentions[0].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HuggingFace `transformer`패키지의 `ViTForImageClassification`모델은 `output_attentions=True`옵션을 통해 attention을 출력하는 기능을 제공합니다.\n",
    "\n",
    "이때 출력되는 `outputs.attentions` 텐서는 `(batch_size, num_heads, num_tokens, num_tokens)`의 차원을 가지며 각 차원의 의미는 다음과 같습니다:\n",
    "- batch_size: 배치에 포함된 이미지 수.\n",
    "- num_heads: 멀티 헤드 self-attention의 헤드 수. 각 헤드는 독립적으로 attention을 계산합니다.\n",
    "- num_tokens (query tokens): 트랜스포머 모델의 입력 토큰 수로, 이미지 패치와 CLS 토큰이 포함됩니다 (197 = 1 + 14*14). CLS 토큰은 분류모델에서 전체 이미지의 요약 정보로 사용됩니다.\n",
    "- num_tokens (key tokens): 각 query token이 주목하는 모든 key token의 attention 값을 나타냅니다.\n",
    "\n",
    "여기서 CLS 토큰과 각 이미지 패치간의 attention 값을 분석하면 ViT 이미지 분류 모델이 이미지의 어느 영역에 가장 집중하고 있는지 파악할 수 있습니다.\n",
    "\n",
    "## <mark>실습</mark> Attention map visualization\n",
    "아래 과정에 따라 `obtain_attention_maps`함수를 완성하세요.\n",
    "1. Head Fusion: 여러 헤드의 attention 값을 평균을 통해 합쳐서 단일 attention map을 생성합니다.\n",
    "2. CLS 토큰의 attention 값 추출: query가 CLS 토큰이고 key가 이미지 패치들인 attention 값들을 추출합니다.\n",
    "3. 2D Attention Map으로 변환: 이미지 패치들은 1D로 나열되어 있으므로, `num_grid x num_grid` 크기로 변환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_attention_maps(attentions, patch_size=16):\n",
    "    \"\"\"\n",
    "    Obtain attention maps from the ViT model.\n",
    "\n",
    "    Args:\n",
    "        attentions (list of torch.Tensor): Attention maps from each layer.\n",
    "        patch_size (int): Size of the patches used in ViT.\n",
    "\n",
    "    Returns:\n",
    "        list of torch.Tensor: List of attention maps for each layer.\n",
    "    \"\"\"\n",
    "    num_grid = 224 // patch_size\n",
    "    \n",
    "    attention_maps = []\n",
    "    for attention in attentions:  # attention shape: (batch_size, num_heads, num_tokens, num_tokens)\n",
    "        ##### YOUR CODE START #####\n",
    "        attention_heads_fused = None # TODO, Output shape:(batch_size, num_tokens, num_tokens)\n",
    "\n",
    "        # attention from the CLS token to the image patches\n",
    "        cls_attentions = None # TODO, Output shape:(num_tokens - 1)\n",
    "        ##### YOUR CODE END #####\n",
    "        cls_attentions = cls_attentions.reshape(num_grid, num_grid)\n",
    "        attention_maps.append(cls_attentions)\n",
    "\n",
    "    return attention_maps\n",
    "\n",
    "def get_attention_maps(model, inputs):\n",
    "    \"\"\"\"\n",
    "    Get the attention maps from the Vision Transformer.\n",
    "\n",
    "    Args:\n",
    "        model (ViTForImageClassification): Pretrained ViT model.\n",
    "        inputs (dict): Dictionary of inputs for the model (e.g., pixel_values).\n",
    "\n",
    "    Returns:\n",
    "        list of torch.Tensor: List containing attention maps from each layer.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_attentions=True)\n",
    "    return outputs.attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "attentions = get_attention_maps(model, inputs)\n",
    "heatmaps = obtain_attention_maps(attentions)\n",
    "\n",
    "assert torch.isclose(heatmaps[4].sum(axis = 0), torch.tensor([0.06968411803245544, 0.08526063710451126, 0.09705086797475815, 0.07389257848262787, 0.060506727546453476, 0.05866185575723648, 0.06139916554093361, 0.0615709163248539, 0.05205957964062691, 0.05494638532400131, 0.05141854286193848, 0.051894575357437134, 0.05805109441280365, 0.062026496976614]), atol = 1e-2).all(), \"Attrention map value is different\"\n",
    "\n",
    "print(\"\\033[92m All tests passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "for i, heatmap in enumerate(heatmaps):\n",
    "    visualize_heatmap(image_path, heatmap, f\"Layer {i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Attention과 정보 혼합 (Information Mixing)\n",
    "Transformer의 self-attention 레이어에서는 Attention에 기반하여 각 패치의 임베딩(embedding)이 다른 패치의 임베딩과 혼합됩니다. 트랜스포머 layer가 깊어질수록 이러한 정보 혼합이 반복되어, 각 패치가 다른 패치의 정보를 점점 더 많이 포함하게 되며, 이를 통해 각 패치가 이미지 전체의 정보를 담을 수 있게 됩니다.\n",
    "\n",
    "Vision Transformer에서 [CLS] 토큰은 모든 패치의 정보를 종합하여 최종 분류에 사용됩니다. 각 레이어에서 [CLS] 토큰은 모든 패치에 주목하며, 레이어를 거듭할수록 이미지의 전체적인 정보가 [CLS] 토큰에 축적됩니다.\n",
    "\n",
    "### Attention map 시각화 결과 해석\n",
    "- 초기 레이어: 위 Attention map을 출력 결과를 살펴보면 초기 레이어의 attention map은 주로 이미지의 일부 영역(특정 패치)에 집중하는 경향을 보입니다.\n",
    "\n",
    "- 깊은 레이어\n",
    "  - 레이어가 깊어질수록 모델은 더 높은 수준의 추상화된 특징을 학습하게 됩니다. 이에 따라 attention map은 점점 더 이미지의 전반적인 객체나 전체적인 맥락을 반영하고, 특정 세부 영역에 대한 Attention은 줄어듭니다. \n",
    "  - 따라서 최종 레이어에 가까워질수록 [CLS] 토큰의 attention은 이미지의 특정 부분이 아닌 이미지 전체에 대한 집중도가 높아집니다. \n",
    "  - 또한 Self-attention으로 인한 정보 혼합으로 인해 깊은 레이어에서는 각 패치가 더 이상 고유한 공간적 정보를 많이 유지하지 않게 됩니다. 대신, 각 패치는 의미론적으로 풍부하지만 공간적으로 혼합된 정보를 표현합니다. 그 결과, 깊은 레이어에서의 attention map은 이미지 전체를 덮거나 특정 객체와 무관한 영역을 포함할 수 있습니다\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Rollout\n",
    "\n",
    "앞서 살펴본 ViT모델의 Attention map은 transformer layer를 거치면서 점차 양상이 달라집니다. 그러면 ViT의 전체 레이어의 종합적인 attention map은 어떻게 구할 수 있을까요?\n",
    "\n",
    "Attention Rollout 기법은 Quantifying Attention Flow in Transformers논문([링크](https://arxiv.org/pdf/2005.00928))에서 제안된 방법으로, 각 레이어의 attention을 결합하여 Transformer 모델의 전체적인 attention 흐름을 분석하는 방법입니다.\n",
    "\n",
    "1. Head Fusion:여러 attention 헤드가 존재하므로, 이 헤드들 간의 attention을 결합합니다. 평균(mean), 최대값(max), 최소값(min) 등 다양한 방식이 있으며 아래 수식은 평균을 사용했을때의 수식입니다:\n",
    "$$A_l^{\\text{avg}} = \\frac{1}{N_{\\text{heads}}} \\sum_{h=1}^{N_{\\text{heads}}} A_l^{(h)}$$\n",
    "\n",
    "2. Residual Connection 반영: Self-attention은 일반적으로 residual connection과 함께 사용되므로 이를 반영하기 위해 identity 행렬을 더해줍니다.\n",
    "$$A_l^{\\text{rollout}} = A_l^{\\text{avg}} + I$$\n",
    "\n",
    "3. 정규화: 각 행의 합이 1이 되도록로 정규화합니다. 이를 통해 각 토큰이 다른 이미지 토큰에 얼마나 집중하는지를 더 명확히 파악할 수 있습니다.\n",
    "$$A_l^{\\text{norm}} = \\frac{A_l^{\\text{rollout}}}{\\sum_{j=1}^{N_{\\text{tokens}}} A_l^{\\text{rollout}}[i, j]}$$\n",
    "\n",
    "4. 레이어별 Attention 곱하기: 정규화된 attention 행렬들을 첫 번째 레이어부터 마지막 레이어까지 순차적으로 곱하여 전체 attention rollout $R_L$을 계산합니다.\n",
    "$$R_L = A_1^{\\text{norm}} \\cdot A_2^{\\text{norm}} \\cdot \\dots \\cdot A_L^{\\text{norm}}$$\n",
    "\n",
    "5. CLS 토큰의 Attention: $R_L$에서 [CLS] 토큰이 이미지 패치에 대해 가지는 attention을 추출합니다. 이를 통해 CLS 토큰이 이미지 전체 정보를 어떻게 통합했는지 확인할 수 있습니다.\\\n",
    "\n",
    "$$\\text{Attn}_{\\text{CLS}} = R_L[0, 1:]$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_attention_rollout(attentions, discard_ratio=0.0, head_fusion='mean'):\n",
    "    \"\"\"\n",
    "    Compute the attention rollout from the attention maps.\n",
    "\n",
    "    Args:\n",
    "        attentions (list of torch.Tensor): Attention maps from each layer.\n",
    "        discard_ratio (float): Ratio of attention to discard.\n",
    "        head_fusion (str): Method to fuse attention heads.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Attention rollout map.\n",
    "    \"\"\"\n",
    "    result = torch.eye(attentions[0].size(-1))  #shape: (197, 197)\n",
    "    with torch.no_grad():\n",
    "        for attention in attentions: # shape: (batch_size, num_heads, num_tokens, num_tokens)\n",
    "            if head_fusion == \"mean\":\n",
    "                attention_heads_fused = attention.mean(axis=1) # shape: (batch_size, num_tokens, num_tokens)\n",
    "            elif head_fusion == \"max\":\n",
    "                attention_heads_fused = attention.max(axis=1)[0]\n",
    "            elif head_fusion == \"min\":\n",
    "                attention_heads_fused = attention.min(axis=1)[0]\n",
    "            else:\n",
    "                raise \"Attention head fusion type Not supported\"\n",
    "\n",
    "            # Remove percentages of the least important attentions\n",
    "            flat = attention_heads_fused.view(attention_heads_fused.size(0), -1)\n",
    "            _, indices = flat.topk(int(flat.size(-1) * discard_ratio), -1, False)\n",
    "            indices = indices[indices != 0]\n",
    "            flat[0, indices] = 0\n",
    "\n",
    "            # Add identity matrix and normalize rows\n",
    "            I = torch.eye(attention_heads_fused.size(-1)) # shape: (197, 197)\n",
    "            a = attention_heads_fused + I\n",
    "            a = a / a.sum(dim=-1)\n",
    "\n",
    "            result = torch.matmul(a, result) #(1, 197, 197)\n",
    "\n",
    "    mask = result[0, 0, 1:] # attention between the class token and the image patches\n",
    "    num_grid = int(mask.size(-1)**0.5)\n",
    "    mask = mask.reshape(num_grid, num_grid)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "image_path = 'resources/dog_cat2.jpg'  # Replace with your image path\n",
    "model, image_processor = load_pretrained_vit()\n",
    "inputs = preprocess_image_vit(image_path, image_processor)\n",
    "attentions = get_attention_maps(model, inputs)\n",
    "\n",
    "attention_rollout = compute_attention_rollout(attentions, discard_ratio= 0.6, head_fusion= \"mean\")\n",
    "print(f\"attention_rollout shape: {attention_rollout.shape}\")\n",
    "visualize_heatmap(image_path, attention_rollout, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "assert torch.isclose(attention_rollout.sum(axis = -1), torch.tensor([0.06629233062267303, 0.06181266903877258, 0.05839885398745537, 0.054853130131959915, 0.059523843228816986, 0.06684955954551697, 0.06055447831749916, 0.05917668342590332, 0.05544937774538994, 0.06043049693107605, 0.05814819782972336, 0.06188401207327843, 0.06259924918413162, 0.0535668320953846]), atol=1e-3).all()\n",
    "\n",
    "print(\"\\033[92m All tests passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2.3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
