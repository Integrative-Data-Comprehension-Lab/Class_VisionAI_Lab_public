{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Segmentation with U-Net\n",
    "\n",
    "ì´ë²ˆ ì‹œê°„ì—ëŠ” U-Net ëª¨ë¸ì„ ì´ìš©í•˜ì—¬ image segmentation ì‘ì—…ì„ ì§„í–‰í•´ë³´ì.\n",
    "\n",
    "Semantic image segmenataionì€ pixelìˆ˜ì¤€ìœ¼ë¡œ ì´ë¯¸ì§€ì˜ ë ˆì´ë¸”ì„ ì˜ˆì¸¡í•˜ëŠ” ë¬¸ì œë¥¼ ì§€ì¹­í•©ë‹ˆë‹¤. ë‹¤ì‹œ ë§í•´, ë‹¨ìˆœíˆ ë¬¼ì²´ê°€ ì´ë¯¸ì§€ì— ì¡´ì¬í•˜ëŠ”ì§€ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, ê° í”½ì…€ì´ ì–´ë–¤ í´ë˜ìŠ¤ì— ì†í•˜ëŠ”ì§€ë¥¼ íŒŒì•…í•©ë‹ˆë‹¤.\n",
    "\n",
    "Segmentationì€ Object detectionê³¼ ìœ ì‚¬í•˜ê²Œ \"ì£¼ì–´ì§„ ì´ë¯¸ì§€ì— ì–´ë–¤ ë¬¼ì²´ê°€ ì¡´ì¬í•˜ê³  ì–´ë””ì— ìœ„ì¹˜í•˜ëŠ”ê°€?\"ë¼ëŠ” ì§ˆë¬¸ì— ë‹µí•©ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ object detectionì€ ë¬¼ì²´ë¥¼ bounding boxë¡œ ê°ì‹¸ê¸° ë•Œë¬¸ì—, ë°•ìŠ¤ ë‚´ì— ë¬¼ì²´ê°€ ì•„ë‹Œ í”½ì…€ë„ í¬í•¨ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. \n",
    "\n",
    "ë°˜ë©´ì—, semantic image segmentationì€ í”½ì…€ ë‹¨ìœ„ë¡œ ì •í™•í•œ ë¬¼ì²´ì˜ ë§ˆìŠ¤í¬ë¥¼ ì–»ì„ ìˆ˜ ìˆì–´ ë” ì„¸ë°€í•œ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤.\n",
    "\n",
    "<img src=\"resources/carseg.png\" style=\"width:500px;height:250;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, models\n",
    "import torchvision.transforms.v2 as transforms\n",
    "\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "from training_utilities import create_dataloaders, train_loop, calculate_pixel_accuracy, AverageMeter, save_checkpoint, load_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "data_root_dir = '/datasets'\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "voc_train_dataset = datasets.VOCSegmentation(root=data_root_dir, year='2012', image_set='train', download=False,\n",
    "                                          transform = train_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë¨¼ì € Pascal VOC 2012 Segmentation ë°ì´í„°ì…‹ì„ ë¶ˆëŸ¬ì˜¤ì.\n",
    "\n",
    "ì´ datasetì€ RGB ì´ë¯¸ì§€ì™€ ì´ì— ëŒ€ì‘ë˜ëŠ” maskë¥¼ ë¦¬í„´í•©ë‹ˆë‹¤.\n",
    "maskì—ëŠ” í”½ì…€ ìˆ˜ì¤€ìœ¼ë¡œ classë¥¼ ì§€ì¹­í•˜ëŠ” ì •ìˆ˜ê°’ì´ ë“¤ì–´ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "- 0 : background\n",
    "- 255 : 'void' or unlabelled.\n",
    "- 1~20 : 20 classes\n",
    "\n",
    "\n",
    "í•œí¸, ì´ë¯¸ì§€ì˜ í¬ê¸°ë¥¼ ë³´ë©´ transformì´ imageì—ë§Œ ì ìš©ë˜ê³  targetì—ëŠ” ì ìš©ë˜ì§€ ì•Šì€ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "targetì—ë„ transformì„ ì ìš©í•˜ë ¤ë©´ ìƒˆë¡œìš´ ë°ì´í„°ì…‹ê³¼ custom transformí•¨ìˆ˜ ì •ì˜ê°€ í•„ìš”í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "idx = 0\n",
    "image, target = voc_train_dataset[idx]\n",
    "mask_palette = target.getpalette()\n",
    "\n",
    "print(f\"Image.shape = {image.shape}, target.shape = {np.array(target).shape}\")\n",
    "print(f\"Target values unique : {np.unique(target)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def mask_tensor_to_pil(mask):\n",
    "    mask_np = mask.numpy().astype(np.uint8)\n",
    "    mask_pil = Image.fromarray(mask_np, mode='P')\n",
    "    mask_pil.putpalette(mask_palette)\n",
    "    return(mask_pil)\n",
    "\n",
    "def visualize_samples(dataset, cols=4, rows=3, select_random = True):\n",
    "    \"\"\"\n",
    "    Visualize a few samples from the VOCSegmentation dataset, showing both the input image and its corresponding label (segmentation mask).\n",
    "\n",
    "    Parameters:\n",
    "        dataset: A dataset object, e.g., VOCSegmentation, where each item is a tuple (image, label).\n",
    "        cols (int): Number of columns in the visualization grid.\n",
    "        rows (int): Number of rows in the visualization grid.\n",
    "    \"\"\"\n",
    "    figure, ax = plt.subplots(nrows=rows, ncols=cols * 2, figsize=(12, 6))\n",
    "    ax = ax.flatten()\n",
    "\n",
    "    if select_random:\n",
    "        indices = random.sample(range(len(dataset)), cols * rows)\n",
    "    else:\n",
    "        indices = range(cols * rows)\n",
    "    \n",
    "    \n",
    "    for i, idx in enumerate(indices):\n",
    "        # Get the image and label (segmentation mask)\n",
    "        img, mask = dataset[idx]\n",
    "\n",
    "        # unnormalize image\n",
    "        mean = torch.tensor([0.485, 0.456, 0.406]).to(img.device).view(-1, 1, 1)\n",
    "        std = torch.tensor([0.229, 0.224, 0.225]).to(img.device).view(-1, 1, 1)\n",
    "        img = img * std + mean\n",
    "        \n",
    "\n",
    "        # Display the image\n",
    "        ax[2 * i].imshow(img.numpy().transpose((1, 2, 0)))\n",
    "        ax[2 * i].set_title(f\"Image {i+1}\")\n",
    "        ax[2 * i].axis(\"off\")\n",
    "\n",
    "        # Display the segmentation mask (assuming it's a single-channel mask)\n",
    "        if isinstance(mask, torch.Tensor):\n",
    "            mask = mask_tensor_to_pil(mask)\n",
    "\n",
    "        ax[2 * i + 1].imshow(mask, cmap=\"gray\")\n",
    "        ax[2 * i + 1].set_title(f\"Label {i+1}\")\n",
    "        ax[2 * i + 1].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "visualize_samples(voc_train_dataset, cols=2, rows=3, select_random = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì´ë¯¸ì§€ ë¶„ë¥˜(classification) ë¬¸ì œì™€ëŠ” ë‹¬ë¦¬, segmentation ë¬¸ì œì—ì„œëŠ” ì´ë¯¸ì§€ì™€ íƒ€ê²Ÿ ë§ˆìŠ¤í¬ì— ë™ì¼í•œ ë³€í™˜(transform)ì„ ì ìš©í•´ì•¼ í•©ë‹ˆë‹¤. \n",
    "\n",
    "ì˜ˆë¥¼ ë“¤ì–´, ì´ë¯¸ì§€ì— ìˆ˜í‰ ë°˜ì „ì„ ì ìš©í–ˆë‹¤ë©´, íƒ€ê²Ÿ ë§ˆìŠ¤í¬ì—ë„ ë™ì¼í•œ ë³€í™˜ì„ ìˆ˜í–‰í•˜ì—¬ ì´ë¯¸ì§€ì™€ ë ˆì´ë¸”ì´ ì¼ì¹˜í•˜ë„ë¡ í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "\n",
    "ë˜í•œ, ë°ì´í„° ì¦ê°•(data augmentation)ì´ ëœë¤í•˜ê²Œ ìˆ˜í–‰ë˜ëŠ” ê²½ìš°ì—ë„ ì´ë¯¸ì§€ì™€ ë§ˆìŠ¤í¬ì— ë™ì¼í•œ ëœë¤ ë³€í™˜ì´ ì ìš©ë˜ë„ë¡ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤. ì´ë ‡ê²Œ í•´ì•¼ë§Œ ë°ì´í„°ì˜ ì¼ê´€ì„±ì„ ìœ ì§€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ìƒˆë¡œìš´ ë°ì´í„°ì…‹ `UNetDataset`ì€ `VOCSegmentation`ë°ì´í„°ì…‹ì—ì„œ `image`ì™€ `target`ì„ ì½ì–´ì™€ ë™ì‹œì— `transforms`í•¨ìˆ˜ì— ì „ë‹¬í•œë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNetDataset(Dataset):\n",
    "    def __init__(self, voc_dataset, transforms=None):\n",
    "        self.dataset = voc_dataset\n",
    "        self.transforms = transforms\n",
    "\n",
    "        self.classes = [\"background\", \"aeroplane\", \"bicycle\", \"bird\", \"boat\", \"bottle\",\n",
    "                        \"bus\", \"car\", \"cat\", \"chair\", \"cow\", \"diningtable\",\n",
    "                        \"dog\", \"horse\", \"motorbike\", \"person\", \"pottedplant\",\n",
    "                        \"sheep\", \"sofa\", \"train\", \"tvmonitor\"]\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, target = self.dataset[idx]\n",
    "\n",
    "        if self.transforms:\n",
    "            image, target = self.transforms(image, target)\n",
    "\n",
    "        return image, target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom transformí•¨ìˆ˜ë¥¼ ì •ì˜í•˜ê¸° ìœ„í•´ì„œëŠ” `__init__` ê³¼ `__call__`ì„ ë©”ì„œë“œë¥¼ í¬í•¨í•œ í˜¸ì¶œ ê°€ëŠ¥í•œ(callable) í´ë˜ìŠ¤ë¥¼ ë§Œë“¤ë©´ ë©ë‹ˆë‹¤.\n",
    "\n",
    "ì•„ë˜ ì½”ë“œëŠ” transformí•¨ìˆ˜ í˜¸ì¶œì‹œ(`__call__`) ì „ë‹¬ë°›ì€ `img`ì™€ `mask`ì— ë™ì¼í•œ ë³€í™˜ì„ ìˆ˜í–‰í•˜ëŠ” transformí•¨ìˆ˜ì˜ ì˜ˆì‹œì…ë‹ˆë‹¤.\n",
    "\n",
    "1. JointResize: ì´ë¯¸ì§€ì™€ íƒ€ê²Ÿ ë§ˆìŠ¤í¬ì˜ í¬ê¸°ë¥¼ ëª¨ë‘ ë³€ê²½í•©ë‹ˆë‹¤.\n",
    "   - ë§ˆìŠ¤í¬ëŠ” ë ˆì´ë¸” ì†ì‹¤ì´ ì—†ë„ë¡ ìµœê·¼ì ‘ ì´ì›ƒ ë³´ê°„ (NEAREST interpolation)ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "2. JointToTensor: ì´ë¯¸ì§€ë¥¼ í…ì„œë¡œ ë³€í™˜í•˜ê³ , ë§ˆìŠ¤í¬ë¥¼ ì •ìˆ˜í˜• í…ì„œë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n",
    "   - ì´ë¯¸ì§€ë¥¼ 0ê³¼ 1 ì‚¬ì´ì˜ ê°’ì„ ê°€ì§€ëŠ” í…ì„œë¡œ ë³€í™˜í•˜ê³ , ë§ˆìŠ¤í¬ëŠ” ë ˆì´ë¸” ì •ë³´ë¥¼ ê°€ì§€ëŠ” ì •ìˆ˜í˜• í…ì„œë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n",
    "3. JointNormalize: ì´ë¯¸ì§€ë¥¼ ì£¼ì–´ì§„ í‰ê· ê³¼ í‘œì¤€í¸ì°¨ë¡œ ì •ê·œí™”í•©ë‹ˆë‹¤. ë§ˆìŠ¤í¬ëŠ” ë³€í˜•í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms.functional as F\n",
    "\n",
    "class JointResize(object):\n",
    "    \"\"\"Resize both image and target mask to the given size.\"\"\"\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "\n",
    "    def __call__(self, img, mask):\n",
    "        img = F.resize(img, self.size)\n",
    "        mask = F.resize(mask, self.size, interpolation=transforms.functional.InterpolationMode.NEAREST)\n",
    "        return img, mask\n",
    "\n",
    "class JointToTensor(object):\n",
    "    \"\"\"Convert PIL image to tensor and mask to integer tensor\"\"\"\n",
    "    def __call__(self, img, mask):\n",
    "        img = F.to_tensor(img)  # Image is converted to a floating-point tensor\n",
    "        mask = torch.as_tensor(np.array(mask), dtype = torch.long)  # Mask is converted to integer tensor\n",
    "        return img, mask\n",
    "    \n",
    "class JointNormalize(object):\n",
    "    \"\"\"Normalize only the image, not the target mask.\"\"\"\n",
    "    def __init__(self, mean, std):\n",
    "        self.normalize = transforms.Normalize(mean=mean, std=std)\n",
    "\n",
    "    def __call__(self, img, mask):\n",
    "        img = self.normalize(img)\n",
    "        return img, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`JointRandomRotation` ê°™ì´ ë³€í™˜ì´ ëœë¤í•˜ê²Œ ìˆ˜í–‰ë˜ëŠ” ê²½ìš°ì—ë„ ì´ë¯¸ì§€ì™€ ë§ˆìŠ¤í¬ì— ë™ì¼í•œ ë³€í™˜ì´ ì ìš©ë˜ë„ë¡ ì„¤ì •í•˜ì—¬ ë°ì´í„°ì˜ ì¼ê´€ì„±ì„ ë³´ì¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "### <mark>ì‹¤ìŠµ</mark> JointRandomHorizontalFlip\n",
    "`F.hflip`í•¨ìˆ˜([docs](https://pytorch.org/vision/main/generated/torchvision.transforms.functional.hflip.html))ì™€ `torch.rand(1).item()` ëœë¤ ê°’ì„ ì´ìš©í•˜ì—¬ ëœë¤ ê°’ì´ `self.p`ë³´ë‹¤ <u>ì‘ìœ¼ë©´</u> horizontal flipì„ ìˆ˜í–‰í•˜ëŠ” Transformí•¨ìˆ˜ `JointRandomHorizontalFlip`ë¥¼ ì™„ì„±í•˜ì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JointRandomRotation(object):\n",
    "    \"\"\"Randomly rotate both image and target mask by an angle within a given range.\"\"\"\n",
    "    def __init__(self, degrees=(-10, 10)):\n",
    "        self.degrees = degrees\n",
    "\n",
    "    def __call__(self, img, mask):\n",
    "        angle = (torch.rand(1).item() * (self.degrees[1] - self.degrees[0])) + self.degrees[0]\n",
    "        img = F.rotate(img, angle)\n",
    "        mask = F.rotate(mask, angle, interpolation=F.InterpolationMode.NEAREST, fill = 255)\n",
    "        return img, mask\n",
    "    \n",
    "class JointRandomHorizontalFlip(object):\n",
    "    \"\"\"Randomly flip both image and target mask horizontally with a given probability.\"\"\"\n",
    "    def __init__(self, p=0.5):\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, img, mask):\n",
    "        ##### YOUR CODE START #####\n",
    "\n",
    "        ##### YOUR CODE END #####\n",
    "        return img, mask\n",
    "    \n",
    "class JointRandomCrop(object):\n",
    "    \"\"\"Randomly crop both image and target mask to the specified size.\"\"\"\n",
    "    def __init__(self, size, pad_fill_value=255):\n",
    "        self.size = size\n",
    "        self.fill_value = pad_fill_value #  Fill padding value (255 for void class)\n",
    "        self.padding = True\n",
    "\n",
    "    def __call__(self, img, mask):\n",
    "        img_w, img_h = img.size\n",
    "        crop_w, crop_h = self.size\n",
    "\n",
    "        if self.padding and (img_w < crop_w or img_h < crop_h):\n",
    "            padding = [0, 0, max(0, crop_w - img_w), max(0, crop_h - img_h)]\n",
    "            img = F.pad(img, padding)\n",
    "            mask = F.pad(mask, padding, fill=self.fill_value)\n",
    "\n",
    "        #i, j, h, w = transforms.RandomCrop.get_params(img, output_size=self.size)\n",
    "        i = torch.randint(0, max(0, img_h - crop_h) + 1, (1,)).item()\n",
    "        j = torch.randint(0, max(0, img_w - crop_w) + 1, (1,)).item()\n",
    "        img = F.crop(img, i, j, crop_h, crop_w)\n",
    "        mask = F.crop(mask, i, j, crop_h, crop_w)\n",
    "        return img, mask\n",
    "\n",
    "class JointRandomRescale(object):\n",
    "    \"\"\"Randomly rescale both image and target mask by a factor.\"\"\"\n",
    "    def __init__(self, scale_range=(0.5, 2.0)):\n",
    "        self.scale_range = scale_range\n",
    "\n",
    "    def __call__(self, img, mask):\n",
    "        scale_factor = torch.rand(1).item() * (self.scale_range[1] - self.scale_range[0]) + self.scale_range[0]\n",
    "        img_w, img_h = img.size\n",
    "        new_w, new_h = int(img_w * scale_factor), int(img_h * scale_factor)\n",
    "\n",
    "        img = F.resize(img, (new_h, new_w))\n",
    "        mask = F.resize(mask, (new_h, new_w), interpolation=F.InterpolationMode.NEAREST) \n",
    "        return img, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_VOC_Segmentation_datasets(data_root_dir):\n",
    "    normalize = JointNormalize(mean=[0.485, 0.456, 0.406],\n",
    "                               std=[0.229, 0.224, 0.225])\n",
    "    \n",
    "    train_transforms = transforms.Compose([\n",
    "        JointRandomHorizontalFlip(p=0.5),\n",
    "        JointRandomRotation(degrees = (-10, 10)),\n",
    "        JointResize((256, 256)),\n",
    "        JointRandomRescale(scale_range = (0.8, 1.2)),\n",
    "        JointRandomCrop(size=(224, 224)),\n",
    "        JointResize((224, 224)),\n",
    "        JointToTensor(),\n",
    "        normalize\n",
    "    ])\n",
    "\n",
    "    test_transforms = transforms.Compose([\n",
    "        JointResize((224, 224)),\n",
    "        JointToTensor(),\n",
    "        normalize\n",
    "    ])\n",
    "    \n",
    "    voc_train_dataset = datasets.VOCSegmentation(root=data_root_dir, year='2012', image_set='train', \n",
    "                                                 download=False, transform = None)\n",
    "    voc_test_dataset = datasets.VOCSegmentation(root=data_root_dir, year='2012', image_set='val', \n",
    "                                                download=False, transform = None)\n",
    "    \n",
    "    train_dataset = UNetDataset(voc_train_dataset, transforms=train_transforms)\n",
    "    test_dataset = UNetDataset(voc_test_dataset, transforms=test_transforms)\n",
    "\n",
    "\n",
    "    return train_dataset, test_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = load_VOC_Segmentation_datasets(data_root_dir)\n",
    "\n",
    "X, y = train_dataset[0]\n",
    "print(f\"Image shape : {X.shape}\")\n",
    "print(f\"Mask shape: {y.shape}\")\n",
    "print(f\"Mask values unique {y.unique()}\\n\")\n",
    "\n",
    "print(f\"Dataset size: Train {len(train_dataset)}, Test {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "visualize_samples(train_dataset, cols = 2, select_random= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## U-Net Architecture\n",
    "\n",
    "U-Net ì•„í‚¤í…ì²˜ëŠ” ê·¸ êµ¬ì¡°ê°€ Uìí˜•ì´ë¼ì„œ ë¶™ì—¬ì§„ ì´ë¦„ìœ¼ë¡œ, 2015ë…„ ì¢…ì–‘ ê²€ì¶œì„ ìœ„í•´ ì²˜ìŒ ì œì•ˆëœ ì´ ëª¨ë¸ì€ í˜„ì¬ê¹Œì§€ë„ ë‹¤ì–‘í•œ semantic segmentation ì‘ì—…ì— ë„ë¦¬ ì‚¬ìš©ë˜ê³  ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "U-Netì€ ê¸°ì¡´ì˜ Convolutional Networkì—ì„œ ë§ˆì§€ë§‰ fully connected ë ˆì´ì–´ë¥¼ transposed convolution ë ˆì´ì–´ë¡œ ëŒ€ì²´í•˜ì—¬, feature mapì˜ ì—…ìƒ˜í”Œë§(upsamplingì„)ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. ì´ ê³¼ì •ì„ í†µí•´ feature mapì„ ì›ë³¸ ì´ë¯¸ì§€ì˜ í¬ê¸°ë¡œ ë‹¤ì‹œ í™•ëŒ€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. \n",
    "\n",
    "í•˜ì§€ë§Œ Convolutional Networkì˜ ë§ˆì§€ë§‰ feature mapì€ ë§ì€ ê³µê°„ì  ì •ë³´ë¥¼ ì´ë¯¸ ë§ì´ ìƒì–´ë²„ë¦° ìƒíƒœì…ë‹ˆë‹¤. ë‹¨ìˆœíˆ ì—…ìƒ˜í”Œë§ë§Œ í•œë‹¤ë©´ ì„¸ë¶€ì ì¸ segmentation ê²°ê³¼ë¥¼ ì–»ê¸° ì–´ë µìŠµë‹ˆë‹¤. \n",
    "\n",
    "ì´ë¥¼ ë³´ì™„í•˜ê¸° ìœ„í•´, U-Netì€ ì…ë ¥ ì´ë¯¸ì§€ì— ëŒ€í•´ ì§„í–‰ëœ ê° conv ì—°ì‚° ìˆ˜ì™€ ë™ì¼í•œ ìˆ˜ì˜ transposed convolutionì„ ìˆ˜í–‰í•˜ê³ , skip connectionì„ ì‚¬ìš©í•˜ì—¬ ë‹¤ìš´ìƒ˜í”Œë§ ê³¼ì •ì—ì„œì˜ feature map ì •ë³´ë¥¼ ì—…ìƒ˜í”Œë§ ë ˆì´ì–´ì— ì „ë‹¬í•©ë‹ˆë‹¤. ì´ ë°©ì‹ì€ ì´ë¯¸ì§€ì˜ ì„¸ë¶€ ì •ë³´ë¥¼ ë³´ì¡´í•˜ê³ , ë” ì •í™•í•œ ë¶„í•  ê²°ê³¼ë¥¼ ì œê³µí•©ë‹ˆë‹¤.\n",
    "\n",
    "<img src=\"resources/unet.png\" style=\"width:700px;height:400;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <mark>ì‹¤ìŠµ</mark> Encoder (Downsampling Block) \n",
    "\n",
    "Encoderì—ì„œ ì´ë¯¸ì§€ëŠ” convolutional layerë¥¼ ê±°ì¹˜ë©´ì„œ ë†’ì´ì™€ ë„ˆë¹„ê°€ ê°ì†Œí•˜ê³  ì±„ë„ ìˆ˜ëŠ” ì¦ê°€í•˜ê²Œ ë©ë‹ˆë‹¤.\n",
    "\n",
    "EncoderëŠ” ë‘ê°œì˜ [Conv2d, BatchNorm2d, ReLU]ë¡œ ì´ë£¨ì–´ì§„ `DoubleConv`ì™€ `MaxPool2d`ë¥¼ ìŒ“ì•„ì„œ ë§Œë“­ë‹ˆë‹¤.\n",
    "\n",
    "1. `DoubleConv`ëŠ” ì•„ë˜ì™€ ê°™ì´ êµ¬ì„±ë˜ì–´ ìˆë‹¤.\n",
    "    - Conv2d: `out_channels`ê°œì˜ 3x3 í•„í„°ì™€ bias = False. padding = 1ë¡œ í•˜ì—¬ ì´ë¯¸ì§€ì˜ í¬ê¸°ë¥¼ ìœ ì§€í•œë‹¤\n",
    "    - BatchNorm2d\n",
    "    - ReLU\n",
    "    - Conv2d: ìœ„ì™€ ë™ì¼\n",
    "    - BatchNorm2d\n",
    "    - ReLU\n",
    "\n",
    "2. `Down` ë¸”ëŸ­ì„ ì™„ì„±í•˜ì„¸ìš”\n",
    "    - MaxPool2d: 2x2 kernel with stride 2\n",
    "    - DoubleConv\n",
    "    - <u>if `dropout_prob` > 0</u>, add [nn.Dropout2d](https://pytorch.org/docs/stable/generated/torch.nn.Dropout2d.html) layer with p = `dropout_prob`\n",
    "    - `nn.Sequential`ì€ ì—¬ëŸ¬ ë ˆì´ì–´ë“¤ì„ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰í•˜ë„ë¡ ë¬¶ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. `*layers`ëŠ” ë¦¬ìŠ¤íŠ¸ë¡œ ì •ì˜ëœ ë ˆì´ì–´ë“¤ì„ nn.Sequentialì— ê°œë³„ì ìœ¼ë¡œ ì „ë‹¬í•©ë‹ˆë‹¤. ì¦‰, `nn.Sequential(*layers)`ëŠ” layers ë¦¬ìŠ¤íŠ¸ì— ìˆëŠ” ë ˆì´ì–´ë“¤ì´ ìˆœì„œëŒ€ë¡œ ì‹¤í–‰ë˜ë„ë¡ ì„¤ì •í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"(convolution => BN => ReLU) * 2\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.double_conv(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Down(nn.Module):\n",
    "    \"\"\"Downscaling with 2x2 maxpool then DoubleConv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, dropout_prob = .0):\n",
    "        super().__init__()\n",
    "\n",
    "        ##### YOUR CODE START #####\n",
    "\n",
    "        ##### YOUR CODE END #####\n",
    "      \n",
    "        self.maxpool_conv = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.maxpool_conv(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "down_block = Down(in_channels=64, out_channels=128, dropout_prob=0.3)\n",
    "\n",
    "input = torch.randn(16, 64, 32, 32) \n",
    "output = down_block(input)\n",
    "\n",
    "print(\"Input shape:\", input.shape)\n",
    "print(\"Output shape:\", output.shape)\n",
    "\n",
    "assert output.shape == (16, 128, 16, 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <mark>ì‹¤ìŠµ</mark> Decoder\n",
    "\n",
    "íŒ½ì°½ ë‹¨ê³„ì—ì„œëŠ” ìˆ˜ì¶•ë‹¨ê³„ì™€ ë°˜ëŒ€ë¡œ ì´ë¯¸ì§€ì˜ í¬ê¸°ë¥¼ ë‹¤ì‹œ ì›ë³¸ ì´ë¯¸ì§€ì˜ í¬ê¸°ë¡œ í‚¤ìš°ë©° ì±„ë„ ìˆ˜ë¥¼ ì ì°¨ ì¤„ì¸ë‹¤.\n",
    "\n",
    "ë¨¼ì € transposed convolutionì„ ì´ìš©í•˜ì—¬ upsamplingì„ ìˆ˜í–‰í•œ ë’¤ encoder blockì—ì„œì˜ ì¶œë ¥ê³¼ í•©ì³(concatenate), `DoubleConv`ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n",
    "\n",
    "Arguments:\n",
    "- `upsampling_input`: Decoder blockì˜ ì´ì „ ë ˆì´ì–´ì—ì„œì˜ ì…ë ¥\n",
    "- `skip_connection` Encoder blockìœ¼ë¡œ ë¶€í„° ì˜¤ëŠ” ì…ë ¥\n",
    "\n",
    "Steps:\n",
    "- [nn.ConvTranspose2d](https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html): kernel size 2x2 and stride 2. ì±„ë„ ìˆ˜ëŠ” ì ˆë°˜ìœ¼ë¡œ ì¤„ì–´ë“ ë‹¤\n",
    "- skip connections: `skip_connection`ì™€ ConvTranspose2ë¥¼ ê±°ì¹œ `upsampling_input`ë¥¼ concatenationí•œë‹¤. ì¼ë°˜ì ìœ¼ë¡œ concat ìˆœì„œëŠ” ìƒê´€ì—†ì§€ë§Œ ì½”ë“œ í…ŒìŠ¤íŠ¸ì„ ìœ„í•´ <u>[`skip_connection`, `upsampling_input`]ì˜ ìˆœì„œë¡œ</u> concatí•  ê²ƒ.\n",
    "- DoubleConv with output channels `out_channels`\n",
    "\n",
    "(ì°¸ê³ ) ë§Œì•½ `skip_connection`ê³¼ `upsampling_input`ì˜ ê³µê°„ ì°¨ì›ì´ ë§ì§€ ì•Šìœ¼ë©´ ë‘˜ì¤‘ í•˜ë‚˜ë¥¼ ì˜ë¼ë‚´ê±°ë‚˜(crop) paddingì„ ë¶™ì—¬ë„£ì–´ ì°¨ì›ì„ ë§ì¶°ì¤€ë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Up(nn.Module):\n",
    "    \"\"\"Upscaling then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        ##### YOUR CODE START #####\n",
    "\n",
    "        ##### YOUR CODE END #####\n",
    "\n",
    "    def forward(self, upsampling_input, skip_connection):\n",
    "        ##### YOUR CODE START #####\n",
    "\n",
    "        ##### YOUR CODE END #####\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "up_block = Up(in_channels=128, out_channels=64)\n",
    "\n",
    "upsampling_input = torch.randn(16, 128, 32, 32) \n",
    "skip_connection = torch.randn(16, 64, 64, 64)\n",
    "output = up_block(upsampling_input, skip_connection)\n",
    "\n",
    "print(\"Input shape:\", input.shape)\n",
    "print(\"Output shape:\", output.shape)\n",
    "\n",
    "assert output.shape == (16, 64, 64, 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <mark>ì‹¤ìŠµ</mark> U-Net\n",
    "\n",
    "<img src=\"resources/unet.png\" style=\"width:700px;height:400;\">\n",
    "\n",
    "ìœ„ ì´ë¯¸ì§€ë¥¼ ì°¸ê³ í•˜ì—¬ `UNet`ëª¨ë¸ì„ ì™„ì„±í•˜ì„¸ìš”\n",
    "- ë§ˆì§€ë§‰ ë ˆì´ì–´ì—ì„œëŠ” 1x1 convolutionì„ ì´ìš©í•˜ì—¬ feature vectorë¥¼ classìˆ˜ `num_classes`ë¡œ ë§¤í•‘í•©ë‹ˆë‹¤\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        ##### YOUR CODE START #####\n",
    "\n",
    "        ##### YOUR CODE END #####\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        ##### YOUR CODE START #####\n",
    "\n",
    "        ##### YOUR CODE END #####\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# unit test\n",
    "model = UNet(in_channels = 3, num_classes = 21)\n",
    "assert model(torch.randn(4, 3, 224, 224)).shape == torch.Size((4, 21, 224, 224)), \"output shape does not match\"\n",
    "assert sum(p.numel() for p in model.parameters()) == 31038933, \"Number of model parameter does not match\"\n",
    "\n",
    "print(\"\\033[92m All test passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model_name, num_classes, config):\n",
    "    if model_name == \"UNet\":\n",
    "        model = UNet(in_channels = 3, num_classes = num_classes)\n",
    "    else:\n",
    "        raise Exception(\"Model not supported: {}\".format(model_name))\n",
    "    \n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "    print(f\"Using model {model_name} with {total_params} parameters ({trainable_params} trainable)\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <mark>ì‹¤ìŠµ</mark> mIoU\n",
    "\n",
    "Sementic segmentation í‰ê°€ë¥¼ ìœ„í•´ì„œëŠ” ì£¼ë¡œ mIoU (mean IoU)ë¥¼ ì‚¬ìš©í•œë‹¤.\n",
    "\n",
    "í´ë˜ìŠ¤ $c$ ì— ëŒ€í•œ IoUê°’ì€ ë‹¤ìŒê³¼ ê°™ì´ ì£¼ì–´ì§„ë‹¤\n",
    "$$ IoU(c) = \\frac{Intersection(c)}{Union(c)} = \\frac{TP_c}{TP_cÂ +Â FP_cÂ +Â FN_c} $$\n",
    "\n",
    "\n",
    "- $TP_c$(True Positives): class $c$ë¡œ ì˜³ë°”ë¥´ê²Œ ì˜ˆì¸¡í•œ í”½ì…€ì˜ ìˆ˜\n",
    "- $FC_c$ (False Positives): ì‹¤ì œë¡œ ë‹¤ë¥¸ classì— ì†í•˜ì§€ë§Œ class $c$ë¡œ í‹€ë¦¬ê²Œ ì˜ˆì¸¡ëœ í”½ì…€ì˜ ìˆ˜\n",
    "- $FN_c$(False Negatives) : ì‹¤ì œë¡œ class $c$ì— ì†í•˜ì§€ë§Œ ë‹¤ë¥¸ classì— ì†í•˜ëŠ” ê²ƒìœ¼ë¡œ ì˜ˆì¸¡ëœ í•…ì…€ì˜ ìˆ˜\n",
    "- $Intersection(c)$: ì˜ˆì¸¡ê³¼ ground truthê°€ ëª¨ë‘ $c$ì¸ í”½ì…€ì˜ ìˆ˜.\n",
    "- $Union(c)$: ì˜ˆì¸¡ê³¼ ground truth ë‘˜ì¤‘ í•˜ë‚˜ê°€ $c$ì¸ í”½ì…€ì˜ ìˆ˜.\n",
    "- (ì°¸ê³ ) IoUê°’ì€ Jaccard Indexì™€ ê°™ì€ ê°’ì„\n",
    "\n",
    "ë§Œì•½ IoUê°€ 1ì´ë©´ ì˜ˆì¸¡ê³¼ ì‹¤ì œ(ground truth) maskê°€ ì™„ì „íˆ ë™ì¼í•œ ê²ƒì„ ì˜ë¯¸í•œë‹¤.\n",
    "\n",
    "IoUê°’ì„ ê°ê°ì˜ classì— ëŒ€í•´ì„œ ê³„ì‚°í•œ ë’¤ ì´ì— ëŒ€í•œ í‰ê· ì„ ê³„ì‚°í•˜ì—¬ mIoUê°’ì„ ê³„ì‚°í•œë‹¤ \n",
    "\n",
    "mean IoU (mIoU) over $ğ‘$ classes:\n",
    "$$mIoU = \\frac{1}{|C_{valid}|}\\sum_{c \\in C_{valid}}{IoU(c)}$$\n",
    "\n",
    "- $C_{valid}$: $Union(c) > 0$ ë§Œì¡±í•˜ëŠ” classë“¤ì˜ ì§‘í•©ë‹ˆë‹¤ (ì˜ˆì¸¡ í˜¹ì€ ground truth ë‘˜ì¤‘ í•˜ë‚˜ì— í•´ë‹¹ classê°€ ë‚˜íƒ€ë‚¨ì„ ì˜ë¯¸)\n",
    "\n",
    "\n",
    "ìœ„ ì •ì˜ë¥¼ ì°¸ê³ í•˜ì—¬ í•¨ìˆ˜ `calculate_mIoU`ë¥¼ ì™„ì„±í•˜ì„¸ìš”\n",
    "- ë¨¼ì € `output` í…ì„œë¡œ ë¶€í„° ì˜ˆì¸¡ëœ class indexë¥¼ ì–»ëŠ”ë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mIoU(output, target, num_classes):\n",
    "    \n",
    "    _, preds = torch.max(output, dim=1)\n",
    "\n",
    "    iou_list = []\n",
    "    for cls in range(num_classes):\n",
    "        pred_mask = (preds == cls)\n",
    "        target_mask = (target == cls)\n",
    "        \n",
    "        ##### YOUR CODE START #####\n",
    "        intersection = None #TODO\n",
    "        union = None #TODO\n",
    "        ##### YOUR CODE END #####\n",
    "\n",
    "        if union != 0:\n",
    "            iou = intersection / union\n",
    "            iou_list.append(iou)\n",
    "    \n",
    "    return sum(iou_list) / len(iou_list) if iou_list else 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_loop(model, device, dataloader, criterion, epoch = 0, phase = \"validation\"):\n",
    "    loss_meter = AverageMeter('Loss', ':.4e')\n",
    "    pixel_acc_meter = AverageMeter('Pixel_Acc', ':6.2f')\n",
    "    mIoU_meter = AverageMeter('mIoU', ':6.4f')\n",
    "    metrics_list = [loss_meter, pixel_acc_meter, mIoU_meter]\n",
    "\n",
    "    model.eval() # switch to evaluate mode\n",
    "\n",
    "    with torch.no_grad():\n",
    "        tqdm_val = tqdm(dataloader, desc='Validation/Test', total=len(dataloader))\n",
    "        for images, target in tqdm_val:\n",
    "            images = images.to(device, non_blocking=True)\n",
    "            target = target.to(device, non_blocking=True)\n",
    "\n",
    "            output = model(images)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            # calculate metrics\n",
    "            pixel_acc = calculate_pixel_accuracy(output, target)\n",
    "            mIoU = calculate_mIoU(output, target, model.num_classes)\n",
    "            \n",
    "            # Update the AverageMeters\n",
    "            loss_meter.update(loss.item(), images.size(0))\n",
    "            pixel_acc_meter.update(pixel_acc, images.size(0))\n",
    "            mIoU_meter.update(mIoU, images.size(0))\n",
    "\n",
    "            tqdm_val.set_postfix(avg_metrics = \", \".join([str(x) for x in metrics_list]))\n",
    "\n",
    "        tqdm_val.close()\n",
    "\n",
    "    wandb.log({\n",
    "        \"epoch\" : epoch,\n",
    "        f\"{phase.capitalize()} Loss\": loss_meter.avg, \n",
    "        f\"{phase.capitalize()} Pixel Acc\": pixel_acc_meter.avg,\n",
    "        f\"{phase.capitalize()} mIoU\": mIoU_meter.avg,\n",
    "    })\n",
    "\n",
    "    return mIoU_meter.avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training (ëª¨ë¸ í•™ìŠµ)\n",
    "\n",
    "### Ignoring Unlabelled Pixels (Index 255)\n",
    "ë¼ë²¨ë§ ë˜ì§€ ì•Šì€ pixelë“¤ì„ Lossê³„ì‚°ì—ì„œ ì œì™¸í•˜ê¸° ìœ„í•´ CrossEntropyLossì˜ `ignore_index` argumentë¥¼ ì´ìš©í•œë‹¤\n",
    "```\n",
    "nn.CrossEntropyLoss(ignore_index=255)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_main(config):\n",
    "    ## data and preprocessing settings\n",
    "    data_root_dir = config['data_root_dir']\n",
    "    num_worker = config.get('num_worker', 4)\n",
    "\n",
    "    ## Hyper parameters\n",
    "    batch_size = config['batch_size']\n",
    "    learning_rate = config['learning_rate']\n",
    "    start_epoch = config.get('start_epoch', 0)\n",
    "    num_epochs = config['num_epochs']\n",
    "    eval_interval = config.get('eval_interval', 10)\n",
    "\n",
    "\n",
    "    ## checkpoint setting\n",
    "    checkpoint_path = config.get('checkpoint_path', \"checkpoints/checkpoint.pth\")\n",
    "    best_model_path = config.get('best_model_path', \"checkpoints/best_model.pth\")\n",
    "    load_from_checkpoint = config.get('load_from_checkpoint', None)\n",
    "\n",
    "    ## variables\n",
    "    best_metric = 0\n",
    "\n",
    "    wandb.init(\n",
    "        project=config[\"wandb_project_name\"],\n",
    "        config=config\n",
    "    )\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "    print(f\"Using {device} device\")\n",
    "\n",
    "    train_dataset, test_dataset = load_VOC_Segmentation_datasets(data_root_dir)\n",
    "    num_classes = len(train_dataset.classes)\n",
    "    \n",
    "    train_dataloader, test_dataloader = create_dataloaders(train_dataset, test_dataset, device, \n",
    "                                                           batch_size = batch_size, num_worker = num_worker)\n",
    "\n",
    "\n",
    "    \n",
    "    model = get_model(model_name = config[\"model_name\"], num_classes= num_classes, config = config).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=255)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.1) \n",
    "\n",
    "    if load_from_checkpoint:\n",
    "        load_checkpoint_path = (best_model_path if load_from_checkpoint == \"best\" else checkpoint_path)\n",
    "        start_epoch, best_metric = load_checkpoint(load_checkpoint_path, model, optimizer, scheduler, device)\n",
    "\n",
    "    if config.get('test_mode', False):\n",
    "        # Only evaluate on the test dataset\n",
    "        print(\"Running test evaluation...\")\n",
    "        test_metric = evaluation_loop(model, device, test_dataloader, criterion, phase = \"test\")\n",
    "        print(f\"Test metric (mIoU): {test_metric}\")\n",
    "        \n",
    "    else:\n",
    "        # Train and validate using train/val datasets\n",
    "        for epoch in range(start_epoch, num_epochs):\n",
    "            train_loop(model, device, train_dataloader, criterion, optimizer, epoch)\n",
    "\n",
    "\n",
    "            if (epoch + 1) % eval_interval == 0 or (epoch + 1) == num_epochs:\n",
    "                test_metric = evaluation_loop(model, device, test_dataloader, criterion, epoch = epoch, phase = \"validation\")\n",
    "\n",
    "                is_best = test_metric > best_metric\n",
    "                best_metric = max(test_metric, best_metric)\n",
    "                save_checkpoint(checkpoint_path, model, optimizer, scheduler, epoch, best_metric, is_best, best_model_path)\n",
    "\n",
    "            scheduler.step()\n",
    "\n",
    "\n",
    "    wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    'data_root_dir': '/datasets',\n",
    "    'batch_size': 16,\n",
    "    'learning_rate': 1e-3,\n",
    "    'model_name': 'UNet',\n",
    "    'num_epochs': 150,\n",
    "    \"eval_interval\" : 10,\n",
    "\n",
    "    \"dataset\": \"VOC2012\",\n",
    "    'wandb_project_name': 'UNet',\n",
    "\n",
    "    \"checkpoint_path\" : \"checkpoints/checkpoint.pth\",\n",
    "    \"best_model_path\" : \"checkpoints/best_model.pth\",\n",
    "    \"load_from_checkpoint\" : None,    # Options: \"latest\", \"best\", or None\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "train_main(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize your model's prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_prediction(X, y, y_pred):\n",
    "    \"\"\"\n",
    "    Visualize a few samples from the VOCSegmentation dataset, showing both the input image and its corresponding label (segmentation mask).\n",
    "\n",
    "    Parameters:\n",
    "        dataset: A dataset object, e.g., VOCSegmentation, where each item is a tuple (image, label).\n",
    "        cols (int): Number of columns in the visualization grid.\n",
    "        rows (int): Number of rows in the visualization grid.\n",
    "    \"\"\"\n",
    "    figure, ax = plt.subplots(nrows=X.shape[0], ncols=3, figsize=(12, X.shape[0] * 3))\n",
    "    ax = ax.flatten()\n",
    "\n",
    "    for i, idx in enumerate(range(X.shape[0])):\n",
    "        # Get the image and label (segmentation mask)\n",
    "        img, mask, mask_pred = X[i], y[i], y_pred[i]\n",
    "\n",
    "        # unnormalize image\n",
    "        mean = torch.tensor([0.485, 0.456, 0.406]).to(img.device).view(-1, 1, 1)\n",
    "        std = torch.tensor([0.229, 0.224, 0.225]).to(img.device).view(-1, 1, 1)\n",
    "        img = img * std + mean\n",
    "        \n",
    "\n",
    "        # Display the image\n",
    "        ax[3 * i].imshow(img.numpy().transpose((1, 2, 0)))\n",
    "        ax[3 * i].set_title(f\"Image {i+1}\")\n",
    "        ax[3 * i].axis(\"off\")\n",
    "\n",
    "        # Display the segmentation mask (assuming it's a single-channel mask)\n",
    "        mask = mask_tensor_to_pil(mask)\n",
    "        mask_pred = mask_tensor_to_pil(mask_pred)\n",
    "            \n",
    "        ax[3 * i + 1].imshow(mask, cmap=\"gray\")\n",
    "        ax[3 * i + 1].set_title(f\"Label {i+1}\")\n",
    "        ax[3 * i + 1].axis(\"off\")\n",
    "\n",
    "        ax[3 * i + 2].imshow(mask_pred, cmap=\"gray\")\n",
    "        ax[3 * i + 2].set_title(f\"Prediction {i+1}\")\n",
    "        ax[3 * i + 2].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "train_dataset, test_dataset = load_VOC_Segmentation_datasets(config['data_root_dir'])\n",
    "\n",
    "num_classes = len(train_dataset.classes)\n",
    "train_dataloader, test_dataloader = create_dataloaders(train_dataset, test_dataset, device, \n",
    "                                                       batch_size = 16, num_worker = 4)\n",
    "model = get_model(model_name = config[\"model_name\"], num_classes= num_classes, config = config).to(device)\n",
    "\n",
    "\n",
    "model_checkpoint_path = config[\"best_model_path\"]\n",
    "checkpoint = torch.load(model_checkpoint_path, map_location=device)\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "print(f\"=> loaded checkpoint '{model_checkpoint_path}' with mIoU {checkpoint['best_metric']} (epoch {checkpoint['epoch']})\")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "print(\"Model ready for inference\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "images, targets = next(iter(test_dataloader))\n",
    "images = images.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(images)\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "visualize_prediction(images.cpu(), targets.cpu(), preds.cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optional ì‹¤ìŠµ\n",
    "mIoUì„±ëŠ¥ ê°œì„ ì„ ìœ„í•œ ë‹¤ì–‘í•œ ì‹¤í—˜ì„ ì‹œë„í•´ë³´ì„¸ìš”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì •ë¦¬\n",
    "Labì„ ë§ˆë¬´ë¦¬ ì§“ê¸° ì „ ì €ì¥ëœ checkpointë¥¼ ëª¨ë‘ ì§€ì›Œ ì €ì¥ê³µê°„ì„ í™•ë³´í•œë‹¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "import shutil, os\n",
    "if os.path.exists('checkpoints/'):\n",
    "    shutil.rmtree('checkpoints/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2.3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
