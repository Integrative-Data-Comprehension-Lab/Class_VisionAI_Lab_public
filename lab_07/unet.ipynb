{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Segmentation with U-Net\n",
    "\n",
    "이번 시간에는 U-Net 모델을 이용하여 image segmentation 작업을 진행해보자.\n",
    "\n",
    "Semantic image segmenataion은 pixel수준으로 이미지의 레이블을 예측하는 문제를 지칭합니다. 다시 말해, 단순히 물체가 이미지에 존재하는지를 예측하는 것이 아니라, 각 픽셀이 어떤 클래스에 속하는지를 파악합니다.\n",
    "\n",
    "Segmentation은 Object detection과 유사하게 \"주어진 이미지에 어떤 물체가 존재하고 어디에 위치하는가?\"라는 질문에 답합니다. 그러나 object detection은 물체를 bounding box로 감싸기 때문에, 박스 내에 물체가 아닌 픽셀도 포함될 수 있습니다. \n",
    "\n",
    "반면에, semantic image segmentation은 픽셀 단위로 정확한 물체의 마스크를 얻을 수 있어 더 세밀한 정보를 제공합니다.\n",
    "\n",
    "<img src=\"resources/carseg.png\" style=\"width:500px;height:250;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, models\n",
    "import torchvision.transforms.v2 as transforms\n",
    "\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "from training_utilities import create_dataloaders, train_loop, calculate_pixel_accuracy, AverageMeter, save_checkpoint, load_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "data_root_dir = '/datasets'\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "voc_train_dataset = datasets.VOCSegmentation(root=data_root_dir, year='2012', image_set='train', download=False,\n",
    "                                          transform = train_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "먼저 Pascal VOC 2012 Segmentation 데이터셋을 불러오자.\n",
    "\n",
    "이 dataset은 RGB 이미지와 이에 대응되는 mask를 리턴합니다.\n",
    "mask에는 픽셀 수준으로 class를 지칭하는 정수값이 들어있습니다.\n",
    "\n",
    "- 0 : background\n",
    "- 255 : 'void' or unlabelled.\n",
    "- 1~20 : 20 classes\n",
    "\n",
    "\n",
    "한편, 이미지의 크기를 보면 transform이 image에만 적용되고 target에는 적용되지 않은 것을 확인할 수 있습니다.\n",
    "\n",
    "target에도 transform을 적용하려면 새로운 데이터셋과 custom transform함수 정의가 필요합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "idx = 0\n",
    "image, target = voc_train_dataset[idx]\n",
    "mask_palette = target.getpalette()\n",
    "\n",
    "print(f\"Image.shape = {image.shape}, target.shape = {np.array(target).shape}\")\n",
    "print(f\"Target values unique : {np.unique(target)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def mask_tensor_to_pil(mask):\n",
    "    mask_np = mask.numpy().astype(np.uint8)\n",
    "    mask_pil = Image.fromarray(mask_np, mode='P')\n",
    "    mask_pil.putpalette(mask_palette)\n",
    "    return(mask_pil)\n",
    "\n",
    "def visualize_samples(dataset, cols=4, rows=3, select_random = True):\n",
    "    \"\"\"\n",
    "    Visualize a few samples from the VOCSegmentation dataset, showing both the input image and its corresponding label (segmentation mask).\n",
    "\n",
    "    Parameters:\n",
    "        dataset: A dataset object, e.g., VOCSegmentation, where each item is a tuple (image, label).\n",
    "        cols (int): Number of columns in the visualization grid.\n",
    "        rows (int): Number of rows in the visualization grid.\n",
    "    \"\"\"\n",
    "    figure, ax = plt.subplots(nrows=rows, ncols=cols * 2, figsize=(12, 6))\n",
    "    ax = ax.flatten()\n",
    "\n",
    "    if select_random:\n",
    "        indices = random.sample(range(len(dataset)), cols * rows)\n",
    "    else:\n",
    "        indices = range(cols * rows)\n",
    "    \n",
    "    \n",
    "    for i, idx in enumerate(indices):\n",
    "        # Get the image and label (segmentation mask)\n",
    "        img, mask = dataset[idx]\n",
    "\n",
    "        # unnormalize image\n",
    "        mean = torch.tensor([0.485, 0.456, 0.406]).to(img.device).view(-1, 1, 1)\n",
    "        std = torch.tensor([0.229, 0.224, 0.225]).to(img.device).view(-1, 1, 1)\n",
    "        img = img * std + mean\n",
    "        \n",
    "\n",
    "        # Display the image\n",
    "        ax[2 * i].imshow(img.numpy().transpose((1, 2, 0)))\n",
    "        ax[2 * i].set_title(f\"Image {i+1}\")\n",
    "        ax[2 * i].axis(\"off\")\n",
    "\n",
    "        # Display the segmentation mask (assuming it's a single-channel mask)\n",
    "        if isinstance(mask, torch.Tensor):\n",
    "            mask = mask_tensor_to_pil(mask)\n",
    "\n",
    "        ax[2 * i + 1].imshow(mask, cmap=\"gray\")\n",
    "        ax[2 * i + 1].set_title(f\"Label {i+1}\")\n",
    "        ax[2 * i + 1].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "visualize_samples(voc_train_dataset, cols=2, rows=3, select_random = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이미지 분류(classification) 문제와는 달리, segmentation 문제에서는 이미지와 타겟 마스크에 동일한 변환(transform)을 적용해야 합니다. \n",
    "\n",
    "예를 들어, 이미지에 수평 반전을 적용했다면, 타겟 마스크에도 동일한 변환을 수행하여 이미지와 레이블이 일치하도록 해야 합니다.\n",
    "\n",
    "또한, 데이터 증강(data augmentation)이 랜덤하게 수행되는 경우에도 이미지와 마스크에 동일한 랜덤 변환이 적용되도록 설정해야 합니다. 이렇게 해야만 데이터의 일관성을 유지할 수 있습니다.\n",
    "\n",
    "새로운 데이터셋 `UNetDataset`은 `VOCSegmentation`데이터셋에서 `image`와 `target`을 읽어와 동시에 `transforms`함수에 전달한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNetDataset(Dataset):\n",
    "    def __init__(self, voc_dataset, transforms=None):\n",
    "        self.dataset = voc_dataset\n",
    "        self.transforms = transforms\n",
    "\n",
    "        self.classes = [\"background\", \"aeroplane\", \"bicycle\", \"bird\", \"boat\", \"bottle\",\n",
    "                        \"bus\", \"car\", \"cat\", \"chair\", \"cow\", \"diningtable\",\n",
    "                        \"dog\", \"horse\", \"motorbike\", \"person\", \"pottedplant\",\n",
    "                        \"sheep\", \"sofa\", \"train\", \"tvmonitor\"]\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, target = self.dataset[idx]\n",
    "\n",
    "        if self.transforms:\n",
    "            image, target = self.transforms(image, target)\n",
    "\n",
    "        return image, target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom transform함수를 정의하기 위해서는 `__init__` 과 `__call__`을 메서드를 포함한 호출 가능한(callable) 클래스를 만들면 됩니다.\n",
    "\n",
    "아래 코드는 transform함수 호출시(`__call__`) 전달받은 `img`와 `mask`에 동일한 변환을 수행하는 transform함수의 예시입니다.\n",
    "\n",
    "1. JointResize: 이미지와 타겟 마스크의 크기를 모두 변경합니다.\n",
    "   - 마스크는 레이블 손실이 없도록 최근접 이웃 보간 (NEAREST interpolation)을 사용합니다.\n",
    "2. JointToTensor: 이미지를 텐서로 변환하고, 마스크를 정수형 텐서로 변환합니다.\n",
    "   - 이미지를 0과 1 사이의 값을 가지는 텐서로 변환하고, 마스크는 레이블 정보를 가지는 정수형 텐서로 변환합니다.\n",
    "3. JointNormalize: 이미지를 주어진 평균과 표준편차로 정규화합니다. 마스크는 변형하지 않습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms.functional as F\n",
    "\n",
    "class JointResize(object):\n",
    "    \"\"\"Resize both image and target mask to the given size.\"\"\"\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "\n",
    "    def __call__(self, img, mask):\n",
    "        img = F.resize(img, self.size)\n",
    "        mask = F.resize(mask, self.size, interpolation=transforms.functional.InterpolationMode.NEAREST)\n",
    "        return img, mask\n",
    "\n",
    "class JointToTensor(object):\n",
    "    \"\"\"Convert PIL image to tensor and mask to integer tensor\"\"\"\n",
    "    def __call__(self, img, mask):\n",
    "        img = F.to_tensor(img)  # Image is converted to a floating-point tensor\n",
    "        mask = torch.as_tensor(np.array(mask), dtype = torch.long)  # Mask is converted to integer tensor\n",
    "        return img, mask\n",
    "    \n",
    "class JointNormalize(object):\n",
    "    \"\"\"Normalize only the image, not the target mask.\"\"\"\n",
    "    def __init__(self, mean, std):\n",
    "        self.normalize = transforms.Normalize(mean=mean, std=std)\n",
    "\n",
    "    def __call__(self, img, mask):\n",
    "        img = self.normalize(img)\n",
    "        return img, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`JointRandomRotation` 같이 변환이 랜덤하게 수행되는 경우에도 이미지와 마스크에 동일한 변환이 적용되도록 설정하여 데이터의 일관성을 보장할 수 있습니다.\n",
    "\n",
    "### <mark>실습</mark> JointRandomHorizontalFlip\n",
    "`F.hflip`함수([docs](https://pytorch.org/vision/main/generated/torchvision.transforms.functional.hflip.html))와 `torch.rand(1).item()` 랜덤 값을 이용하여 랜덤 값이 `self.p`보다 <u>작으면</u> horizontal flip을 수행하는 Transform함수 `JointRandomHorizontalFlip`를 완성하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JointRandomRotation(object):\n",
    "    \"\"\"Randomly rotate both image and target mask by an angle within a given range.\"\"\"\n",
    "    def __init__(self, degrees=(-10, 10)):\n",
    "        self.degrees = degrees\n",
    "\n",
    "    def __call__(self, img, mask):\n",
    "        angle = (torch.rand(1).item() * (self.degrees[1] - self.degrees[0])) + self.degrees[0]\n",
    "        img = F.rotate(img, angle)\n",
    "        mask = F.rotate(mask, angle, interpolation=F.InterpolationMode.NEAREST, fill = 255)\n",
    "        return img, mask\n",
    "    \n",
    "class JointRandomHorizontalFlip(object):\n",
    "    \"\"\"Randomly flip both image and target mask horizontally with a given probability.\"\"\"\n",
    "    def __init__(self, p=0.5):\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, img, mask):\n",
    "        ##### YOUR CODE START #####\n",
    "\n",
    "        ##### YOUR CODE END #####\n",
    "        return img, mask\n",
    "    \n",
    "class JointRandomCrop(object):\n",
    "    \"\"\"Randomly crop both image and target mask to the specified size.\"\"\"\n",
    "    def __init__(self, size, pad_fill_value=255):\n",
    "        self.size = size\n",
    "        self.fill_value = pad_fill_value #  Fill padding value (255 for void class)\n",
    "        self.padding = True\n",
    "\n",
    "    def __call__(self, img, mask):\n",
    "        img_w, img_h = img.size\n",
    "        crop_w, crop_h = self.size\n",
    "\n",
    "        if self.padding and (img_w < crop_w or img_h < crop_h):\n",
    "            padding = [0, 0, max(0, crop_w - img_w), max(0, crop_h - img_h)]\n",
    "            img = F.pad(img, padding)\n",
    "            mask = F.pad(mask, padding, fill=self.fill_value)\n",
    "\n",
    "        #i, j, h, w = transforms.RandomCrop.get_params(img, output_size=self.size)\n",
    "        i = torch.randint(0, max(0, img_h - crop_h) + 1, (1,)).item()\n",
    "        j = torch.randint(0, max(0, img_w - crop_w) + 1, (1,)).item()\n",
    "        img = F.crop(img, i, j, crop_h, crop_w)\n",
    "        mask = F.crop(mask, i, j, crop_h, crop_w)\n",
    "        return img, mask\n",
    "\n",
    "class JointRandomRescale(object):\n",
    "    \"\"\"Randomly rescale both image and target mask by a factor.\"\"\"\n",
    "    def __init__(self, scale_range=(0.5, 2.0)):\n",
    "        self.scale_range = scale_range\n",
    "\n",
    "    def __call__(self, img, mask):\n",
    "        scale_factor = torch.rand(1).item() * (self.scale_range[1] - self.scale_range[0]) + self.scale_range[0]\n",
    "        img_w, img_h = img.size\n",
    "        new_w, new_h = int(img_w * scale_factor), int(img_h * scale_factor)\n",
    "\n",
    "        img = F.resize(img, (new_h, new_w))\n",
    "        mask = F.resize(mask, (new_h, new_w), interpolation=F.InterpolationMode.NEAREST) \n",
    "        return img, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_VOC_Segmentation_datasets(data_root_dir):\n",
    "    normalize = JointNormalize(mean=[0.485, 0.456, 0.406],\n",
    "                               std=[0.229, 0.224, 0.225])\n",
    "    \n",
    "    train_transforms = transforms.Compose([\n",
    "        JointRandomHorizontalFlip(p=0.5),\n",
    "        JointRandomRotation(degrees = (-10, 10)),\n",
    "        JointResize((256, 256)),\n",
    "        JointRandomRescale(scale_range = (0.8, 1.2)),\n",
    "        JointRandomCrop(size=(224, 224)),\n",
    "        JointResize((224, 224)),\n",
    "        JointToTensor(),\n",
    "        normalize\n",
    "    ])\n",
    "\n",
    "    test_transforms = transforms.Compose([\n",
    "        JointResize((224, 224)),\n",
    "        JointToTensor(),\n",
    "        normalize\n",
    "    ])\n",
    "    \n",
    "    voc_train_dataset = datasets.VOCSegmentation(root=data_root_dir, year='2012', image_set='train', \n",
    "                                                 download=False, transform = None)\n",
    "    voc_test_dataset = datasets.VOCSegmentation(root=data_root_dir, year='2012', image_set='val', \n",
    "                                                download=False, transform = None)\n",
    "    \n",
    "    train_dataset = UNetDataset(voc_train_dataset, transforms=train_transforms)\n",
    "    test_dataset = UNetDataset(voc_test_dataset, transforms=test_transforms)\n",
    "\n",
    "\n",
    "    return train_dataset, test_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = load_VOC_Segmentation_datasets(data_root_dir)\n",
    "\n",
    "X, y = train_dataset[0]\n",
    "print(f\"Image shape : {X.shape}\")\n",
    "print(f\"Mask shape: {y.shape}\")\n",
    "print(f\"Mask values unique {y.unique()}\\n\")\n",
    "\n",
    "print(f\"Dataset size: Train {len(train_dataset)}, Test {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "visualize_samples(train_dataset, cols = 2, select_random= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## U-Net Architecture\n",
    "\n",
    "U-Net 아키텍처는 그 구조가 U자형이라서 붙여진 이름으로, 2015년 종양 검출을 위해 처음 제안된 이 모델은 현재까지도 다양한 semantic segmentation 작업에 널리 사용되고 있습니다.\n",
    "\n",
    "U-Net은 기존의 Convolutional Network에서 마지막 fully connected 레이어를 transposed convolution 레이어로 대체하여, feature map의 업샘플링(upsampling을)을 수행합니다. 이 과정을 통해 feature map을 원본 이미지의 크기로 다시 확대할 수 있습니다. \n",
    "\n",
    "하지만 Convolutional Network의 마지막 feature map은 많은 공간적 정보를 이미 많이 잃어버린 상태입니다. 단순히 업샘플링만 한다면 세부적인 segmentation 결과를 얻기 어렵습니다. \n",
    "\n",
    "이를 보완하기 위해, U-Net은 입력 이미지에 대해 진행된 각 conv 연산 수와 동일한 수의 transposed convolution을 수행하고, skip connection을 사용하여 다운샘플링 과정에서의 feature map 정보를 업샘플링 레이어에 전달합니다. 이 방식은 이미지의 세부 정보를 보존하고, 더 정확한 분할 결과를 제공합니다.\n",
    "\n",
    "<img src=\"resources/unet.png\" style=\"width:700px;height:400;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <mark>실습</mark> Encoder (Downsampling Block) \n",
    "\n",
    "Encoder에서 이미지는 convolutional layer를 거치면서 높이와 너비가 감소하고 채널 수는 증가하게 됩니다.\n",
    "\n",
    "Encoder는 두개의 [Conv2d, BatchNorm2d, ReLU]로 이루어진 `DoubleConv`와 `MaxPool2d`를 쌓아서 만듭니다.\n",
    "\n",
    "1. `DoubleConv`는 아래와 같이 구성되어 있다.\n",
    "    - Conv2d: `out_channels`개의 3x3 필터와 bias = False. padding = 1로 하여 이미지의 크기를 유지한다\n",
    "    - BatchNorm2d\n",
    "    - ReLU\n",
    "    - Conv2d: 위와 동일\n",
    "    - BatchNorm2d\n",
    "    - ReLU\n",
    "\n",
    "2. `Down` 블럭을 완성하세요\n",
    "    - MaxPool2d: 2x2 kernel with stride 2\n",
    "    - DoubleConv\n",
    "    - <u>if `dropout_prob` > 0</u>, add [nn.Dropout2d](https://pytorch.org/docs/stable/generated/torch.nn.Dropout2d.html) layer with p = `dropout_prob`\n",
    "    - `nn.Sequential`은 여러 레이어들을 순차적으로 실행하도록 묶는 방법입니다. `*layers`는 리스트로 정의된 레이어들을 nn.Sequential에 개별적으로 전달합니다. 즉, `nn.Sequential(*layers)`는 layers 리스트에 있는 레이어들이 순서대로 실행되도록 설정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"(convolution => BN => ReLU) * 2\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.double_conv(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Down(nn.Module):\n",
    "    \"\"\"Downscaling with 2x2 maxpool then DoubleConv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, dropout_prob = .0):\n",
    "        super().__init__()\n",
    "\n",
    "        ##### YOUR CODE START #####\n",
    "\n",
    "        ##### YOUR CODE END #####\n",
    "      \n",
    "        self.maxpool_conv = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.maxpool_conv(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "down_block = Down(in_channels=64, out_channels=128, dropout_prob=0.3)\n",
    "\n",
    "input = torch.randn(16, 64, 32, 32) \n",
    "output = down_block(input)\n",
    "\n",
    "print(\"Input shape:\", input.shape)\n",
    "print(\"Output shape:\", output.shape)\n",
    "\n",
    "assert output.shape == (16, 128, 16, 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <mark>실습</mark> Decoder\n",
    "\n",
    "팽창 단계에서는 수축단계와 반대로 이미지의 크기를 다시 원본 이미지의 크기로 키우며 채널 수를 점차 줄인다.\n",
    "\n",
    "먼저 transposed convolution을 이용하여 upsampling을 수행한 뒤 encoder block에서의 출력과 합쳐(concatenate), `DoubleConv`를 수행합니다.\n",
    "\n",
    "Arguments:\n",
    "- `upsampling_input`: Decoder block의 이전 레이어에서의 입력\n",
    "- `skip_connection` Encoder block으로 부터 오는 입력\n",
    "\n",
    "Steps:\n",
    "- [nn.ConvTranspose2d](https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html): kernel size 2x2 and stride 2. 채널 수는 절반으로 줄어든다\n",
    "- skip connections: `skip_connection`와 ConvTranspose2를 거친 `upsampling_input`를 concatenation한다. 일반적으로 concat 순서는 상관없지만 코드 테스트을 위해 <u>[`skip_connection`, `upsampling_input`]의 순서로</u> concat할 것.\n",
    "- DoubleConv with output channels `out_channels`\n",
    "\n",
    "(참고) 만약 `skip_connection`과 `upsampling_input`의 공간 차원이 맞지 않으면 둘중 하나를 잘라내거나(crop) padding을 붙여넣어 차원을 맞춰준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Up(nn.Module):\n",
    "    \"\"\"Upscaling then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        ##### YOUR CODE START #####\n",
    "\n",
    "        ##### YOUR CODE END #####\n",
    "\n",
    "    def forward(self, upsampling_input, skip_connection):\n",
    "        ##### YOUR CODE START #####\n",
    "\n",
    "        ##### YOUR CODE END #####\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "up_block = Up(in_channels=128, out_channels=64)\n",
    "\n",
    "upsampling_input = torch.randn(16, 128, 32, 32) \n",
    "skip_connection = torch.randn(16, 64, 64, 64)\n",
    "output = up_block(upsampling_input, skip_connection)\n",
    "\n",
    "print(\"Input shape:\", input.shape)\n",
    "print(\"Output shape:\", output.shape)\n",
    "\n",
    "assert output.shape == (16, 64, 64, 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <mark>실습</mark> U-Net\n",
    "\n",
    "<img src=\"resources/unet.png\" style=\"width:700px;height:400;\">\n",
    "\n",
    "위 이미지를 참고하여 `UNet`모델을 완성하세요\n",
    "- 마지막 레이어에서는 1x1 convolution을 이용하여 feature vector를 class수 `num_classes`로 매핑합니다\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        ##### YOUR CODE START #####\n",
    "\n",
    "        ##### YOUR CODE END #####\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        ##### YOUR CODE START #####\n",
    "\n",
    "        ##### YOUR CODE END #####\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# unit test\n",
    "model = UNet(in_channels = 3, num_classes = 21)\n",
    "assert model(torch.randn(4, 3, 224, 224)).shape == torch.Size((4, 21, 224, 224)), \"output shape does not match\"\n",
    "assert sum(p.numel() for p in model.parameters()) == 31038933, \"Number of model parameter does not match\"\n",
    "\n",
    "print(\"\\033[92m All test passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model_name, num_classes, config):\n",
    "    if model_name == \"UNet\":\n",
    "        model = UNet(in_channels = 3, num_classes = num_classes)\n",
    "    else:\n",
    "        raise Exception(\"Model not supported: {}\".format(model_name))\n",
    "    \n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "    print(f\"Using model {model_name} with {total_params} parameters ({trainable_params} trainable)\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <mark>실습</mark> mIoU\n",
    "\n",
    "Sementic segmentation 평가를 위해서는 주로 mIoU (mean IoU)를 사용한다.\n",
    "\n",
    "클래스 $c$ 에 대한 IoU값은 다음과 같이 주어진다\n",
    "$$ IoU(c) = \\frac{Intersection(c)}{Union(c)} = \\frac{TP_c}{TP_c + FP_c + FN_c} $$\n",
    "\n",
    "\n",
    "- $TP_c$(True Positives): class $c$로 옳바르게 예측한 픽셀의 수\n",
    "- $FC_c$ (False Positives): 실제로 다른 class에 속하지만 class $c$로 틀리게 예측된 픽셀의 수\n",
    "- $FN_c$(False Negatives) : 실제로 class $c$에 속하지만 다른 class에 속하는 것으로 예측된 핅셀의 수\n",
    "- $Intersection(c)$: 예측과 ground truth가 모두 $c$인 픽셀의 수.\n",
    "- $Union(c)$: 예측과 ground truth 둘중 하나가 $c$인 픽셀의 수.\n",
    "- (참고) IoU값은 Jaccard Index와 같은 값임\n",
    "\n",
    "만약 IoU가 1이면 예측과 실제(ground truth) mask가 완전히 동일한 것을 의미한다.\n",
    "\n",
    "IoU값을 각각의 class에 대해서 계산한 뒤 이에 대한 평균을 계산하여 mIoU값을 계산한다 \n",
    "\n",
    "mean IoU (mIoU) over $𝑁$ classes:\n",
    "$$mIoU = \\frac{1}{|C_{valid}|}\\sum_{c \\in C_{valid}}{IoU(c)}$$\n",
    "\n",
    "- $C_{valid}$: $Union(c) > 0$ 만족하는 class들의 집합니다 (예측 혹은 ground truth 둘중 하나에 해당 class가 나타남을 의미)\n",
    "\n",
    "\n",
    "위 정의를 참고하여 함수 `calculate_mIoU`를 완성하세요\n",
    "- 먼저 `output` 텐서로 부터 예측된 class index를 얻는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mIoU(output, target, num_classes):\n",
    "    \n",
    "    _, preds = torch.max(output, dim=1)\n",
    "\n",
    "    iou_list = []\n",
    "    for cls in range(num_classes):\n",
    "        pred_mask = (preds == cls)\n",
    "        target_mask = (target == cls)\n",
    "        \n",
    "        ##### YOUR CODE START #####\n",
    "        intersection = None #TODO\n",
    "        union = None #TODO\n",
    "        ##### YOUR CODE END #####\n",
    "\n",
    "        if union != 0:\n",
    "            iou = intersection / union\n",
    "            iou_list.append(iou)\n",
    "    \n",
    "    return sum(iou_list) / len(iou_list) if iou_list else 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_loop(model, device, dataloader, criterion, epoch = 0, phase = \"validation\"):\n",
    "    loss_meter = AverageMeter('Loss', ':.4e')\n",
    "    pixel_acc_meter = AverageMeter('Pixel_Acc', ':6.2f')\n",
    "    mIoU_meter = AverageMeter('mIoU', ':6.4f')\n",
    "    metrics_list = [loss_meter, pixel_acc_meter, mIoU_meter]\n",
    "\n",
    "    model.eval() # switch to evaluate mode\n",
    "\n",
    "    with torch.no_grad():\n",
    "        tqdm_val = tqdm(dataloader, desc='Validation/Test', total=len(dataloader))\n",
    "        for images, target in tqdm_val:\n",
    "            images = images.to(device, non_blocking=True)\n",
    "            target = target.to(device, non_blocking=True)\n",
    "\n",
    "            output = model(images)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            # calculate metrics\n",
    "            pixel_acc = calculate_pixel_accuracy(output, target)\n",
    "            mIoU = calculate_mIoU(output, target, model.num_classes)\n",
    "            \n",
    "            # Update the AverageMeters\n",
    "            loss_meter.update(loss.item(), images.size(0))\n",
    "            pixel_acc_meter.update(pixel_acc, images.size(0))\n",
    "            mIoU_meter.update(mIoU, images.size(0))\n",
    "\n",
    "            tqdm_val.set_postfix(avg_metrics = \", \".join([str(x) for x in metrics_list]))\n",
    "\n",
    "        tqdm_val.close()\n",
    "\n",
    "    wandb.log({\n",
    "        \"epoch\" : epoch,\n",
    "        f\"{phase.capitalize()} Loss\": loss_meter.avg, \n",
    "        f\"{phase.capitalize()} Pixel Acc\": pixel_acc_meter.avg,\n",
    "        f\"{phase.capitalize()} mIoU\": mIoU_meter.avg,\n",
    "    })\n",
    "\n",
    "    return mIoU_meter.avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training (모델 학습)\n",
    "\n",
    "### Ignoring Unlabelled Pixels (Index 255)\n",
    "라벨링 되지 않은 pixel들을 Loss계산에서 제외하기 위해 CrossEntropyLoss의 `ignore_index` argument를 이용한다\n",
    "```\n",
    "nn.CrossEntropyLoss(ignore_index=255)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_main(config):\n",
    "    ## data and preprocessing settings\n",
    "    data_root_dir = config['data_root_dir']\n",
    "    num_worker = config.get('num_worker', 4)\n",
    "\n",
    "    ## Hyper parameters\n",
    "    batch_size = config['batch_size']\n",
    "    learning_rate = config['learning_rate']\n",
    "    start_epoch = config.get('start_epoch', 0)\n",
    "    num_epochs = config['num_epochs']\n",
    "    eval_interval = config.get('eval_interval', 10)\n",
    "\n",
    "\n",
    "    ## checkpoint setting\n",
    "    checkpoint_path = config.get('checkpoint_path', \"checkpoints/checkpoint.pth\")\n",
    "    best_model_path = config.get('best_model_path', \"checkpoints/best_model.pth\")\n",
    "    load_from_checkpoint = config.get('load_from_checkpoint', None)\n",
    "\n",
    "    ## variables\n",
    "    best_metric = 0\n",
    "\n",
    "    wandb.init(\n",
    "        project=config[\"wandb_project_name\"],\n",
    "        config=config\n",
    "    )\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "    print(f\"Using {device} device\")\n",
    "\n",
    "    train_dataset, test_dataset = load_VOC_Segmentation_datasets(data_root_dir)\n",
    "    num_classes = len(train_dataset.classes)\n",
    "    \n",
    "    train_dataloader, test_dataloader = create_dataloaders(train_dataset, test_dataset, device, \n",
    "                                                           batch_size = batch_size, num_worker = num_worker)\n",
    "\n",
    "\n",
    "    \n",
    "    model = get_model(model_name = config[\"model_name\"], num_classes= num_classes, config = config).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=255)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.1) \n",
    "\n",
    "    if load_from_checkpoint:\n",
    "        load_checkpoint_path = (best_model_path if load_from_checkpoint == \"best\" else checkpoint_path)\n",
    "        start_epoch, best_metric = load_checkpoint(load_checkpoint_path, model, optimizer, scheduler, device)\n",
    "\n",
    "    if config.get('test_mode', False):\n",
    "        # Only evaluate on the test dataset\n",
    "        print(\"Running test evaluation...\")\n",
    "        test_metric = evaluation_loop(model, device, test_dataloader, criterion, phase = \"test\")\n",
    "        print(f\"Test metric (mIoU): {test_metric}\")\n",
    "        \n",
    "    else:\n",
    "        # Train and validate using train/val datasets\n",
    "        for epoch in range(start_epoch, num_epochs):\n",
    "            train_loop(model, device, train_dataloader, criterion, optimizer, epoch)\n",
    "\n",
    "\n",
    "            if (epoch + 1) % eval_interval == 0 or (epoch + 1) == num_epochs:\n",
    "                test_metric = evaluation_loop(model, device, test_dataloader, criterion, epoch = epoch, phase = \"validation\")\n",
    "\n",
    "                is_best = test_metric > best_metric\n",
    "                best_metric = max(test_metric, best_metric)\n",
    "                save_checkpoint(checkpoint_path, model, optimizer, scheduler, epoch, best_metric, is_best, best_model_path)\n",
    "\n",
    "            scheduler.step()\n",
    "\n",
    "\n",
    "    wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    'data_root_dir': '/datasets',\n",
    "    'batch_size': 16,\n",
    "    'learning_rate': 1e-3,\n",
    "    'model_name': 'UNet',\n",
    "    'num_epochs': 150,\n",
    "    \"eval_interval\" : 10,\n",
    "\n",
    "    \"dataset\": \"VOC2012\",\n",
    "    'wandb_project_name': 'UNet',\n",
    "\n",
    "    \"checkpoint_path\" : \"checkpoints/checkpoint.pth\",\n",
    "    \"best_model_path\" : \"checkpoints/best_model.pth\",\n",
    "    \"load_from_checkpoint\" : None,    # Options: \"latest\", \"best\", or None\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "train_main(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize your model's prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_prediction(X, y, y_pred):\n",
    "    \"\"\"\n",
    "    Visualize a few samples from the VOCSegmentation dataset, showing both the input image and its corresponding label (segmentation mask).\n",
    "\n",
    "    Parameters:\n",
    "        dataset: A dataset object, e.g., VOCSegmentation, where each item is a tuple (image, label).\n",
    "        cols (int): Number of columns in the visualization grid.\n",
    "        rows (int): Number of rows in the visualization grid.\n",
    "    \"\"\"\n",
    "    figure, ax = plt.subplots(nrows=X.shape[0], ncols=3, figsize=(12, X.shape[0] * 3))\n",
    "    ax = ax.flatten()\n",
    "\n",
    "    for i, idx in enumerate(range(X.shape[0])):\n",
    "        # Get the image and label (segmentation mask)\n",
    "        img, mask, mask_pred = X[i], y[i], y_pred[i]\n",
    "\n",
    "        # unnormalize image\n",
    "        mean = torch.tensor([0.485, 0.456, 0.406]).to(img.device).view(-1, 1, 1)\n",
    "        std = torch.tensor([0.229, 0.224, 0.225]).to(img.device).view(-1, 1, 1)\n",
    "        img = img * std + mean\n",
    "        \n",
    "\n",
    "        # Display the image\n",
    "        ax[3 * i].imshow(img.numpy().transpose((1, 2, 0)))\n",
    "        ax[3 * i].set_title(f\"Image {i+1}\")\n",
    "        ax[3 * i].axis(\"off\")\n",
    "\n",
    "        # Display the segmentation mask (assuming it's a single-channel mask)\n",
    "        mask = mask_tensor_to_pil(mask)\n",
    "        mask_pred = mask_tensor_to_pil(mask_pred)\n",
    "            \n",
    "        ax[3 * i + 1].imshow(mask, cmap=\"gray\")\n",
    "        ax[3 * i + 1].set_title(f\"Label {i+1}\")\n",
    "        ax[3 * i + 1].axis(\"off\")\n",
    "\n",
    "        ax[3 * i + 2].imshow(mask_pred, cmap=\"gray\")\n",
    "        ax[3 * i + 2].set_title(f\"Prediction {i+1}\")\n",
    "        ax[3 * i + 2].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "train_dataset, test_dataset = load_VOC_Segmentation_datasets(config['data_root_dir'])\n",
    "\n",
    "num_classes = len(train_dataset.classes)\n",
    "train_dataloader, test_dataloader = create_dataloaders(train_dataset, test_dataset, device, \n",
    "                                                       batch_size = 16, num_worker = 4)\n",
    "model = get_model(model_name = config[\"model_name\"], num_classes= num_classes, config = config).to(device)\n",
    "\n",
    "\n",
    "model_checkpoint_path = config[\"best_model_path\"]\n",
    "checkpoint = torch.load(model_checkpoint_path, map_location=device)\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "print(f\"=> loaded checkpoint '{model_checkpoint_path}' with mIoU {checkpoint['best_metric']} (epoch {checkpoint['epoch']})\")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "print(\"Model ready for inference\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "images, targets = next(iter(test_dataloader))\n",
    "images = images.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(images)\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "visualize_prediction(images.cpu(), targets.cpu(), preds.cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optional 실습\n",
    "mIoU성능 개선을 위한 다양한 실험을 시도해보세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 정리\n",
    "Lab을 마무리 짓기 전 저장된 checkpoint를 모두 지워 저장공간을 확보한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "import shutil, os\n",
    "if os.path.exists('checkpoints/'):\n",
    "    shutil.rmtree('checkpoints/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2.3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
